utils:
  # changes what counts as a "token", which affects the identifiers/terms collected
  # from symbols and docs, which ultimately can shift TF-IDF/BM25 vocab and embedding text
  token_pattern: "[A-Za-z0-9_]+" 
  # changes paper_id and any slug outputs that propagate to filenames/keys
  slugify_maxlen: 80
  # changes all emitted IDs (stable but different length), which flow into symbols/chunks/matches
  id_truncate: 12         
  python:
    # how far upward we scan for # blocks before a symbol, which creates longer windows and
    # can pull in more doc text leading to different docstring, identifiers and BoW, leading to different matches
    max_leading_lines: 12
    # stop scanning on the first blank line, which can merge separated comment islands 
    # into one docstring (bigger docs)
    stop_on_blank_line: true
  c_like:
    # upward scan limit for slashes/JSDoc, which imapcts the doc size
    max_leading_lines: 12
chunks:
  # max <div> nesting to descend into
  section_depth_limit: 3           
  # skip sections with titles matching this regex
  excluded_section_titles_regex: "(acknowledg(e)?ments?|references|funding|appendix|supplement)"
  excluded_section_types: ["references"] # TEI @type values to skip (case-insensitive)
  # include <list>/<item> as paragraph chunks
  include_lists_as_paragraphs: true
  # drop very short paragraphs/items
  paragraph_min_chars: 20               
  # collect paragraph-level biblio refs
  include_citations: true                
  # prefer back-matter listBibl over header
  prefer_back_list_for_refs: true        
  # media capture
  include_figures: true
  include_tables: true
  include_equations: true
  # include <graphic url=...>
  figure_include_graphic_url: true       
  # hard cap per table (prevents huge JSON)
  table_max_rows: 100                    
  # decide to ignore xml:id/id and always generate 
  force_generate_ids: false          
  # decide to ignore title when building paper_id
  prefer_filename_for_paper_id: false    
