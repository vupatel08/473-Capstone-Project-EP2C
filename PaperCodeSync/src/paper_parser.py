

"""
Paper Parser Module
==================

This module provides functionality to parse academic research papers from TEI XML format 
(produced by GROBID) into a structured JSON format. It extracts and organizes various 
components of a research paper including metadata, sections, figures, tables, equations, 
and references.

The module is designed to work with TEI XML files generated by GROBID, an ML library for 
extracting information from research papers. 

Setup Requirements:
-----------------
1. GROBID server running in Docker:
   ```
   docker run --rm --init --ulimit core=0 -p 8070:8070 grobid/grobid:0.8.2-crf
   ```

2. Convert PDF to TEI XML using GROBID:
   ```
   curl -sS -X POST \
     -F "input=@Paper.pdf;type=application/pdf" \
     http://localhost:8070/api/processFulltextDocument \
     -o Paper.tei.xml
   ```

Output Format:
------------
The parser converts TEI XML into a structured JSON with the following main components:
- paper_id: A unique identifier for the paper
- metadata: Title, authors, abstract, keywords, etc.
- sections: Hierarchical structure of the paper's content
- figures: All figures with captions and references
- tables: Parsed table data
- equations: Mathematical equations
- references: Bibliography entries

Dependencies:
-----------
- lxml
- Standard Python libraries: json, os, re, hashlib
"""

import argparse
import json
import os
import re
import hashlib
from typing import Dict, Any, List, Optional, Tuple
from lxml import etree as ET


# XML namespace mapping for TEI (Text Encoding Initiative)
# Maps the 'tei' prefix to the official TEI namespace URI
# This is used in all XPath expressions to identify TEI-specific elements
# For example: './/tei:figure' will find <figure> elements in the TEI namespace
TEI_NS = {"tei": "http://www.tei-c.org/ns/1.0"}

# --------------------------
# Utility Helper Functions
# --------------------------

def slugify(text: str, maxlen: int = 80) -> str:
    """
    Convert text into a URL-friendly slug format.
    
    Args:
        text (str): The text to convert into a slug
        maxlen (int, optional): Maximum length of the resulting slug (defaults to 80)
    
    Returns:
        str: A URL-friendly slug (lowercase, hyphen-separated)
    
    Example:
        >>> slugify("Hello World! 123")
        'hello-world-123'
    """
    if not text:
        return ""
    text = text.strip().lower()
    text = re.sub(r"[^a-z0-9]+", "-", text)
    text = re.sub(r"-{2,}", "-", text).strip("-")
    return text[:maxlen] or "untitled"

def text_of(el: Optional[ET.Element]) -> str:
    """
    Extract all text content from an XML element, including nested elements.
    
    Args:
        el (Optional[ET.Element]): The XML element to extract text from
    
    Returns:
        str: Combined text content with whitespace normalized
    """
    if el is None:
        return ""
    # join all text content including tails
    return "".join(el.itertext()).strip()

def sha1_id(*parts: str, prefix: str = "") -> str:
    """
    Generate a deterministic short ID by hashing multiple string parts.
    
    Args:
        *parts: Variable number of strings to combine for hashing
        prefix (str, optional): Prefix to add to the generated hash. Defaults to "".
    
    Returns:
        str: A 12-character hexadecimal hash, optionally prefixed
    
    Example:
        >>> sha1_id("section", "introduction", prefix="sec_")
        'sec_1a2b3c4d5e6f'
    """
    h = hashlib.sha1()
    for p in parts:
        if p:
            h.update(p.encode("utf-8"))
            h.update(b"\x00")
    base = h.hexdigest()[:12]
    return f"{prefix}{base}" if prefix else base

def get_attr(el: Optional[ET._Element], name: str) -> Optional[str]:
    """
    Safely get an attribute value from an XML element, handling namespaces.
    
    Args:
        el (Optional[ET._Element]): The XML element to get the attribute from
        name (str): Name of the attribute to retrieve
    
    Returns:
        Optional[str]: The attribute value or None if not found
        
    Note:
        Handles special case for xml:id which might be in XML namespace
    """
    if el is None:
        return None
    # xml:id might be in the XML namespace
    if name == "xml:id":
        return el.get("{http://www.w3.org/XML/1998/namespace}id")
    return el.get(name)

def first(el: ET._Element, xpath: str) -> Optional[ET._Element]:
    """
    Get the first element matching an XPath expression.
    
    Args:
        el (ET._Element): The root element to search from
        xpath (str): The XPath expression to evaluate
    
    Returns:
        Optional[ET._Element]: First matching element or None if none found
    
    Note:
        Handles both lxml (with full XPath) and ElementTree (limited XPath)
    """
    arr = el.xpath(xpath, namespaces=TEI_NS) if hasattr(el, "xpath") else el.findall(xpath)
    return arr[0] if arr else None

def all_nodes(el: ET._Element, xpath: str) -> List[ET._Element]:
    """
    Get all elements matching an XPath expression.
    
    Args:
        el (ET._Element): The root element to search from
        xpath (str): The XPath expression to evaluate
    
    Returns:
        List[ET._Element]: List of all matching elements (may be empty)
    """
    return el.xpath(xpath, namespaces=TEI_NS) if hasattr(el, "xpath") else el.findall(xpath)

def ensure_id(el: ET._Element, fallback_parts: List[str], prefix: str) -> str:
    """
    Ensure an element has an ID, generating one if necessary.
    
    Args:
        el (ET._Element): The element needing an ID
        fallback_parts (List[str]): Parts to use for generating a fallback ID
        prefix (str): Prefix for the generated ID
    
    Returns:
        str: Either the existing ID or a newly generated one
        
    Example:
        >>> ensure_id(figure_element, ["section1", "fig1"], "fig")
        'fig_1a2b3c4d5e6f'
    """
    xid = get_attr(el, "xml:id") or el.get("id")
    if xid:
        return xid
    # fallback deterministic id
    joined = "||".join([p for p in fallback_parts if p])
    return f"{prefix}_{sha1_id(joined)}"

# --------------------------
# TEI XML Section Parsers
# --------------------------

def parse_metadata(root: ET._Element) -> Dict[str, Any]:
    """
    Extract metadata from the TEI document header.
    
    Processes the following metadata elements:
    - Title
    - Authors (with affiliations and email if available)
    - Publication date
    - DOI
    - Journal name
    - Abstract
    - Keywords
    
    Args:
        root (ET._Element): Root element of the TEI document
        
    Returns:
        Dict[str, Any]: Dictionary containing all available metadata
    
    Example return value:
    {
        "title": "Example Paper Title",
        "authors": [{
            "name": "John Doe",
            "email": "john@university.edu",
            "affiliation": "Example University"
        }],
        "abstract": "Paper abstract text...",
        "keywords": ["keyword1", "keyword2"]
    }
    """
    title_el = first(root, ".//tei:teiHeader//tei:titleStmt/tei:title")
    title = text_of(title_el)

    # authors & affiliations
    authors = []
    for a in all_nodes(root, ".//tei:teiHeader//tei:sourceDesc//tei:biblStruct//tei:author") or \
              all_nodes(root, ".//tei:teiHeader//tei:titleStmt//tei:author"):
        # Try persName
        forename = text_of(first(a, ".//tei:persName/tei:forename"))
        surname = text_of(first(a, ".//tei:persName/tei:surname"))
        full = " ".join([forename, surname]).strip() or text_of(a)
        email = text_of(first(a, ".//tei:email"))
        # affiliation can be in <affiliation> or via orgName
        aff = text_of(first(a, ".//tei:affiliation")) or text_of(first(a, ".//tei:orgName"))
        authors.append({
            "name": full,
            "forename": forename or None,
            "surname": surname or None,
            "email": email or None,
            "affiliation": aff or None
        })

    # publication info
    date = get_attr(first(root, ".//tei:teiHeader//tei:publicationStmt//tei:date"), "when") or \
           text_of(first(root, ".//tei:teiHeader//tei:publicationStmt//tei:date"))
    doi = text_of(first(root, ".//tei:teiHeader//tei:idno[@type='DOI']")) or \
          text_of(first(root, ".//tei:teiHeader//tei:idno[@type='doi']"))
    journal = text_of(first(root, ".//tei:teiHeader//tei:monogr/tei:title")) or \
              text_of(first(root, ".//tei:teiHeader//tei:series/tei:title"))

    # abstract
    abstract_text = " ".join([
        text_of(p) for p in all_nodes(root, ".//tei:profileDesc//tei:abstract//tei:p")
    ]) or " ".join([
        text_of(p) for p in all_nodes(root, ".//tei:abstract//tei:p")
    ])

    # keywords
    keywords = [text_of(kw) for kw in all_nodes(root, ".//tei:profileDesc//tei:textClass//tei:keywords//tei:term")]

    return {
        "title": title or None,
        "doi": doi or None,
        "journal": journal or None,
        "date": date or None,
        "authors": authors,
        "abstract": abstract_text or None,
        "keywords": keywords or []
    }

def parse_figures(div_or_root: ET._Element, scope_id: str) -> List[Dict[str, Any]]:
    """
    Extract figure information from a section or document root.
    
    Processes all <figure> elements within the given scope, extracting:
    - Figure ID (generated if not present)
    - Label (e.g., "Figure 1")
    - Caption text
    - Associated graphic URL
    
    Args:
        div_or_root (ET._Element): XML element containing figures (section or document root)
        scope_id (str): ID of the containing scope (used for generating figure IDs)
    
    Returns:
        List[Dict[str, Any]]: List of dictionaries containing figure information
    
    Example return value:
    [
        {
            "id": "fig_abc123",
            "label": "Figure 1",
            "caption": "Architecture diagram of the system",
            "graphic": "images/figure1.png"
        }
    ]
    """
    figures = []
    for fig in all_nodes(div_or_root, ".//tei:figure"):
        # Generate or get figure ID using description as fallback
        fig_id = ensure_id(fig, [scope_id, text_of(first(fig, "./tei:figDesc"))], "fig")
        
        # Extract figure components
        label = text_of(first(fig, "./tei:label"))
        caption = text_of(first(fig, "./tei:figDesc")) or text_of(first(fig, "./tei:head"))
        graphic = first(fig, ".//tei:graphic")
        url = get_attr(graphic, "url") if graphic is not None else None
        
        figures.append({
            "id": fig_id,
            "label": label or None,
            "caption": caption or None,
            "graphic": url or None
        })
    return figures

def parse_tables(div_or_root: ET._Element, scope_id: str) -> List[Dict[str, Any]]:
    """
    Extract table data from a section or document root.
    
    Processes all <table> elements within the given scope, extracting:
    - Table ID (generated if not present)
    - Table title/caption
    - Row and cell content in a structured format
    
    Args:
        div_or_root (ET._Element): XML element containing tables (section or document root)
        scope_id (str): ID of the containing scope (used for generating table IDs)
    
    Returns:
        List[Dict[str, Any]]: List of dictionaries containing table information
    
    Example return value:
    [
        {
            "id": "tbl_abc123",
            "title": "Performance Comparison",
            "rows": [
                ["Method", "Accuracy", "Time"],
                ["A", "95%", "10s"],
                ["B", "92%", "8s"]
            ]
        }
    ]
    
    Note:
        This implements a simple row/column extraction as TEI table structures can vary.
        More complex table structures (e.g., with rowspans/colspans) may need additional processing.
    """
    tables = []
    for tb in all_nodes(div_or_root, ".//tei:table"):
        # Generate or get table ID using header as fallback
        tb_id = ensure_id(tb, [scope_id, text_of(first(tb, "./tei:head"))], "tbl")
        head = text_of(first(tb, "./tei:head"))
        
        # Extract rows and cells
        rows = []
        for row in all_nodes(tb, ".//tei:row"):
            cells = [text_of(c) for c in all_nodes(row, "./tei:cell")]
            if cells:  # Only add non-empty rows
                rows.append(cells)
                
        tables.append({
            "id": tb_id,
            "title": head or None,
            "rows": rows
        })
    return tables

def parse_equations(div_or_root: ET._Element, scope_id: str) -> List[Dict[str, Any]]:
    """
    Extract mathematical equations from a section or document root.
    
    Processes mathematical content that may be encoded in various TEI formats:
    - <formula> elements
    - <figure type="formula">
    - Occasionally in <code> or <hi rend='formula'> elements
    
    Args:
        div_or_root (ET._Element): XML element containing equations
        scope_id (str): ID of the containing scope (used for generating equation IDs)
    
    Returns:
        List[Dict[str, Any]]: List of dictionaries containing equation information
    
    Example return value:
    [
        {
            "id": "eq_abc123",
            "text": "E = mc^2"
        }
    ]
    
    Note:
        The actual equation content may be in various formats (LaTeX, MathML, plain text)
        depending on how GROBID processed the original document.
    """
    eqs = []
    # TEI may encode math as <formula> or <figure type="formula"> etc.
    for fm in all_nodes(div_or_root, ".//tei:formula"):
        fm_id = ensure_id(fm, [scope_id, text_of(fm)], "eq")
        eqs.append({"id": fm_id, "text": text_of(fm)})
    # fallback: math in <code> or <hi rend='formula'> (rare)
    return eqs

def parse_citations_in_p(p: ET._Element) -> List[str]:
    """
    Extract citation references from a paragraph element.
    
    Looks for bibliographic references within the paragraph that link to the
    bibliography section, typically encoded as <ref type="bibr"> elements.
    
    Args:
        p (ET._Element): Paragraph element to search for citations
    
    Returns:
        List[str]: List of unique citation reference IDs, sorted
    
    Example:
        For a paragraph with "[1], [2], [1]", returns ['b1', 'b2']
    
    Note:
        - Citation targets typically start with '#' which is stripped
        - Duplicate citations are removed
        - Results are sorted for consistent output
    """
    # Collect targets of bibliographic refs, like <ref type="bibr" target="#b1">
    cites = []
    for ref in all_nodes(p, ".//tei:ref[@type='bibr']"):
        tgt = get_attr(ref, "target")
        if tgt:
            cites.append(tgt.lstrip("#"))
    return sorted(set(cites))

def parse_paragraphs(div: ET._Element, sec_id: str) -> List[Dict[str, Any]]:
    """
    Extract paragraphs and list items from a section element.
    
    Processes both regular paragraphs (<p>) and list items (<item>), treating
    both as text blocks. Each paragraph is assigned a unique ID and includes
    any citations it contains.
    
    Args:
        div (ET._Element): Section element containing paragraphs
        sec_id (str): ID of the containing section
    
    Returns:
        List[Dict[str, Any]]: List of dictionaries containing paragraph information
    
    Example return value:
    [
        {
            "id": "p_abc123",
            "text": "This is a paragraph with a citation [1]",
            "citations": ["b1"]
        }
    ]
    
    Note:
        - List items are treated as paragraphs with a special ID prefix
        - Empty paragraphs are included but can be filtered downstream
        - Citations are extracted and linked to bibliography entries
    """
    paras = []
    
    # Process regular paragraphs
    for i, p in enumerate(all_nodes(div, "./tei:p")):
        pid = ensure_id(p, [sec_id, str(i), text_of(p)[:80]], "p")
        paras.append({
            "id": pid,
            "text": text_of(p),
            "citations": parse_citations_in_p(p)
        })
    
    # Process list items as paragraphs
    for ul in all_nodes(div, "./tei:list"):
        for i, item in enumerate(all_nodes(ul, "./tei:item")):
            pid = ensure_id(item, [sec_id, "li", str(i), text_of(item)[:80]], "p")
            paras.append({
                "id": pid,
                "text": text_of(item),
                "citations": parse_citations_in_p(item)
            })
            
    return paras

def parse_section(div: ET._Element, parent_id: str, depth: int = 1) -> Dict[str, Any]:
    """
    Recursively parse a section and its subsections from the document.
    
    Processes a section division (<div>) element, extracting:
    - Section title and type
    - Paragraphs and list items
    - Figures, tables, and equations
    - Nested subsections
    
    Args:
        div (ET._Element): Division element representing a section
        parent_id (str): ID of the parent section
        depth (int): Current section nesting depth (used for ID generation)
    
    Returns:
        Dict[str, Any]: Dictionary containing all section content
    
    Example return value:
    {
        "id": "sec1_abc123",
        "title": "Introduction",
        "type": "section",
        "paragraphs": [...],
        "figures": [...],
        "tables": [...],
        "equations": [...],
        "subsections": [...]  # optional
    }
    
    Note:
        - Sections can be nested to arbitrary depth
        - All content elements (paragraphs, figures, etc.) are assigned unique IDs
        - Empty sections are included but can be filtered downstream
    """
    # Extract section metadata
    head = text_of(first(div, "./tei:head"))
    div_type = get_attr(div, "type") or None
    sec_id = ensure_id(div, [parent_id, head or "", div_type or ""], f"sec{depth}")

    # Parse direct content
    paragraphs = parse_paragraphs(div, sec_id)
    figures = parse_figures(div, sec_id)
    tables = parse_tables(div, sec_id)
    equations = parse_equations(div, sec_id)

    # Recursively parse subsections
    subsections = []
    for sub_div in all_nodes(div, "./tei:div"):
        subsections.append(parse_section(sub_div, parent_id=sec_id, depth=depth+1))

    # Construct section object
    sec_obj = {
        "id": sec_id,
        "title": head or None,
        "type": div_type,
        "paragraphs": paragraphs,
        "figures": figures,
        "tables": tables,
        "equations": equations
    }
    if subsections:
        sec_obj["subsections"] = subsections
    return sec_obj

def parse_body(root: ET._Element) -> List[Dict[str, Any]]:
    """
    Extract the main content sections from the document body.
    
    Locates the document body and processes its top-level divisions into
    a hierarchical section structure. Handles different TEI body locations:
    - Standard: text/body
    - Fallback: direct body element
    
    Args:
        root (ET._Element): Root element of the TEI document
        
    Returns:
        List[Dict[str, Any]]: List of top-level sections with their content
        
    Example return value:
    [
        {
            "id": "sec1_abc123",
            "title": "Introduction",
            "paragraphs": [...],
            "subsections": [...]
        },
        ...
    ]
    
    Note:
        - Returns empty list if no body or sections are found
        - Each section can contain nested subsections to arbitrary depth
        - All section content (paragraphs, figures, etc.) is parsed recursively
    """
    # Try standard TEI body location
    body = first(root, ".//tei:text/tei:body")
    if body is None:
        # fallback — sometimes <body> sits directly under <text>
        body = first(root, ".//tei:body")
        
    sections = []
    if body is not None:
        # Parse each top-level division
        for div in all_nodes(body, "./tei:div"):
            sections.append(parse_section(div, parent_id="body", depth=1))
            
    return sections

def parse_references(root: ET._Element) -> List[Dict[str, Any]]:
    """
    Extract bibliography references from the document.
    
    Processes bibliographic entries which can be found in different locations:
    - Primarily in <back><listBibl> section
    - Sometimes under teiHeader/sourceDesc
    
    Args:
        root (ET._Element): Root element of the TEI document
    
    Returns:
        List[Dict[str, Any]]: List of dictionaries containing reference information
    
    Example return value:
    [
        {
            "id": "b1",
            "title": "Example Paper Title",
            "authors": ["John Smith", "Jane Doe"],
            "year": "2020",
            "doi": "10.1234/example"
        }
    ]
    
    Note:
        - References can be in different formats (analytic vs monographic)
        - Author names are normalized to "forename surname" format when possible
        - DOIs are extracted when available for linking
    """
    refs = []
    # Most GROBID TEI puts bib in <back><listBibl>
    bibl_xpath = ".//tei:text/tei:back//tei:listBibl//tei:biblStruct"
    candidates = all_nodes(root, bibl_xpath)
    if not candidates:
        # fallback: occasionally under teiHeader
        candidates = all_nodes(root, ".//tei:teiHeader//tei:sourceDesc//tei:biblStruct")
        
    for b in candidates:
        rid = ensure_id(b, [get_attr(b, "xml:id") or "", text_of(b)], "ref")

        # Extract title from analytic or monographic part
        title = text_of(first(b, "./tei:analytic/tei:title")) or text_of(first(b, "./tei:monogr/tei:title"))
        
        # Try to get year from date element or attribute
        year = text_of(first(b, ".//tei:date"))
        if not year:
            year = get_attr(first(b, ".//tei:date"), "when")
            
        # Look for DOI in different formats
        doi = text_of(first(b, ".//tei:idno[@type='DOI']")) or text_of(first(b, ".//tei:idno[@type='doi']"))

        # Extract authors with name parts
        auth_nodes = all_nodes(b, "./tei:analytic/tei:author") or all_nodes(b, "./tei:monogr/tei:author")
        authors = []
        for a in auth_nodes:
            forename = text_of(first(a, ".//tei:forename"))
            surname = text_of(first(a, ".//tei:surname"))
            full = " ".join([forename, surname]).strip() or text_of(a)
            authors.append(full)

        refs.append({
            "id": get_attr(b, "xml:id") or rid,
            "title": title or None,
            "authors": authors,
            "year": year or None,
            "doi": doi or None
        })
    return refs

# --------------------------
# Main Conversion Functions
# --------------------------

def build_paper_id(meta: Dict[str, Any], input_path: str) -> str:
    """
    Generate a unique, URL-friendly identifier for the paper.
    
    Attempts to create an ID from the paper title, falling back to the
    input filename if no title is available.
    
    Args:
        meta (Dict[str, Any]): Paper metadata containing title
        input_path (str): Path to the input TEI file
        
    Returns:
        str: URL-friendly identifier for the paper
        
    Note:
        The resulting ID is truncated to 96 characters for practical use
    """
    title = (meta.get("title") or "").strip()
    if title:
        return slugify(title, maxlen=96)
    base = os.path.splitext(os.path.basename(input_path))[0]
    return slugify(base, maxlen=96)

def tei_to_json(input_path: str) -> Dict[str, Any]:
    """
    Convert a TEI XML document to a structured JSON format.
    
    This is the main entry point for converting a GROBID-generated TEI XML
    file into the EP2C paper structure format.
    
    Args:
        input_path (str): Path to the input TEI XML file
        
    Returns:
        Dict[str, Any]: Complete paper structure in JSON format
        
    Example return value:
    {
        "paper_id": "example-paper-title",
        "metadata": {...},
        "sections": [...],
        "figures": [...],
        "tables": [...],
        "equations": [...],
        "references": [...],
        "source": {
            "tei_path": "/absolute/path/to/file.xml",
            "tei_flavor": "GROBID-TEI"
        },
        "schema_version": "ep2c.paper.v1"
    }
    
    Note:
        - Uses a permissive XML parser to handle potential TEI issues
        - Extracts both hierarchical content (sections) and floating elements
        - Provides source tracking for downstream processing
    """
    # Initialize parser with error recovery
    parser = ET.XMLParser(remove_comments=True, recover=True)
    tree = ET.parse(input_path, parser=parser)
    root = tree.getroot()

    # Extract document components
    metadata = parse_metadata(root)
    sections = parse_body(root)
    
    # Handle top-level elements (outside sections)
    top_figs = parse_figures(first(root, ".//tei:text") or root, "body")
    top_tbls = parse_tables(first(root, ".//tei:text") or root, "body")
    top_eqs  = parse_equations(first(root, ".//tei:text") or root, "body")

    # Get bibliography
    references = parse_references(root)

    # Generate paper identifier
    paper_id = build_paper_id(metadata, input_path)

    # Construct output structure
    out: Dict[str, Any] = {
        "paper_id": paper_id,
        "metadata": metadata,
        "sections": sections,
        "figures": top_figs,
        "tables": top_tbls,
        "equations": top_eqs,
        "references": references,
        # helpful derived fields for downstream alignment
        "source": {
            "tei_path": os.path.abspath(input_path),
            "tei_flavor": "GROBID-TEI"
        },
        "schema_version": "ep2c.paper.v1"
    }
    return out

if __name__ == "__main__":
    """
    Command-line execution entry point.
    
    Demonstrates basic usage of the TEI to JSON converter:
    1. Reads a TEI XML file produced by GROBID
    2. Converts it to structured JSON format
    3. Writes the result to a JSON file
    
    Example:
        $ python paper_parser.py
        [OK] Wrote paper_structure.json
    """
    INPUT_PATH = "../Paper.tei.xml"
    OUTPUT_PATH = "../Paper.json"

    # Convert TEI to JSON structure
    result = tei_to_json(INPUT_PATH)
    
    # Write result to file with nice formatting
    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
        
    print(f"[OK] Wrote {OUTPUT_PATH}")