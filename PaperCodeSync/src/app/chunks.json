{
  "paper_id": "test",
  "metadata": {
    "title": "FireRedTTS-2: Towards Long Conversational Speech Generation for Podcast and Chatbot",
    "doi": null,
    "journal": null,
    "date": null,
    "authors": [],
    "abstract": null,
    "keywords": []
  },
  "sections": [
    {
      "id": "sec1-74a3cfbf264c",
      "title": "FireRedTTS-2: Towards Long Conversational Speech Generation for Podcast and Chatbot",
      "type": null,
      "paragraphs": [
        {
          "id": "p-d6e2f5a6744b",
          "text": "Kun Xie∗, Feiyu Shen∗, Junjie Li, Fenglong Xie†, Xu Tang, Yao Hu Xiaohongshu",
          "citations": []
        }
      ],
      "figures": [],
      "tables": [],
      "equations": []
    },
    {
      "id": "sec1-62ce44b5a22d",
      "title": "Abstract",
      "type": null,
      "paragraphs": [
        {
          "id": "p-7b0c753f4b1a",
          "text": "Current dialogue generation approaches typically require the complete dialogue text before synthesis and produce a single, inseparable speech containing all voices, making them unsuitable for interactive chat; moreover, they suffer from unstable synthesis, inaccurate speaker transitions, and incoherent prosody. In this work, we present FireRedTTS-2, a long-form streaming TTS system for multi-speaker dialogue generation, delivering stable, natural speech with reliable speaker switching and context-aware prosody. A new $1 2 . 5 \\mathrm { H z }$ streaming speech tokenizer accelerates training and inference, extends maximum dialogue length, encodes richer semantics to stabilize text-to-token modeling and supports high-fidelity streaming generation for real-time applications. We adopt a text–speech interleaved format, concatenating speaker-labeled text with aligned speech tokens in chronological order, and model it with a dual-transformer: a large decoder-only transformer predicts tokens at the first layer, and a smaller one completes subsequent layers. Experimental results show that FireRedTTS-2 integrates seamlessly with chat frameworks and, with minimal fine-tuning, produces emotionally expressive speech guided by implicit contextual cues. In podcast generation, it surpasses existing systems including MoonCast, Zipvoice-Dialogue, and MOSS-TTSD in objective intelligibility, speaker-turn reliability, and perceived naturalness with context-consistent prosody. Our demos are available at https://fireredteam.github.io/demos/firered_tts_2.",
          "citations": []
        }
      ],
      "figures": [],
      "tables": [],
      "equations": []
    },
    {
      "id": "sec1-63866a78487c",
      "title": "1 Introduction",
      "type": null,
      "paragraphs": [
        {
          "id": "p-88308230dda7",
          "text": "Large language model (LLM) based text-to-speech (TTS) systems can generate natural-sounding speech with zero-shot voice cloning and are widely used for monologue applications like video dubbing. These systems typically follow one of two modeling paradigms: an autoregressive, decoderonly transformer that predicts speech tokens[1–7], or a non-autoregressive flow-matching model that produces mel-spectrograms directly from text[8, 9]. While these monologue TTS systems can be adapted to dialogue generation by segmenting dialogue text and synthesizing each fragment independently[10–12], this strategy ignores preceding text and speech context, leading to a loss of conversational coherence.\nRecent works have extended TTS system to two-speaker dialogue generation, which can be grouped into three categories based on how text and speech are organized across turns: (1) splitting the dialogue text into two parallel channels and synthesizing a single mixed speech track containing both voices, which can naturally handles overlapping speech and generate interjections effects[13, 14]; (2) concatenating the dialogue text in chronological order with each utterance prefixed by a speaker label, which likewise produces a mixed speech track[15–20]; and (3) interleaving the text and speech of each utterance[21]. Approaches (1) and (2) require the complete dialogue text before synthesis and yield a single inseparable mixed speech, limiting their suitability for interactive scenarios such as chat, whereas (3) supports flexible sentence-by-sentence generation, suitable for both interactive chat and podcast production.\nIn this work, we present FireRedTTS-2, a long-form, streaming TTS system for multi-speaker dialogue and podcast generation that delivers stable, natural speech, reliable speaker switching, and context-aware prosody. A new streaming $1 2 . 5 \\mathrm { H z }$ speech tokenizer accelerates training and inference, lengthens the effective dialogue context, encodes richer semantics to stabilize text-to-token modeling and supports high-fidelity streaming generation for real-time applications. We adopt an interleaved text–speech format by concatenating speaker-labeled text with speech tokens in chronological order, and model it with a dual-transformer architecture: a large decoder-only network predicts tokens at the first layer, while a smaller network refines the subsequent layers. Experimental results show that FireRedTTS-2 integrates seamlessly with chat frameworks and, with minimal fine-tuning, produces emotionally expressive speech guided by implicit context. In podcast generation, it surpasses the state of the art systems including MoonCast[15], ZipVoice-Dialogue[18], and MOSS-TTSD[19] in objective intelligibility, speaker-turn reliability, and perceived naturalness, while maintaining prosody consistent with long-range context.",
          "citations": []
        }
      ],
      "figures": [],
      "tables": [],
      "equations": []
    },
    {
      "id": "sec1-e02bf42c8e0c",
      "title": "2 FireRedTTS-2",
      "type": null,
      "paragraphs": [
        {
          "id": "p-ebeeda9fc683",
          "text": "As shown in Figure 1, FireRedTTS-2 consists of a newly developed speech tokenizer and a text-tospeech model with perception to previous text and speech context.\nFigure 1: An overview of FireRedTTS-2, including: (a) a new speech tokenizer with a $1 2 . 5 \\mathrm { H z }$ frame rate and enhanced semantic information, and (b) a text-to-speech model using a dual-transformer architecture with interleaved text–speech input, enabling sentence-by-sentence generation and contextually coherent prosody.",
          "citations": []
        }
      ],
      "figures": [],
      "tables": [],
      "equations": []
    },
    {
      "id": "sec1-1c97af3f0a23",
      "title": "2.1 Speech Tokenizer",
      "type": null,
      "paragraphs": [
        {
          "id": "p-ae67c9c86886",
          "text": "We design our speech tokenizer to enhance dialogue modeling, with a focus on long, multi-speaker speech sequence. To make such sequences tractable, we reduce the frame rate to $1 2 . 5 \\mathrm { H z }$ , half that of most open-source tokenizers[3–5, 22–24]. We further employ semantic injection and supervision to simplify text-to-token modeling, which has been shown to improve synthesis stability[19, 22–25]. In addition, the tokenizer supports high-fidelity streaming generation for real-time applications.\nAs illustrated in Figure 1(a), our speech tokenizer employs a pretrained Whisper[26] encoder to extract semantic features from the 16kHz input speech. These semantic features are encoded by an adapter and then concatenated with acoustic features from a trainable acoustic encoder structurally identical to the Whisper encoder. The combined features undergo 4 times downsampling from $5 0 \\mathrm { H z }$ to $1 2 . 5 \\mathrm { H z }$ and are discretized by a residual vector quantizer (RVQ)[27] with 16 layers, each containing 2048 code entries. The quantized features are upsampled to $5 0 \\mathrm { H z }$ and fed to a semantic decoder to predict the original semantic features derived from the pretrained Whisper encoder. The same upsampled features are also used by a Vocos[28]-based acoustic decoder to reconstruct the waveform. Depending on the reception fields of its inner convolution and attention layers, the acoustic decoder can be implemented as either streaming or non-streaming.\nTo balance generalization capability and speech quality, we train our speech tokenizer in two stages similar to[19]. First, the acoustic decoder is implemented as non-streaming and optimized to predict 16kHz speech. We use approximately $5 0 0 \\mathrm { k }$ hours of speech data and train the model for $3 2 0 \\mathrm { k }$ steps on 32 H800 GPUs, with each sample randomly cropped to 6 seconds. For the final $3 5 \\mathrm { k }$ steps, we incorporate the perceptual loss[23, 29] to further improve semantic details. In the second stage, we freeze the encoding part and replace the acoustic decoder with a fully streaming variant that predicts 24kHz speech. We continue to train the speech tokenizer on a subset of 60k hours high-fidelity speech data for 80k steps.",
          "citations": []
        }
      ],
      "figures": [],
      "tables": [],
      "equations": []
    },
    {
      "id": "sec1-2d0aa7c74064",
      "title": "2.2 Text-to-Speech Model",
      "type": null,
      "paragraphs": [
        {
          "id": "p-72ce3a56a2ca",
          "text": "Building on the new speech tokenizer, we employ a dual-transformer architecture akin to [21, 25] that operates on a text–speech interleaved sequence, enabling flexible sentence-bysentence generation and reducing first-packet latency. As illustrated in Figure 1(b), each dialogue text is prefixed with a speaker tag (e.g., \"[S1]\") and concatenated with its corresponding speech tokens; these segments are then joined in temporal order to form sequences such as \"[S1]<text><audio>[S2]<text><audio>[S3]<text><audio>...\". Existing approaches[16, 17, 19] model multi-layer speech tokens using the delay-pattern[30]: for $N$ token layers, the $i ^ { \\mathrm { t h } }$ layer is shifted $i - 1$ timesteps to the right, and $N$ prediction heads predict these shifted layers in parallel. This design has two main drawbacks: first, at each timestep the model has only partial access to the speech tokens from previous steps due to the rightward shifts, weakening contextual conditioning; second, obtaining the complete set of $N$ layer tokens for the first timestep requires $N$ autoregressive steps, resulting in high latency. To overcome these issues, we adopt a dual-transformer architecture comprising a backbone transformer that processes the text–speech interleaved sequence and predicts the first-layer tokens, and a smaller decoder transformer that generates remaining token layers. Both transformers are based on Qwen2.5[31] structure. At each timestep, the decoder consumes both the predicted first layer token and the backbone’s hidden states, which provide complete contextual information. Comparing with the delay-pattern, it requires one auto-regressive inference step of the backbone transformer and $N - 1$ steps of the smaller decoder, reducing computation and first-packet latency. Moreover, our speech tokenizer produces high-fidelity speech in a streaming manner without requiring separate token-to-speech modules, simplifying the overall system.\nThe text-to-speech model is optimized with the following loss function:",
          "citations": []
        },
        {
          "id": "p-021b4cb4f328",
          "text": "[eq:eq-7768ee53c13d-1]\n$$\n\\mathcal{ L } _ { l o s s } = 2 * ( ( 1 - \\lambda_ { d e c o d e r } ) \\mathcal{ L } _ { b a c k b o n e } + \\lambda_ { d e c o d e r } \\mathcal{ L } _ { d e c o d e r } ) + \\lambda_ { t e x t } \\mathcal{ L } _ { t e x t }\n$$\nHere, $\\mathcal { L } _ { b a c k b o n e }$ and $\\mathcal { L } _ { d e c o d e r }$ denote the cross-entropy loss of the backbone and decoder transformer respectively. To improve training efficiency, we optimize the decoder transformer only on 1/8 of the speech segments in the interleaved sequence. Additionally, we incorporate a cross-entropy loss for the textual part $( \\mathcal { L } _ { t e x t } )$ to stabilize training. In our experiments, we set $\\lambda _ { t e x t } = 0 . 0 1$ and $\\lambda _ { d e c o d e r } = 0 . 6$\nTo enable the model with dialogue generation capability, we adopt a three-stage curriculum training process utilized in [13, 19], comprising pretraining, post-training, and supervised fine-tuning (SFT). The pretraining stage leverages 1.1M hours of monologue speech data and trains the model for 2 epochs to build foundational text-to-speech ability. Subsequently, we post-train FireRedTTS-2 for 5 epochs on $3 0 0 \\mathrm { k }$ hours of multi-speaker dialogue data, with each dialogue containing 2 to 5 speakers, to enable robust multi-speaker dialogue generation. Finally, the SFT stage is applied to tailor the model to specific voices with minimal data.",
          "citations": []
        }
      ],
      "figures": [],
      "tables": [],
      "equations": [
        {
          "id": "eq-7768ee53c13d-1",
          "page": null,
          "latex": "\\mathcal{ L } _ { l o s s } = 2 * ( ( 1 - \\lambda_ { d e c o d e r } ) \\mathcal{ L } _ { b a c k b o n e } + \\lambda_ { d e c o d e r } \\mathcal{ L } _ { d e c o d e r } ) + \\lambda_ { t e x t } \\mathcal{ L } _ { t e x t }",
          "has_latex": true
        }
      ]
    },
    {
      "id": "sec1-f44b78a1e52a",
      "title": "3 Downstream Applications",
      "type": null,
      "paragraphs": [
        {
          "id": "p-9158b9f667b9",
          "text": "FireRedTTS-2 excels at both monologue and dialogue speech generation. For monologues, it offers competitive zero-shot voice cloning suitable for tasks like video dubbing. For dialogues generation, it surpasses monologue TTS systems due to its perception of text and speech context, producing speech with coherent prosody. Compared with existing dialogue TTS systems, it supports sentence-bysentence generation, enabling both interactive chat and offline podcast production. Across both modes, FireRedTTS-2 can be tailored to specific application requirements with minimal data, demonstrating strong flexibility.",
          "citations": []
        }
      ],
      "figures": [],
      "tables": [],
      "equations": []
    },
    {
      "id": "sec1-8f9cc9174536",
      "title": "3.1 Voice Cloning",
      "type": null,
      "paragraphs": [
        {
          "id": "p-62dccf79cb8d",
          "text": "Our speech tokenizer captures both semantic and acoustic information, enabling fine-grained acoustic modeling. Paired with large-scale pretraining on monologue speech, it allows FireRedTTS-2 to deliver robust zero-shot voice cloning. Given a speech prompt and its transcript, we concatenate the prompt transcript, the target text to be synthesized, and the discretized prompt speech tokens, then feed this sequence into the text-to-speech model to autoregressively generate new speech tokens. The generated tokens are appended to the prompt tokens and decoded into a waveform by the speech tokenizer’s decoder, after which the portion corresponding to the original prompt is removed.",
          "citations": []
        }
      ],
      "figures": [],
      "tables": [],
      "equations": []
    },
    {
      "id": "sec1-46c716926647",
      "title": "3.2 Interactive Chat",
      "type": null,
      "paragraphs": [
        {
          "id": "p-edce760829b6",
          "text": "Current interactive chat frameworks[10, 11] typically rely on monologue TTS systems, which lack perception of prior user queries and system responses, often resulting in inconsistent emotion and prosody. This can be partially mitigated with explicit instructions such as emotion labels, but it requires additional fine-tuning of the text LLM and adds unnecessary complexity.\nFigure 2: Integration of FireRedTTS-2 into interactive chat scenarios.\nAs shown in Figure 2, FireRedTTS-2 can be seamlessly integrated into existing chat frameworks without modifying other modules. To address the inconsistency issue, we fine-tune the post-trained FireRedTTS-2 to infer and adjust emotion and prosody from implicit contextual cues. Specifically, we curate a 15-hour speech corpus of a distinctive female voice expressing six emotions: surprise, sadness, happiness, concern, apology, and anger. Then we emulate conversational context by first generating text context with a text LLM and then synthesizing it into speech. After fine-tuning, FireRedTTS-2 dynamically shifts emotion and tone in response to preceding chat history, delivering a near-human interactive experience.",
          "citations": []
        }
      ],
      "figures": [],
      "tables": [],
      "equations": []
    },
    {
      "id": "sec1-71b2189ccb37",
      "title": "3.3 Podcast Generation",
      "type": null,
      "paragraphs": [
        {
          "id": "p-5c1af97004f2",
          "text": "Subsequent post-training on dialogue corpus equips FireRedTTS-2 with conversational generation abilities, making it well-suited for podcast generation. Compared with conventional segmenting approaches that utlizes monologue TTS systems, it simplifies the workflow and can synthesizes contextually coherent prosody. Moreover, it generates dialogue speech sentence by sentence, providing greater flexibility for editing and post-processing.\nAs shown in Figure 3, FireRedTTS-2 can perform zero-shot podcast generation by placing two dialogue turns as prompt context and then generating subsequent turns one by one. It currently supports 3 minutes dialogues with 4 speakers and can be easily scaled to longer conversations with more speakers by extending training corpus. It can also be tailored to specific speakers with minimal data. In this work, we collected approximately 50 hours of dialogue speech featuring a male and a female podcast host and fine-tuned the post-trained model for 15 epochs. Once customized,\nFigure 3: Zero-shot podcast generation of FireRedTTS-2.\nFireRedTTS-2 delivers stable synthesis, accurate speaker transitions, and contextually coherent prosody that matches the hosts’ distinctive speaking styles.",
          "citations": []
        }
      ],
      "figures": [],
      "tables": [],
      "equations": []
    },
    {
      "id": "sec1-20f45aa0113b",
      "title": "4 Results",
      "type": null,
      "paragraphs": [],
      "figures": [],
      "tables": [],
      "equations": []
    },
    {
      "id": "sec1-fce9e653f24e",
      "title": "4.1 Speech Tokenizer Evaluation",
      "type": null,
      "paragraphs": [
        {
          "id": "p-0d66effb9582",
          "text": "We evaluate our speech tokenizer on intelligibility, speaker similarity, and speech quality of the reconstructed speech using the LibriSpeech test-clean set, which contains 2,620 utterances at $1 6 \\mathrm { k H z }$ . Speech intelligibility is measured by word error rate (WER) using a HuBERT-based automatic speech recognition (ASR) system3. Speech quality is assessed with speaker similarity (SPK-SIM) computed by the WavLM-Large model 4, Short-Time Objective Intelligibility (STOI), Perceptual Evaluation of Speech Quality (PESQ), and UTMOS 5. We compare our speech tokenizer with other methods that likewise incorporate semantic injection and supervision, including XY-Tokenizer[19], XCodec2[23], SpeechTokenizer[24], and Mimi[25]; results are reported in Table 1.\n<table><tr><td>Models</td><td>BPS</td><td>Frame Rate</td><td>WER↓</td><td>SPK SIM</td><td>STOI↑</td><td>PESQ↑ WB</td><td>PESQ↑ NB</td><td>UT</td></tr><tr><td>Ground Truth</td><td>-</td><td>-</td><td>1.96</td><td>-</td><td>1.00</td><td>4.64</td><td>4.55</td><td>4.09</td></tr><tr><td>Xcodec2</td><td>800</td><td>50</td><td>2.46</td><td>0.82</td><td>0.92</td><td>2.43</td><td>3.04</td><td>4.13</td></tr><tr><td>XY-Tokneizer</td><td>1000</td><td>12.5</td><td>-</td><td>0.83</td><td>0.91</td><td>2.41</td><td>3.00</td><td>-</td></tr><tr><td>SpeechTokenizer</td><td>2000</td><td>50</td><td>2.86</td><td>0.66</td><td>0.88</td><td>1.92</td><td>2.38</td><td>3.56</td></tr><tr><td>Mimi</td><td>2200</td><td>12.5</td><td>2.26</td><td>0.87</td><td>0.94</td><td>2.88</td><td>3.42</td><td>3.87</td></tr><tr><td>Ours</td><td>2200</td><td>12.5</td><td>2.16</td><td>0.87</td><td>0.94</td><td>2.73</td><td>3.28</td><td>3.88</td></tr></table>\nTable 1: Comparison between different speech tokenizers. Best results are marked in bold.\nTable 1 shows that our speech tokenizer achieves the highest intelligibility, which we attribute to the semantic injection with explicit supervision. It also ranks first or second on speaker similarity and speech quality metrics, even at a $1 2 . 5 \\mathrm { H z }$ frame rate, thanks to a larger quantizer that reduces quantization error and a Vocos[28]-based acoustic decoder. However, it trails Mimi on the PESQ metrics, likely because Mimi uses a massive, purely English training corpus that more closely matches the test set. It also scores lower on UTMOS than Xcodec2, due to its lower $1 2 . 5 \\mathrm { H z }$ frame rate. Overall, these results verify that our speech tokenizer can produce high-quality speech despite its low $1 2 . 5 \\mathrm { H z }$ frame rate.",
          "citations": []
        }
      ],
      "figures": [],
      "tables": [],
      "equations": []
    },
    {
      "id": "sec1-eed9be3889dd",
      "title": "4.2 Voice Cloning Evaluation",
      "type": null,
      "paragraphs": [
        {
          "id": "p-3c728ae39fa1",
          "text": "We evaluate FireRedTTS-2 for voice cloning on the \"Test-ZH\" and \"Test-EN\" sets from the SeedTTS-eval[32] benchmark, using the model after the pretraining stage. We compare it against popular monologue TTS systems, including Seed-TTS[32], F5-TTS[8], MaskedGCT[33], SparkTTS[7], and the CosyVoice series[3–5]. Results are reported in Table 2.\n<table><tr><td rowspan=\"2\">System</td><td rowspan=\"2\">Frame Rate</td><td colspan=\"2\">Test-ZH</td><td colspan=\"2\">Test-EN</td></tr><tr><td>CER ↓</td><td>SIM↑</td><td>WER ↓</td><td>SIM↑</td></tr><tr><td>Human</td><td></td><td>1.26</td><td>0.755</td><td>2.14</td><td>0.734</td></tr><tr><td>Seed-TTS</td><td></td><td>1.12</td><td>0.796</td><td>2.25</td><td>0.762</td></tr><tr><td>F5-TTS</td><td>-</td><td>1.56</td><td>0.741</td><td>1.83</td><td>0.647</td></tr><tr><td>MaskedGCT</td><td>50</td><td>2.27</td><td>0.774</td><td>2.62</td><td>0.714</td></tr><tr><td>SparkTTS</td><td>50</td><td>1.20</td><td>0.672</td><td>1.98</td><td>0.584</td></tr><tr><td>CosyVoice 3-1.5B</td><td>25</td><td>1.12</td><td>0.781</td><td>2.21</td><td>0.720</td></tr><tr><td>FireRedTTS-1S</td><td>25</td><td>1.00</td><td>0.753</td><td>2.20</td><td>0.663</td></tr><tr><td>FireRedTTS-2</td><td>12.5</td><td>1.14</td><td>0.736</td><td>1.95</td><td>0.665</td></tr></table>\nTable 2: The objective evaluation on Seed-TTS test set. Best results are marked in bold.\nOur pretrained FireRedTTS-2 achieves $1 . 1 4 \\%$ CER on Mandarin and $1 . 9 5 \\%$ WER on English, closely matching the best results of $1 . 1 2 \\%$ CER and $1 . 8 3 \\%$ WER. We attribute this to enhanced semantic information in the speech tokens, which strengthens text-to-token modeling. For speaker similarity, it aligns with human recordings in Mandarin but trails in English, likely due to limited voice diversity in the English training data. Moreover, systems such as Seed-TTS inject timbre through dedicated diffusion or flow-matching modules, further boosting similarity. Compared with previous FireRedTTS-1S, it delivers better English WER and speaker similarity, with a slight drop on Mandarin that may stem from the halved frame rate of speech tokenizer. However, as noted in [2], objective metrics cannot faithfully reflect TTS performance due to limited test set coverage and imprecise evaluation tools, and speech with more expressive prosody tends to be rated less intelligible, while plainer ones typically score higher; we therefore place greater weight on the following subjective evaluations.",
          "citations": []
        }
      ],
      "figures": [],
      "tables": [],
      "equations": []
    },
    {
      "id": "sec1-a7476e527e8b",
      "title": "4.3 Interactive Chat Evaluation",
      "type": null,
      "paragraphs": [
        {
          "id": "p-8f7919ad8ffb",
          "text": "To assess the chat fine-tuned FireRedTTS-2’s ability to infer and adjust synthesis emotions from implicit contextual cues, we built a test set with 30 test cases for each of six emotions: surprise, sadness, happiness, concern, apology, and anger. For each test case, we use the Qwen3[34] model to generate a text query–response pair that implicitly conveyed the target emotion. The text query was then synthesized into speech using random voices, and FireRedTTS-2 produced the speech response. We manually label the emotion of generated speech response and calculate the emotion control accuracy. The results in Table 3 show that FireRedTTS-2 can infer appropriate emotions from implicit contextual cues by leveraging preceding text and speech context, thereby enabling a more human-like chat experience and validating the effectiveness of our approach.\n<table><tr><td rowspan=\"2\">Model</td><td colspan=\"6\">Emotion Accuracy ↑</td></tr><tr><td>Surprised</td><td>Sad</td><td>Happy</td><td>Concern</td><td>Apology</td><td>Angry</td></tr><tr><td>FireRedTTS-2</td><td>83.3%</td><td>86.7%</td><td>90.0%</td><td>86.7%</td><td>93.3%</td><td>76.7%</td></tr></table>\nTable 3: Emotion control accuracy of FireRedTTS-2 after fine-tuning for interactive chat scenario.",
          "citations": []
        }
      ],
      "figures": [],
      "tables": [],
      "equations": []
    },
    {
      "id": "sec1-4ed5ae96b332",
      "title": "4.4 Podcast Generation Evaluation",
      "type": null,
      "paragraphs": [
        {
          "id": "p-0212d69b9ec4",
          "text": "To evaluate zero-shot podcast generation, we curated two two-speaker podcast evaluation sets: dialogue-zh and dialogue-en, containing 100 Mandarin and 115 English dialogues, respectively. Each dialogue test set spans 4 to 10 turns, totaling 1.67 hours for Mandarin and 2.35 hours for English. For each dialogue, we use the first two turns as the prompt and generate the remaining turns with the post-trained FireRedTTS-2 model.\nWe assess the generated dialogues using three objective metrics: intelligibility, speaker similarity, and Mel-cepstral distortion $( \\mathrm { M C D } ) ^ { 6 }$ . For intelligibility, we use Whisper-large- $\\dot { { \\mathbf v } } 3 ^ { 7 }$ to compute word error rate (WER) for English and Paraformer- $\\cdot z \\mathrm { h } ^ { 8 }$ to compute character error rate (CER) for Mandarin. Speaker similarity is measured with the WavLM-Large model9. For subjective evaluation, we conduct a comparative mean opinion score (CMOS) test in which raters choose the more natural synthesized dialogue between FireRedTTS-2 and competing models. We compare against opensource dialogue TTS systems including MoonCast, ZipVoice-Dialog, and MOSS-TTSD10. Because MoonCast, ZipVoice-Dialog, and MOSS-TTSD produce a single mixed-track dialogue containing both voices, we use Pyannote[35, 36] to segment each speaker before evaluation. The results are listed in Table 4.\nTable 4: Objective and subjective evaluation of zero-shot podcast generation.\n<table><tr><td rowspan=\"2\">Model</td><td colspan=\"4\">dialogue-zh</td><td colspan=\"4\">dialogue-en</td></tr><tr><td>CER↓</td><td>SIM↑</td><td>MCD↓</td><td>CMOS↑</td><td>WER↓</td><td>SIM↑</td><td>MCD↓</td><td>CMOS↑</td></tr><tr><td>MoonCast</td><td>3.81</td><td>0.658</td><td>11.37</td><td>-0.21</td><td>3.81</td><td>0.620</td><td>10.96</td><td>-0.21</td></tr><tr><td>ZipVoice-Dialog</td><td>2.93</td><td>0.736</td><td>9.29</td><td>-0.18</td><td>11.71</td><td>0.701</td><td>9.88</td><td>-0.31</td></tr><tr><td>MOSS-TTSD</td><td>3.99</td><td>0.659</td><td>8.32</td><td>-0.16</td><td>5.43</td><td>0.550</td><td>9.25</td><td>-0.13</td></tr><tr><td>FireRedTTS-2</td><td>2.08</td><td>0.753</td><td>7.99</td><td>0.0</td><td>3.16</td><td>0.703</td><td>9.06</td><td>0.0</td></tr></table>\nTable 4 shows that FireRedTTS-2 delivers the most stable synthesis, achieving the lowest WER/CER on dialogue-zh and dialogue-en, suggesting that our lower frame rate and semantically enhanced speech tokenizer enable robust modeling of long speech sequences. It also attains the highest speaker similarity, reflecting strong cross-turn voice cloning and reliable speaker transitions, and the lowest MCD, indicating minimal deviation from ground truth; CMOS results further confirm its contextually coherent naturalness and highlight the effectiveness of the dual-transformer’s context-learning capabilities.\nFigure 4: Subjective preference results between FireRedTTS-2 fine-tuned on two podcast speakers and ground truth recordings. \"Win\": FireRedTTS-2 synthesis is more natural than ground truth dialogue speech; \"Even\": indistinguishable; \"Fail\": ground truth is more natural.\nWe also evaluate the intelligibility and naturalness of the FireRedTTS-2 model fine-tuned on two podcast speakers using the dialogue-zh set. For intelligibility, the fine-tuned model maintains stable synthesis performance and achieves a lower CER of $1 . 6 6 \\%$ than in zero-shot mode. For naturalness, we conduct a subjective test in which raters are asked to choose the more natural sample between",
          "citations": []
        }
      ],
      "figures": [],
      "tables": [],
      "equations": []
    }
  ],
  "chunks": [
    {
      "id": "chk-d6e2f5a6744b",
      "page": null,
      "section": "FireRedTTS-2: Towards Long Conversational Speech Generation for Podcast and Chatbot",
      "title": null,
      "text": "Kun Xie∗, Feiyu Shen∗, Junjie Li, Fenglong Xie†, Xu Tang, Yao Hu Xiaohongshu"
    },
    {
      "id": "chk-d50792c02fc2",
      "page": null,
      "section": "Abstract",
      "title": null,
      "text": "Current dialogue generation approaches typically require the complete dialogue text before synthesis and produce a single, inseparable speech containing all voices, making them unsuitable for interactive chat; moreover, they suffer from unstable synthesis, inaccurate speaker transitions, and incoherent prosody. In this work, we present FireRedTTS-2, a long-form streaming TTS system for multi-speaker dialogue generation, delivering stable, natural speech with reliable speaker switching and context-aware prosody. A new $1 2 . 5 \\mathrm { H z }$ streaming speech tokenizer accelerates training and inference, extends maximum dialogue length, encodes richer semantics to stabilize text-to-token modeling and supports high-fidelity streaming generation for real-time applications. We adopt a text–speech interleaved format, concatenating speaker-labeled text with aligned speech tokens in chronological order, and model it with a dual-transformer: a large decoder-only transformer predicts tokens at the first layer, and a smaller one completes subsequent layers. Experimental results show that FireRedTTS-2 integrates seamlessly with chat frameworks and, with minimal fine-tuning, produces emotionally expressive speech guided by implicit contextual cues. In podcast generation, it surpasses existing systems including MoonCast, Zipvoice-Dialogue, and MOSS-TTSD in objective intelligibility, speaker-turn reliability, and perceived naturalness with context-consistent prosody. Our demos are available at https://fireredteam.github.io/demos/firered_tts_2."
    },
    {
      "id": "chk-458556619b8e",
      "page": null,
      "section": "1 Introduction",
      "title": null,
      "text": "Large language model (LLM) based text-to-speech (TTS) systems can generate natural-sounding speech with zero-shot voice cloning and are widely used for monologue applications like video dubbing. These systems typically follow one of two modeling paradigms: an autoregressive, decoderonly transformer that predicts speech tokens[1–7], or a non-autoregressive flow-matching model that produces mel-spectrograms directly from text[8, 9]. While these monologue TTS systems can be adapted to dialogue generation by segmenting dialogue text and synthesizing each fragment independently[10–12], this strategy ignores preceding text and speech context, leading to a loss of conversational coherence.\nRecent works have extended TTS system to two-speaker dialogue generation, which can be grouped into three categories based on how text and speech are organized across turns: (1) splitting the dialogue text into two parallel channels and synthesizing a single mixed speech track containing both voices, which can naturally handles overlapping speech and generate interjections effects[13, 14]; (2) concatenating the dialogue text in chronological order with each utterance prefixed by a speaker label, which likewise produces a mixed speech track[15–20]; and (3) interleaving the text and speech of each utterance[21]. Approaches (1) and (2) require the complete dialogue text before synthesis and yield a single inseparable mixed speech, limiting their suitability for interactive scenarios such as chat, whereas (3) supports flexible sentence-by-sentence generation, suitable for both interactive chat and podcast production.\nIn this work, we present FireRedTTS-2, a long-form, streaming TTS system for multi-speaker dialogue and podcast generation that delivers stable, natural speech, reliable speaker switching, and context-aware prosody. A new streaming $1 2 . 5 \\mathrm { H z }$ speech tokenizer accelerates training and inference, lengthens the effective dialogue context, encodes richer semantics to stabilize text-to-token modeling and supports high-fidelity streaming generation for real-time applications. We adopt an interleaved text–speech format by concatenating speaker-labeled text with speech tokens in chronological order, and model it with a dual-transformer architecture: a large decoder-only network predicts tokens at the first layer, while a smaller network refines the subsequent layers. Experimental results show that FireRedTTS-2 integrates seamlessly with chat frameworks and, with minimal fine-tuning, produces emotionally expressive speech guided by implicit context. In podcast generation, it surpasses the state of the art systems including MoonCast[15], ZipVoice-Dialogue[18], and MOSS-TTSD[19] in objective intelligibility, speaker-turn reliability, and perceived naturalness, while maintaining prosody consistent with long-range context."
    },
    {
      "id": "chk-638c7b029a6c",
      "page": null,
      "section": "2 FireRedTTS-2",
      "title": null,
      "text": "As shown in Figure 1, FireRedTTS-2 consists of a newly developed speech tokenizer and a text-tospeech model with perception to previous text and speech context.\nFigure 1: An overview of FireRedTTS-2, including: (a) a new speech tokenizer with a $1 2 . 5 \\mathrm { H z }$ frame rate and enhanced semantic information, and (b) a text-to-speech model using a dual-transformer architecture with interleaved text–speech input, enabling sentence-by-sentence generation and contextually coherent prosody."
    },
    {
      "id": "chk-be804529c67c",
      "page": null,
      "section": "2.1 Speech Tokenizer",
      "title": null,
      "text": "We design our speech tokenizer to enhance dialogue modeling, with a focus on long, multi-speaker speech sequence. To make such sequences tractable, we reduce the frame rate to $1 2 . 5 \\mathrm { H z }$ , half that of most open-source tokenizers[3–5, 22–24]. We further employ semantic injection and supervision to simplify text-to-token modeling, which has been shown to improve synthesis stability[19, 22–25]. In addition, the tokenizer supports high-fidelity streaming generation for real-time applications.\nAs illustrated in Figure 1(a), our speech tokenizer employs a pretrained Whisper[26] encoder to extract semantic features from the 16kHz input speech. These semantic features are encoded by an adapter and then concatenated with acoustic features from a trainable acoustic encoder structurally identical to the Whisper encoder. The combined features undergo 4 times downsampling from $5 0 \\mathrm { H z }$ to $1 2 . 5 \\mathrm { H z }$ and are discretized by a residual vector quantizer (RVQ)[27] with 16 layers, each containing 2048 code entries. The quantized features are upsampled to $5 0 \\mathrm { H z }$ and fed to a semantic decoder to predict the original semantic features derived from the pretrained Whisper encoder. The same upsampled features are also used by a Vocos[28]-based acoustic decoder to reconstruct the waveform. Depending on the reception fields of its inner convolution and attention layers, the acoustic decoder can be implemented as either streaming or non-streaming.\nTo balance generalization capability and speech quality, we train our speech tokenizer in two stages similar to[19]. First, the acoustic decoder is implemented as non-streaming and optimized to predict 16kHz speech. We use approximately $5 0 0 \\mathrm { k }$ hours of speech data and train the model for $3 2 0 \\mathrm { k }$ steps on 32 H800 GPUs, with each sample randomly cropped to 6 seconds. For the final $3 5 \\mathrm { k }$ steps, we incorporate the perceptual loss[23, 29] to further improve semantic details. In the second stage, we freeze the encoding part and replace the acoustic decoder with a fully streaming variant that predicts 24kHz speech. We continue to train the speech tokenizer on a subset of 60k hours high-fidelity speech data for 80k steps."
    },
    {
      "id": "chk-2be6bc576f5c",
      "page": null,
      "section": "2.2 Text-to-Speech Model",
      "title": null,
      "text": "Building on the new speech tokenizer, we employ a dual-transformer architecture akin to [21, 25] that operates on a text–speech interleaved sequence, enabling flexible sentence-bysentence generation and reducing first-packet latency. As illustrated in Figure 1(b), each dialogue text is prefixed with a speaker tag (e.g., \"[S1]\") and concatenated with its corresponding speech tokens; these segments are then joined in temporal order to form sequences such as \"[S1]<text><audio>[S2]<text><audio>[S3]<text><audio>...\". Existing approaches[16, 17, 19] model multi-layer speech tokens using the delay-pattern[30]: for $N$ token layers, the $i ^ { \\mathrm { t h } }$ layer is shifted $i - 1$ timesteps to the right, and $N$ prediction heads predict these shifted layers in parallel. This design has two main drawbacks: first, at each timestep the model has only partial access to the speech tokens from previous steps due to the rightward shifts, weakening contextual conditioning; second, obtaining the complete set of $N$ layer tokens for the first timestep requires $N$ autoregressive steps, resulting in high latency. To overcome these issues, we adopt a dual-transformer architecture comprising a backbone transformer that processes the text–speech interleaved sequence and predicts the first-layer tokens, and a smaller decoder transformer that generates remaining token layers. Both transformers are based on Qwen2.5[31] structure. At each timestep, the decoder consumes both the predicted first layer token and the backbone’s hidden states, which provide complete contextual information. Comparing with the delay-pattern, it requires one auto-regressive inference step of the backbone transformer and $N - 1$ steps of the smaller decoder, reducing computation and first-packet latency. Moreover, our speech tokenizer produces high-fidelity speech in a streaming manner without requiring separate token-to-speech modules, simplifying the overall system.\nThe text-to-speech model is optimized with the following loss function:"
    },
    {
      "id": "chk-43df846b2ab2",
      "page": null,
      "section": "2.2 Text-to-Speech Model",
      "title": null,
      "text": "[eq:eq-7768ee53c13d-1]\n$$\n\\mathcal{ L } _ { l o s s } = 2 * ( ( 1 - \\lambda_ { d e c o d e r } ) \\mathcal{ L } _ { b a c k b o n e } + \\lambda_ { d e c o d e r } \\mathcal{ L } _ { d e c o d e r } ) + \\lambda_ { t e x t } \\mathcal{ L } _ { t e x t }\n$$\nHere, $\\mathcal { L } _ { b a c k b o n e }$ and $\\mathcal { L } _ { d e c o d e r }$ denote the cross-entropy loss of the backbone and decoder transformer respectively. To improve training efficiency, we optimize the decoder transformer only on 1/8 of the speech segments in the interleaved sequence. Additionally, we incorporate a cross-entropy loss for the textual part $( \\mathcal { L } _ { t e x t } )$ to stabilize training. In our experiments, we set $\\lambda _ { t e x t } = 0 . 0 1$ and $\\lambda _ { d e c o d e r } = 0 . 6$\nTo enable the model with dialogue generation capability, we adopt a three-stage curriculum training process utilized in [13, 19], comprising pretraining, post-training, and supervised fine-tuning (SFT). The pretraining stage leverages 1.1M hours of monologue speech data and trains the model for 2 epochs to build foundational text-to-speech ability. Subsequently, we post-train FireRedTTS-2 for 5 epochs on $3 0 0 \\mathrm { k }$ hours of multi-speaker dialogue data, with each dialogue containing 2 to 5 speakers, to enable robust multi-speaker dialogue generation. Finally, the SFT stage is applied to tailor the model to specific voices with minimal data."
    },
    {
      "id": "chk-09600686b9d6",
      "page": null,
      "section": "3 Downstream Applications",
      "title": null,
      "text": "FireRedTTS-2 excels at both monologue and dialogue speech generation. For monologues, it offers competitive zero-shot voice cloning suitable for tasks like video dubbing. For dialogues generation, it surpasses monologue TTS systems due to its perception of text and speech context, producing speech with coherent prosody. Compared with existing dialogue TTS systems, it supports sentence-bysentence generation, enabling both interactive chat and offline podcast production. Across both modes, FireRedTTS-2 can be tailored to specific application requirements with minimal data, demonstrating strong flexibility."
    },
    {
      "id": "chk-602ab174fce9",
      "page": null,
      "section": "3.1 Voice Cloning",
      "title": null,
      "text": "Our speech tokenizer captures both semantic and acoustic information, enabling fine-grained acoustic modeling. Paired with large-scale pretraining on monologue speech, it allows FireRedTTS-2 to deliver robust zero-shot voice cloning. Given a speech prompt and its transcript, we concatenate the prompt transcript, the target text to be synthesized, and the discretized prompt speech tokens, then feed this sequence into the text-to-speech model to autoregressively generate new speech tokens. The generated tokens are appended to the prompt tokens and decoded into a waveform by the speech tokenizer’s decoder, after which the portion corresponding to the original prompt is removed."
    },
    {
      "id": "chk-65ffdf0be50a",
      "page": null,
      "section": "3.2 Interactive Chat",
      "title": null,
      "text": "Current interactive chat frameworks[10, 11] typically rely on monologue TTS systems, which lack perception of prior user queries and system responses, often resulting in inconsistent emotion and prosody. This can be partially mitigated with explicit instructions such as emotion labels, but it requires additional fine-tuning of the text LLM and adds unnecessary complexity.\nFigure 2: Integration of FireRedTTS-2 into interactive chat scenarios.\nAs shown in Figure 2, FireRedTTS-2 can be seamlessly integrated into existing chat frameworks without modifying other modules. To address the inconsistency issue, we fine-tune the post-trained FireRedTTS-2 to infer and adjust emotion and prosody from implicit contextual cues. Specifically, we curate a 15-hour speech corpus of a distinctive female voice expressing six emotions: surprise, sadness, happiness, concern, apology, and anger. Then we emulate conversational context by first generating text context with a text LLM and then synthesizing it into speech. After fine-tuning, FireRedTTS-2 dynamically shifts emotion and tone in response to preceding chat history, delivering a near-human interactive experience."
    },
    {
      "id": "chk-66e51df018ec",
      "page": null,
      "section": "3.3 Podcast Generation",
      "title": null,
      "text": "Subsequent post-training on dialogue corpus equips FireRedTTS-2 with conversational generation abilities, making it well-suited for podcast generation. Compared with conventional segmenting approaches that utlizes monologue TTS systems, it simplifies the workflow and can synthesizes contextually coherent prosody. Moreover, it generates dialogue speech sentence by sentence, providing greater flexibility for editing and post-processing.\nAs shown in Figure 3, FireRedTTS-2 can perform zero-shot podcast generation by placing two dialogue turns as prompt context and then generating subsequent turns one by one. It currently supports 3 minutes dialogues with 4 speakers and can be easily scaled to longer conversations with more speakers by extending training corpus. It can also be tailored to specific speakers with minimal data. In this work, we collected approximately 50 hours of dialogue speech featuring a male and a female podcast host and fine-tuned the post-trained model for 15 epochs. Once customized,\nFigure 3: Zero-shot podcast generation of FireRedTTS-2.\nFireRedTTS-2 delivers stable synthesis, accurate speaker transitions, and contextually coherent prosody that matches the hosts’ distinctive speaking styles."
    },
    {
      "id": "chk-66736fb2deff",
      "page": null,
      "section": "4.1 Speech Tokenizer Evaluation",
      "title": null,
      "text": "We evaluate our speech tokenizer on intelligibility, speaker similarity, and speech quality of the reconstructed speech using the LibriSpeech test-clean set, which contains 2,620 utterances at $1 6 \\mathrm { k H z }$ . Speech intelligibility is measured by word error rate (WER) using a HuBERT-based automatic speech recognition (ASR) system3. Speech quality is assessed with speaker similarity (SPK-SIM) computed by the WavLM-Large model 4, Short-Time Objective Intelligibility (STOI), Perceptual Evaluation of Speech Quality (PESQ), and UTMOS 5. We compare our speech tokenizer with other methods that likewise incorporate semantic injection and supervision, including XY-Tokenizer[19], XCodec2[23], SpeechTokenizer[24], and Mimi[25]; results are reported in Table 1.\n<table><tr><td>Models</td><td>BPS</td><td>Frame Rate</td><td>WER↓</td><td>SPK SIM</td><td>STOI↑</td><td>PESQ↑ WB</td><td>PESQ↑ NB</td><td>UT</td></tr><tr><td>Ground Truth</td><td>-</td><td>-</td><td>1.96</td><td>-</td><td>1.00</td><td>4.64</td><td>4.55</td><td>4.09</td></tr><tr><td>Xcodec2</td><td>800</td><td>50</td><td>2.46</td><td>0.82</td><td>0.92</td><td>2.43</td><td>3.04</td><td>4.13</td></tr><tr><td>XY-Tokneizer</td><td>1000</td><td>12.5</td><td>-</td><td>0.83</td><td>0.91</td><td>2.41</td><td>3.00</td><td>-</td></tr><tr><td>SpeechTokenizer</td><td>2000</td><td>50</td><td>2.86</td><td>0.66</td><td>0.88</td><td>1.92</td><td>2.38</td><td>3.56</td></tr><tr><td>Mimi</td><td>2200</td><td>12.5</td><td>2.26</td><td>0.87</td><td>0.94</td><td>2.88</td><td>3.42</td><td>3.87</td></tr><tr><td>Ours</td><td>2200</td><td>12.5</td><td>2.16</td><td>0.87</td><td>0.94</td><td>2.73</td><td>3.28</td><td>3.88</td></tr></table>\nTable 1: Comparison between different speech tokenizers. Best results are marked in bold.\nTable 1 shows that our speech tokenizer achieves the highest intelligibility, which we attribute to the semantic injection with explicit supervision. It also ranks first or second on speaker similarity and speech quality metrics, even at a $1 2 . 5 \\mathrm { H z }$ frame rate, thanks to a larger quantizer that reduces quantization error and a Vocos[28]-based acoustic decoder. However, it trails Mimi on the PESQ metrics, likely because Mimi uses a massive, purely English training corpus that more closely matches the test set. It also scores lower on UTMOS than Xcodec2, due to its lower $1 2 . 5 \\mathrm { H z }$ frame rate. Overall, these results verify that our speech tokenizer can produce high-quality speech despite its low $1 2 . 5 \\mathrm { H z }$ frame rate."
    },
    {
      "id": "chk-1b131fadc09d",
      "page": null,
      "section": "4.2 Voice Cloning Evaluation",
      "title": null,
      "text": "We evaluate FireRedTTS-2 for voice cloning on the \"Test-ZH\" and \"Test-EN\" sets from the SeedTTS-eval[32] benchmark, using the model after the pretraining stage. We compare it against popular monologue TTS systems, including Seed-TTS[32], F5-TTS[8], MaskedGCT[33], SparkTTS[7], and the CosyVoice series[3–5]. Results are reported in Table 2.\n<table><tr><td rowspan=\"2\">System</td><td rowspan=\"2\">Frame Rate</td><td colspan=\"2\">Test-ZH</td><td colspan=\"2\">Test-EN</td></tr><tr><td>CER ↓</td><td>SIM↑</td><td>WER ↓</td><td>SIM↑</td></tr><tr><td>Human</td><td></td><td>1.26</td><td>0.755</td><td>2.14</td><td>0.734</td></tr><tr><td>Seed-TTS</td><td></td><td>1.12</td><td>0.796</td><td>2.25</td><td>0.762</td></tr><tr><td>F5-TTS</td><td>-</td><td>1.56</td><td>0.741</td><td>1.83</td><td>0.647</td></tr><tr><td>MaskedGCT</td><td>50</td><td>2.27</td><td>0.774</td><td>2.62</td><td>0.714</td></tr><tr><td>SparkTTS</td><td>50</td><td>1.20</td><td>0.672</td><td>1.98</td><td>0.584</td></tr><tr><td>CosyVoice 3-1.5B</td><td>25</td><td>1.12</td><td>0.781</td><td>2.21</td><td>0.720</td></tr><tr><td>FireRedTTS-1S</td><td>25</td><td>1.00</td><td>0.753</td><td>2.20</td><td>0.663</td></tr><tr><td>FireRedTTS-2</td><td>12.5</td><td>1.14</td><td>0.736</td><td>1.95</td><td>0.665</td></tr></table>\nTable 2: The objective evaluation on Seed-TTS test set. Best results are marked in bold.\nOur pretrained FireRedTTS-2 achieves $1 . 1 4 \\%$ CER on Mandarin and $1 . 9 5 \\%$ WER on English, closely matching the best results of $1 . 1 2 \\%$ CER and $1 . 8 3 \\%$ WER. We attribute this to enhanced semantic information in the speech tokens, which strengthens text-to-token modeling. For speaker similarity, it aligns with human recordings in Mandarin but trails in English, likely due to limited voice diversity in the English training data. Moreover, systems such as Seed-TTS inject timbre through dedicated diffusion or flow-matching modules, further boosting similarity. Compared with previous FireRedTTS-1S, it delivers better English WER and speaker similarity, with a slight drop on Mandarin that may stem from the halved frame rate of speech tokenizer. However, as noted in [2], objective metrics cannot faithfully reflect TTS performance due to limited test set coverage and imprecise evaluation tools, and speech with more expressive prosody tends to be rated less intelligible, while plainer ones typically score higher; we therefore place greater weight on the following subjective evaluations."
    },
    {
      "id": "chk-378a189fc557",
      "page": null,
      "section": "4.3 Interactive Chat Evaluation",
      "title": null,
      "text": "To assess the chat fine-tuned FireRedTTS-2’s ability to infer and adjust synthesis emotions from implicit contextual cues, we built a test set with 30 test cases for each of six emotions: surprise, sadness, happiness, concern, apology, and anger. For each test case, we use the Qwen3[34] model to generate a text query–response pair that implicitly conveyed the target emotion. The text query was then synthesized into speech using random voices, and FireRedTTS-2 produced the speech response. We manually label the emotion of generated speech response and calculate the emotion control accuracy. The results in Table 3 show that FireRedTTS-2 can infer appropriate emotions from implicit contextual cues by leveraging preceding text and speech context, thereby enabling a more human-like chat experience and validating the effectiveness of our approach.\n<table><tr><td rowspan=\"2\">Model</td><td colspan=\"6\">Emotion Accuracy ↑</td></tr><tr><td>Surprised</td><td>Sad</td><td>Happy</td><td>Concern</td><td>Apology</td><td>Angry</td></tr><tr><td>FireRedTTS-2</td><td>83.3%</td><td>86.7%</td><td>90.0%</td><td>86.7%</td><td>93.3%</td><td>76.7%</td></tr></table>\nTable 3: Emotion control accuracy of FireRedTTS-2 after fine-tuning for interactive chat scenario."
    },
    {
      "id": "chk-b1b932a15e0d",
      "page": null,
      "section": "4.4 Podcast Generation Evaluation",
      "title": null,
      "text": "To evaluate zero-shot podcast generation, we curated two two-speaker podcast evaluation sets: dialogue-zh and dialogue-en, containing 100 Mandarin and 115 English dialogues, respectively. Each dialogue test set spans 4 to 10 turns, totaling 1.67 hours for Mandarin and 2.35 hours for English. For each dialogue, we use the first two turns as the prompt and generate the remaining turns with the post-trained FireRedTTS-2 model.\nWe assess the generated dialogues using three objective metrics: intelligibility, speaker similarity, and Mel-cepstral distortion $( \\mathrm { M C D } ) ^ { 6 }$ . For intelligibility, we use Whisper-large- $\\dot { { \\mathbf v } } 3 ^ { 7 }$ to compute word error rate (WER) for English and Paraformer- $\\cdot z \\mathrm { h } ^ { 8 }$ to compute character error rate (CER) for Mandarin. Speaker similarity is measured with the WavLM-Large model9. For subjective evaluation, we conduct a comparative mean opinion score (CMOS) test in which raters choose the more natural synthesized dialogue between FireRedTTS-2 and competing models. We compare against opensource dialogue TTS systems including MoonCast, ZipVoice-Dialog, and MOSS-TTSD10. Because MoonCast, ZipVoice-Dialog, and MOSS-TTSD produce a single mixed-track dialogue containing both voices, we use Pyannote[35, 36] to segment each speaker before evaluation. The results are listed in Table 4.\nTable 4: Objective and subjective evaluation of zero-shot podcast generation.\n<table><tr><td rowspan=\"2\">Model</td><td colspan=\"4\">dialogue-zh</td><td colspan=\"4\">dialogue-en</td></tr><tr><td>CER↓</td><td>SIM↑</td><td>MCD↓</td><td>CMOS↑</td><td>WER↓</td><td>SIM↑</td><td>MCD↓</td><td>CMOS↑</td></tr><tr><td>MoonCast</td><td>3.81</td><td>0.658</td><td>11.37</td><td>-0.21</td><td>3.81</td><td>0.620</td><td>10.96</td><td>-0.21</td></tr><tr><td>ZipVoice-Dialog</td><td>2.93</td><td>0.736</td><td>9.29</td><td>-0.18</td><td>11.71</td><td>0.701</td><td>9.88</td><td>-0.31</td></tr><tr><td>MOSS-TTSD</td><td>3.99</td><td>0.659</td><td>8.32</td><td>-0.16</td><td>5.43</td><td>0.550</td><td>9.25</td><td>-0.13</td></tr><tr><td>FireRedTTS-2</td><td>2.08</td><td>0.753</td><td>7.99</td><td>0.0</td><td>3.16</td><td>0.703</td><td>9.06</td><td>0.0</td></tr></table>\nTable 4 shows that FireRedTTS-2 delivers the most stable synthesis, achieving the lowest WER/CER on dialogue-zh and dialogue-en, suggesting that our lower frame rate and semantically enhanced speech tokenizer enable robust modeling of long speech sequences. It also attains the highest speaker similarity, reflecting strong cross-turn voice cloning and reliable speaker transitions, and the lowest MCD, indicating minimal deviation from ground truth; CMOS results further confirm its contextually coherent naturalness and highlight the effectiveness of the dual-transformer’s context-learning capabilities.\nFigure 4: Subjective preference results between FireRedTTS-2 fine-tuned on two podcast speakers and ground truth recordings. \"Win\": FireRedTTS-2 synthesis is more natural than ground truth dialogue speech; \"Even\": indistinguishable; \"Fail\": ground truth is more natural.\nWe also evaluate the intelligibility and naturalness of the FireRedTTS-2 model fine-tuned on two podcast speakers using the dialogue-zh set. For intelligibility, the fine-tuned model maintains stable synthesis performance and achieves a lower CER of $1 . 6 6 \\%$ than in zero-shot mode. For naturalness, we conduct a subjective test in which raters are asked to choose the more natural sample between"
    }
  ],
  "figures": [],
  "tables": [],
  "equations": [
    {
      "id": "eq-7768ee53c13d-1",
      "page": null,
      "latex": "\\mathcal{ L } _ { l o s s } = 2 * ( ( 1 - \\lambda_ { d e c o d e r } ) \\mathcal{ L } _ { b a c k b o n e } + \\lambda_ { d e c o d e r } \\mathcal{ L } _ { d e c o d e r } ) + \\lambda_ { t e x t } \\mathcal{ L } _ { t e x t }",
      "has_latex": true
    }
  ],
  "references": [],
  "source": {
    "mineru_markdown_path": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/test.md",
    "mineru_source": "markdown"
  },
  "schema_version": "ep2c.paper.v1"
}