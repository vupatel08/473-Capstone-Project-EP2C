[
  {
    "id": "356e758f799e77bde76083323dc6c8cff0b791ad",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/gradio_demo.py",
    "kind": "function",
    "name": "get_args",
    "signature": "get_args()",
    "docstring": "================================================\n                  Options\n================================================",
    "identifiers": [
      "add_argument",
      "args",
      "argumentparser",
      "get_args",
      "parse_args",
      "parser",
      "required",
      "str",
      "type"
    ],
    "start_line": 349,
    "end_line": 353,
    "text": "def get_args():\n    parser = ArgumentParser()\n    parser.add_argument(\"--pretrained-dir\", type=str, required=True)\n    args = parser.parse_args()\n    return args"
  },
  {
    "id": "0204740afea3cb482e638431f2653a81d29e9ab1",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/gradio_demo.py",
    "kind": "function",
    "name": "render_interface",
    "signature": "render_interface()",
    "docstring": "UI rendering",
    "identifiers": [
      "_change_component_language",
      "_change_prompt_input_visibility",
      "audio",
      "blocks",
      "button",
      "change",
      "choices",
      "click",
      "column",
      "default",
      "dialogue_synthesis_function",
      "dialogue_text_input",
      "editable",
      "enable",
      "fn",
      "format",
      "generate_audio",
      "generate_btn",
      "global_lang",
      "gr",
      "group",
      "i18n",
      "inputs",
      "interactive",
      "label",
      "lang",
      "lang_choice",
      "lines",
      "markdown",
      "outputs",
      "page",
      "placeholder",
      "radio",
      "render_interface",
      "row",
      "scale",
      "size",
      "spk1_prompt_audio",
      "spk1_prompt_group",
      "spk1_prompt_text",
      "spk2_prompt_audio",
      "spk2_prompt_group",
      "spk2_prompt_text",
      "textbox",
      "theme",
      "themes",
      "title",
      "title_desc",
      "type",
      "update",
      "value",
      "variant",
      "visible",
      "voice_mode",
      "voice_mode_choice"
    ],
    "start_line": 202,
    "end_line": 343,
    "text": "def render_interface() -> gr.Blocks:\n    with gr.Blocks(title=\"FireRedTTS-2\", theme=gr.themes.Default()) as page:\n        # ======================== UI ========================\n        # A large title\n        title_desc = gr.Markdown(value=\"# {}\".format(i18n(\"title_md_desc\")))\n        with gr.Row():\n            lang_choice = gr.Radio(\n                choices=[\"中文\", \"English\"],\n                value=\"中文\",\n                label=\"Display Language/显示语言\",\n                type=\"index\",\n                interactive=True,\n            )\n            voice_mode_choice = gr.Radio(\n                choices=[i18n(\"voice_model_choice1\"), i18n(\"voice_model_choice2\")],\n                value=i18n(\"voice_model_choice1\"),\n                label=i18n(\"voice_mode_label\"),\n                type=\"index\",\n                interactive=True,\n            )\n        with gr.Row():\n            # ==== Speaker1 Prompt ====\n            with gr.Column(scale=1):\n                with gr.Group(visible=True) as spk1_prompt_group:\n                    spk1_prompt_audio = gr.Audio(\n                        label=i18n(\"spk1_prompt_audio_label\"),\n                        type=\"filepath\",\n                        editable=False,\n                        interactive=True,\n                    )  # Audio component returns tmp audio path\n                    spk1_prompt_text = gr.Textbox(\n                        label=i18n(\"spk1_prompt_text_label\"),\n                        placeholder=i18n(\"spk1_prompt_text_placeholder\"),\n                        lines=3,\n                    )\n            # ==== Speaker2 Prompt ====\n            with gr.Column(scale=1):\n                with gr.Group(visible=True) as spk2_prompt_group:\n                    spk2_prompt_audio = gr.Audio(\n                        label=i18n(\"spk2_prompt_audio_label\"),\n                        type=\"filepath\",\n                        editable=False,\n                        interactive=True,\n                    )\n                    spk2_prompt_text = gr.Textbox(\n                        label=i18n(\"spk2_prompt_text_label\"),\n                        placeholder=i18n(\"spk2_prompt_text_placeholder\"),\n                        lines=3,\n                    )\n            # ==== Text input ====\n            with gr.Column(scale=2):\n                dialogue_text_input = gr.Textbox(\n                    label=i18n(\"dialogue_text_input_label\"),\n                    placeholder=i18n(\"dialogue_text_input_placeholder\"),\n                    lines=18,\n                )\n        # Generate button\n        generate_btn = gr.Button(\n            value=i18n(\"generate_btn_label\"), variant=\"primary\", size=\"lg\"\n        )\n        # Long output audio\n        generate_audio = gr.Audio(\n            label=i18n(\"generated_audio_label\"),\n            interactive=False,\n        )\n\n        # ======================== Action ========================\n        # Language action\n        def _change_component_language(lang):\n            global global_lang\n            global_lang = [\"zh\", \"en\"][lang]\n            return [\n                # title_desc\n                gr.update(value=\"# {}\".format(i18n(\"title_md_desc\"))),\n                # voice_mode_choice\n                gr.update(\n                    choices=[i18n(\"voice_model_choice1\"), i18n(\"voice_model_choice2\")],\n                    value=i18n(\"voice_model_choice1\"),\n                    label=i18n(\"voice_mode_label\"),\n                ),\n                # spk1_prompt_{audio,text}\n                gr.update(label=i18n(\"spk1_prompt_audio_label\")),\n                gr.update(\n                    label=i18n(\"spk1_prompt_text_label\"),\n                    placeholder=i18n(\"spk1_prompt_text_placeholder\"),\n                ),\n                # spk2_prompt_{audio,text}\n                gr.update(label=i18n(\"spk2_prompt_audio_label\")),\n                gr.update(\n                    label=i18n(\"spk2_prompt_text_label\"),\n                    placeholder=i18n(\"spk2_prompt_text_placeholder\"),\n                ),\n                # dialogue_text"
  },
  {
    "id": "8530ee02ad417a439d36bb077c799100f20c6c78",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/gradio_demo.py",
    "kind": "function",
    "name": "_change_prompt_input_visibility",
    "signature": "_change_prompt_input_visibility(voice_mode)",
    "docstring": "Voice clone mode action",
    "identifiers": [
      "_change_prompt_input_visibility",
      "enable",
      "gr",
      "update",
      "visible",
      "voice_mode"
    ],
    "start_line": 322,
    "end_line": 324,
    "text": "def _change_prompt_input_visibility(voice_mode):\n            enable = voice_mode == 0\n            return [gr.update(visible=enable), gr.update(visible=enable)]"
  },
  {
    "id": "660bc858056f6c46a14b9121a31d2e4a16d21459",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/gradio_demo.py",
    "kind": "function",
    "name": "_change_component_language",
    "signature": "_change_component_language(lang)",
    "docstring": "======================== Action ========================\nLanguage action",
    "identifiers": [
      "_change_component_language",
      "choices",
      "format",
      "global_lang",
      "gr",
      "i18n",
      "label",
      "lang",
      "placeholder",
      "update",
      "value"
    ],
    "start_line": 270,
    "end_line": 303,
    "text": "def _change_component_language(lang):\n            global global_lang\n            global_lang = [\"zh\", \"en\"][lang]\n            return [\n                # title_desc\n                gr.update(value=\"# {}\".format(i18n(\"title_md_desc\"))),\n                # voice_mode_choice\n                gr.update(\n                    choices=[i18n(\"voice_model_choice1\"), i18n(\"voice_model_choice2\")],\n                    value=i18n(\"voice_model_choice1\"),\n                    label=i18n(\"voice_mode_label\"),\n                ),\n                # spk1_prompt_{audio,text}\n                gr.update(label=i18n(\"spk1_prompt_audio_label\")),\n                gr.update(\n                    label=i18n(\"spk1_prompt_text_label\"),\n                    placeholder=i18n(\"spk1_prompt_text_placeholder\"),\n                ),\n                # spk2_prompt_{audio,text}\n                gr.update(label=i18n(\"spk2_prompt_audio_label\")),\n                gr.update(\n                    label=i18n(\"spk2_prompt_text_label\"),\n                    placeholder=i18n(\"spk2_prompt_text_placeholder\"),\n                ),\n                # dialogue_text_input\n                gr.update(\n                    label=i18n(\"dialogue_text_input_label\"),\n                    placeholder=i18n(\"dialogue_text_input_placeholder\"),\n                ),\n                # generate_btn\n                gr.update(value=i18n(\"generate_btn_label\")),\n                # generate_audio\n                gr.update(label=i18n(\"generated_audio_label\")),\n            ]"
  },
  {
    "id": "a64ae1cf32e3bfb85f556069a44a164ff5c499a2",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/gradio_demo.py",
    "kind": "function",
    "name": "dialogue_synthesis_function",
    "signature": "dialogue_synthesis_function(\n    target_text: str,\n    voice_mode: Literal[0, 1] = 0,  # 0 means voice clone\n    spk1_prompt_text: str | None = \"\",\n    spk1_prompt_audio: str | None = None,\n    spk2_prompt_text: str | None = \"\",\n    spk2_prompt_audio: str | None = None,\n)",
    "docstring": "",
    "identifiers": [
      "all",
      "check_dialogue_text",
      "check_monologue_text",
      "dialogue_synthesis_function",
      "findall",
      "generate_dialogue",
      "gr",
      "i18n",
      "list",
      "literal",
      "message",
      "model",
      "numpy",
      "progress",
      "progress_bar",
      "prompt_has_value",
      "prompt_text_list",
      "prompt_wav_list",
      "re",
      "spk1_prompt_audio",
      "spk1_prompt_text",
      "spk2_prompt_audio",
      "spk2_prompt_text",
      "squeeze",
      "str",
      "strip",
      "target_audio",
      "target_text",
      "target_text_list",
      "temperature",
      "text",
      "text_list",
      "topk",
      "track_tqdm",
      "voice_mode",
      "warning"
    ],
    "start_line": 153,
    "end_line": 198,
    "text": "def dialogue_synthesis_function(\n    target_text: str,\n    voice_mode: Literal[0, 1] = 0,  # 0 means voice clone\n    spk1_prompt_text: str | None = \"\",\n    spk1_prompt_audio: str | None = None,\n    spk2_prompt_text: str | None = \"\",\n    spk2_prompt_audio: str | None = None,\n):\n    # Voice clone mode, check prompt info\n    if voice_mode == 0:\n        prompt_has_value = [\n            spk1_prompt_text != \"\",\n            spk1_prompt_audio is not None,\n            spk2_prompt_text != \"\",\n            spk2_prompt_audio is not None,\n        ]\n        if not all(prompt_has_value):\n            gr.Warning(message=i18n(\"warn_incomplete_prompt\"))\n            return None\n        if not check_monologue_text(spk1_prompt_text, \"[S1]\"):\n            gr.Warning(message=i18n(\"warn_invalid_spk1_prompt_text\"))\n            return None\n        if not check_monologue_text(spk2_prompt_text, \"[S2]\"):\n            gr.Warning(message=i18n(\"warn_invalid_spk2_prompt_text\"))\n            return None\n    # Check dialogue text\n    target_text_list: List[str] = re.findall(r\"(\\[S[0-9]\\][^\\[\\]]*)\", target_text)\n    target_text_list = [text.strip() for text in target_text_list]\n    if not check_dialogue_text(target_text_list):\n        gr.Warning(message=i18n(\"warn_invalid_dialogue_text\"))\n        return None\n\n    # Go synthesis\n    progress_bar = gr.Progress(track_tqdm=True)\n    prompt_wav_list = (\n        None if voice_mode != 0 else [spk1_prompt_audio, spk2_prompt_audio]\n    )\n    prompt_text_list = None if voice_mode != 0 else [spk1_prompt_text, spk2_prompt_text]\n    target_audio = model.generate_dialogue(\n        text_list=target_text_list,\n        prompt_wav_list=prompt_wav_list,\n        prompt_text_list=prompt_text_list,\n        temperature=0.9,\n        topk=30,\n    )\n    return (24000, target_audio.squeeze(0).numpy())"
  },
  {
    "id": "5ebdb5ee170204cc5c43d922600a3bfcf6203348",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/gradio_demo.py",
    "kind": "function",
    "name": "check_dialogue_text",
    "signature": "check_dialogue_text(text_list: List[str])",
    "docstring": "",
    "identifiers": [
      "bool",
      "check_dialogue_text",
      "check_monologue_text",
      "len",
      "list",
      "str",
      "text",
      "text_list"
    ],
    "start_line": 139,
    "end_line": 150,
    "text": "def check_dialogue_text(text_list: List[str]) -> bool:\n    if len(text_list) == 0:\n        return False\n    for text in text_list:\n        if not (\n            check_monologue_text(text, \"[S1]\")\n            or check_monologue_text(text, \"[S2]\")\n            or check_monologue_text(text, \"[S3]\")\n            or check_monologue_text(text, \"[S4]\")\n        ):\n            return False\n    return True"
  },
  {
    "id": "691e823218f0736191e1998cc1dd9513094abf03",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/gradio_demo.py",
    "kind": "function",
    "name": "check_monologue_text",
    "signature": "check_monologue_text(text: str, prefix: str = None)",
    "docstring": "",
    "identifiers": [
      "bool",
      "check_monologue_text",
      "len",
      "prefix",
      "removeprefix",
      "startswith",
      "str",
      "strip",
      "text"
    ],
    "start_line": 124,
    "end_line": 136,
    "text": "def check_monologue_text(text: str, prefix: str = None) -> bool:\n    text = text.strip()\n    # Check speaker tags\n    if prefix is not None and (not text.startswith(prefix)):\n        return False\n    # Remove prefix\n    if prefix is not None:\n        text = text.removeprefix(prefix)\n    text = text.strip()\n    # If empty?\n    if len(text) == 0:\n        return False\n    return True"
  },
  {
    "id": "f6ff30f88e745ad969bc19cf2565fb758093bb86",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/gradio_demo.py",
    "kind": "function",
    "name": "i18n",
    "signature": "i18n(key)",
    "docstring": "",
    "identifiers": [
      "_i18n_key2lang_dict",
      "global_lang",
      "i18n",
      "key"
    ],
    "start_line": 119,
    "end_line": 121,
    "text": "def i18n(key):\n    global global_lang\n    return _i18n_key2lang_dict[key][global_lang]"
  },
  {
    "id": "193523d54027e649adb70aed70acbd2c1b9aaec3",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/gradio_demo.py",
    "kind": "function",
    "name": "initiate_model",
    "signature": "initiate_model(pretrained_dir: str, device=\"cuda\")",
    "docstring": "",
    "identifiers": [
      "device",
      "fireredtts2",
      "gen_type",
      "initiate_model",
      "model",
      "pretrained_dir",
      "str"
    ],
    "start_line": 16,
    "end_line": 23,
    "text": "def initiate_model(pretrained_dir: str, device=\"cuda\"):\n    global model\n    if model is None:\n        model = FireRedTTS2(\n            pretrained_dir=pretrained_dir,\n            gen_type=\"dialogue\",\n            device=device,\n        )"
  },
  {
    "id": "092279f71202e374121d7ca1014d35d951f8343e",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/fireredtts2.py",
    "kind": "class",
    "name": "FireRedTTS2_Stream",
    "signature": "class FireRedTTS2_Stream(FireRedTTS2)",
    "docstring": "",
    "identifiers": [
      "_",
      "_audio_tokenizer",
      "_model",
      "_tokenize_segment",
      "_tokenize_text_segment",
      "all",
      "all_generated_segments",
      "append",
      "arange",
      "audio",
      "audio_16k",
      "audio_chunk",
      "audio_generator",
      "audio_path",
      "audio_tensor",
      "bool",
      "cat",
      "clean_text",
      "codec_cache",
      "context",
      "cpu",
      "curr_pos",
      "curr_tokens",
      "curr_tokens_mask",
      "decode_one_token",
      "device",
      "dim",
      "exists",
      "fireredtts2",
      "fireredtts2_stream",
      "float",
      "format",
      "functional",
      "gen_segment_tokens",
      "gen_segment_tokens_mask",
      "generate",
      "generate_dialogue",
      "generate_frame",
      "generate_monologue",
      "generate_single",
      "i",
      "inference_mode",
      "input_text",
      "int",
      "last_token",
      "len",
      "length",
      "list",
      "long",
      "max_audio_length_ms",
      "max_context_len",
      "max_generation_len",
      "max_seq_len",
      "ones_like",
      "os",
      "path",
      "prepare_prompt",
      "prev_sample",
      "print",
      "process_text_list",
      "prompt_a",
      "prompt_segments",
      "prompt_text",
      "prompt_text_list",
      "prompt_tokens",
      "prompt_tokens_mask",
      "prompt_wav",
      "prompt_wav_list",
      "range",
      "resample",
      "reset_caches",
      "sample",
      "segment",
      "segment_tokens",
      "segment_tokens_mask",
      "self",
      "size",
      "speaker",
      "split_text",
      "squeeze",
      "str",
      "strip",
      "temperature",
      "tensor",
      "text",
      "text_list",
      "to",
      "token",
      "token_generator",
      "tokens",
      "tokens_mask",
      "topk",
      "torch",
      "torchaudio",
      "tqdm",
      "unsqueeze",
      "valueerror",
      "zeros"
    ],
    "start_line": 383,
    "end_line": 626,
    "text": "class FireRedTTS2_Stream(FireRedTTS2):\n\n    def generate(\n        self,\n        text: str,\n        speaker: str,\n        context: List[Segment],\n        max_audio_length_ms: float = 90_000,\n        temperature: float = 0.9,\n        topk: int = 20,\n    ):\n        self._model.reset_caches()\n\n        max_generation_len = int(max_audio_length_ms / 80)\n        tokens, tokens_mask = [], []\n        for segment in context:\n            segment_tokens, segment_tokens_mask = self._tokenize_segment(segment)\n            tokens.append(segment_tokens)\n            tokens_mask.append(segment_tokens_mask)\n\n        gen_segment_tokens, gen_segment_tokens_mask = self._tokenize_text_segment(\n            text, speaker\n        )\n        tokens.append(gen_segment_tokens)\n        tokens_mask.append(gen_segment_tokens_mask)\n\n        prompt_tokens = torch.cat(tokens, dim=0).long().to(self.device)\n        prompt_tokens_mask = torch.cat(tokens_mask, dim=0).bool().to(self.device)\n\n        curr_tokens = prompt_tokens.unsqueeze(0)\n        curr_tokens_mask = prompt_tokens_mask.unsqueeze(0)\n        curr_pos = (\n            torch.arange(0, prompt_tokens.size(0)).unsqueeze(0).long().to(self.device)\n        )\n\n        max_seq_len = 3100\n        max_context_len = max_seq_len - max_generation_len\n        if curr_tokens.size(1) >= max_context_len:\n            raise ValueError(\n                f\"Inputs too long, must be below max_seq_len - max_generation_len: {max_context_len}\"\n            )\n\n        # for streaming token2audio\n        codec_cache = {}\n        prev_sample = None\n        for _ in range(max_generation_len):\n            sample = self._model.generate_frame(\n                curr_tokens, curr_tokens_mask, curr_pos, temperature, topk\n            )\n            # eos\n            if torch.all(sample == 0):\n                break\n            \n            # token2audio one step\n            if prev_sample is None:\n                prev_sample = sample\n            else:\n                audio_chunk, codec_cache = self._audio_tokenizer.decode_one_token(\n                    prev_sample.unsqueeze(-1),\n                    codec_cache,\n                    last_token=False,\n                )\n                prev_sample = sample\n                yield audio_chunk.squeeze(0)\n\n            curr_tokens = torch.cat(\n                [sample, torch.zeros(1, 1).long().to(self.device)], dim=1\n            ).unsqueeze(1)\n            curr_tokens_mask = torch.cat(\n                [\n                    torch.ones_like(sample).bool(),\n                    torch.zeros(1, 1).bool().to(self.device),\n                ],\n                dim=1,\n            ).unsqueeze(1)\n            curr_pos = curr_pos[:, -1:] + 1\n\n        audio_chunk, codec_cache = self._audio_tokenizer.decode_one_token(\n            prev_sample.unsqueeze(-1),\n            codec_cache,\n            last_token=True,\n        )\n        yield audio_chunk.squeeze(0)\n\n    def generate_single(\n        self, context: List[Segment], temperature: float = 0.9, topk: int = 20\n    ):\n        self._model.reset_caches()\n        max_generation_len = 400\n        tokens, tokens_mask = [], []\n        for segment in context:\n            segment_tokens, segment_tokens_mask = self._tokenize_segment(segment)\n            tokens.append(segment_tokens)\n            tokens_mask.append(segment_tokens_mask)\n\n        prompt_tokens = torch.cat(tokens, dim=0).long().to(self.device)\n        prompt_tokens_mask = torch.cat(tokens_mask, dim=0).bool().to(self.device)\n        prompt_tokens = prompt_tokens[:-3, :]\n        prompt_tokens_mask = prompt_tokens_mask[:-3, :]\n\n        curr_tokens = prompt_tokens.unsqueeze(0)\n        curr_tokens_mask = prompt_tokens_mask.unsqueeze(0)\n        curr_pos = (\n            torch.arange(0, prompt_tokens.size(0)).unsqueeze(0).long().to(self.device)\n        )\n\n        for _ in range(max_generation_len):\n            # sample: (1, nq)\n            sample = self._model.generate_frame(\n                curr_tokens, curr_tokens_mask, curr_pos, tem"
  },
  {
    "id": "e8bb76c73766edac30d09f8e852f3d7ae154f9fc",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/fireredtts2.py",
    "kind": "function",
    "name": "generate_monologue",
    "signature": "generate_monologue(\n        self, text, prompt_wav=None, prompt_text=None, temperature=0.75, topk=20\n    )",
    "docstring": "",
    "identifiers": [
      "_audio_tokenizer",
      "append",
      "audio_chunk",
      "audio_generator",
      "audio_path",
      "clean_text",
      "codec_cache",
      "context",
      "cpu",
      "decode_one_token",
      "exists",
      "format",
      "generate",
      "generate_monologue",
      "generate_single",
      "input_text",
      "last_token",
      "len",
      "length",
      "list",
      "max_audio_length_ms",
      "os",
      "path",
      "prepare_prompt",
      "print",
      "prompt_a",
      "prompt_text",
      "prompt_wav",
      "self",
      "speaker",
      "split_text",
      "strip",
      "temperature",
      "tensor",
      "text",
      "text_list",
      "token",
      "token_generator",
      "tokens",
      "topk",
      "torch",
      "unsqueeze"
    ],
    "start_line": 568,
    "end_line": 626,
    "text": "def generate_monologue(\n        self, text, prompt_wav=None, prompt_text=None, temperature=0.75, topk=20\n    ):\n        # step1. construct context\n        if prompt_wav is not None:\n            assert os.path.exists(prompt_wav)\n            assert prompt_text is not None\n\n            prompt_text = clean_text(text=prompt_text)\n            text = clean_text(text=text)\n            text_list = split_text(text=text, length=400)\n\n            print('[INFO] text_list: {}'.format(text_list))\n\n            tokens: List[torch.Tensor] = []\n            codec_cache = {}\n            for text in text_list:\n                text = clean_text(text=text)\n                input_text = prompt_text[:-1] + \",\" + text\n                prompt_a = self.prepare_prompt(\n                    text=input_text, speaker=\"[S1]\", audio_path=prompt_wav\n                )\n                context = [prompt_a]\n\n                token_generator = self.generate_single(\n                    context=context, temperature=temperature, topk=topk\n                )\n                for token in token_generator:\n                    # token: (1, nq)\n                    if len(tokens) > 2:\n                        # generate previous token\n                        audio_chunk, codec_cache = self._audio_tokenizer.decode_one_token(\n                            tokens[-1].unsqueeze(-1),\n                            codec_cache,\n                            last_token=False,\n                        )\n                        yield audio_chunk.cpu()\n                    tokens.append(token)\n\n            # process last token\n            audio_chunk, codec_cache = self._audio_tokenizer.decode_one_token(\n                tokens[-1].unsqueeze(-1),\n                codec_cache,\n                last_token=True,\n            )\n            yield audio_chunk.cpu()\n        else:\n            # random speaker\n            text = clean_text(text=text.strip())\n            audio_generator = self.generate(\n                text=text,\n                speaker=\"[S1]\",\n                context=[],\n                max_audio_length_ms=30_000,\n                temperature=temperature,\n                topk=topk,\n            )\n            for audio_chunk in audio_generator:\n                yield audio_chunk.unsqueeze(0).cpu()"
  },
  {
    "id": "0f82f1fb3fdcd053544d4383e0907ecfed8a26ef",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/fireredtts2.py",
    "kind": "function",
    "name": "generate_dialogue",
    "signature": "generate_dialogue(\n        self,\n        text_list,\n        prompt_wav_list=None,\n        prompt_text_list=None,\n        temperature=0.9,\n        topk=20,\n    )",
    "docstring": "",
    "identifiers": [
      "all_generated_segments",
      "append",
      "audio",
      "audio_16k",
      "audio_chunk",
      "audio_generator",
      "audio_path",
      "audio_tensor",
      "cat",
      "context",
      "cpu",
      "dim",
      "functional",
      "generate",
      "generate_dialogue",
      "i",
      "len",
      "max_audio_length_ms",
      "prepare_prompt",
      "process_text_list",
      "prompt_segments",
      "prompt_text",
      "prompt_text_list",
      "prompt_wav",
      "prompt_wav_list",
      "range",
      "resample",
      "segment",
      "self",
      "speaker",
      "temperature",
      "text",
      "text_list",
      "topk",
      "torch",
      "torchaudio",
      "tqdm",
      "unsqueeze"
    ],
    "start_line": 513,
    "end_line": 565,
    "text": "def generate_dialogue(\n        self,\n        text_list,\n        prompt_wav_list=None,\n        prompt_text_list=None,\n        temperature=0.9,\n        topk=20,\n    ):\n        all_generated_segments = []\n        prompt_segments = []\n        text_list = process_text_list(text_list=text_list)\n        if prompt_wav_list is not None:\n            assert len(prompt_wav_list) == len(prompt_text_list)\n            # Prepare prompts\n            for i in range(len(prompt_wav_list)):\n                prompt_wav = prompt_wav_list[i]\n                prompt_text = prompt_text_list[i]\n                speaker = prompt_text[:4]\n                assert speaker in [\"[S1]\", \"[S2]\", \"[S3]\", \"[S4]\"]\n                prompt_segments.append(\n                    self.prepare_prompt(\n                        text=prompt_text, speaker=speaker, audio_path=prompt_wav\n                    )\n                )\n\n        for text in tqdm(text_list):\n            speaker = text[:4]\n            text = text[4:]\n            # print(\"---speaker:\", speaker)\n            # print(\"---text:\", text)\n            assert speaker in [\"[S1]\", \"[S2]\", \"[S3]\", \"[S4]\"]\n\n            audio_generator = self.generate(\n                text=text,\n                speaker=speaker,\n                context=prompt_segments + all_generated_segments,\n                max_audio_length_ms=30_000,\n                temperature=temperature,\n                topk=topk,\n            )\n            audio_tensor = []\n            for audio_chunk in audio_generator:\n                audio_tensor.append(audio_chunk)\n                yield audio_chunk.unsqueeze(0).cpu()\n            audio_tensor = torch.cat(audio_tensor, dim=0)\n\n            # 做上下文管理的时候需要将audio 转到16k\n            audio_16k = torchaudio.functional.resample(\n                audio_tensor.unsqueeze(0), 24000, 16000\n            )\n            all_generated_segments.append(\n                Segment(text=text, speaker=speaker, audio=audio_16k)\n            )"
  },
  {
    "id": "b57a8f541fc33a62b0bb0ef78ee3cb68690ee6ba",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/fireredtts2.py",
    "kind": "function",
    "name": "generate_single",
    "signature": "generate_single(\n        self, context: List[Segment], temperature: float = 0.9, topk: int = 20\n    )",
    "docstring": "",
    "identifiers": [
      "_",
      "_model",
      "_tokenize_segment",
      "all",
      "append",
      "arange",
      "bool",
      "cat",
      "context",
      "curr_pos",
      "curr_tokens",
      "curr_tokens_mask",
      "device",
      "dim",
      "float",
      "generate_frame",
      "generate_single",
      "int",
      "list",
      "long",
      "max_generation_len",
      "ones_like",
      "prompt_tokens",
      "prompt_tokens_mask",
      "range",
      "reset_caches",
      "sample",
      "segment",
      "segment_tokens",
      "segment_tokens_mask",
      "self",
      "size",
      "temperature",
      "to",
      "tokens",
      "tokens_mask",
      "topk",
      "torch",
      "unsqueeze",
      "zeros"
    ],
    "start_line": 467,
    "end_line": 510,
    "text": "def generate_single(\n        self, context: List[Segment], temperature: float = 0.9, topk: int = 20\n    ):\n        self._model.reset_caches()\n        max_generation_len = 400\n        tokens, tokens_mask = [], []\n        for segment in context:\n            segment_tokens, segment_tokens_mask = self._tokenize_segment(segment)\n            tokens.append(segment_tokens)\n            tokens_mask.append(segment_tokens_mask)\n\n        prompt_tokens = torch.cat(tokens, dim=0).long().to(self.device)\n        prompt_tokens_mask = torch.cat(tokens_mask, dim=0).bool().to(self.device)\n        prompt_tokens = prompt_tokens[:-3, :]\n        prompt_tokens_mask = prompt_tokens_mask[:-3, :]\n\n        curr_tokens = prompt_tokens.unsqueeze(0)\n        curr_tokens_mask = prompt_tokens_mask.unsqueeze(0)\n        curr_pos = (\n            torch.arange(0, prompt_tokens.size(0)).unsqueeze(0).long().to(self.device)\n        )\n\n        for _ in range(max_generation_len):\n            # sample: (1, nq)\n            sample = self._model.generate_frame(\n                curr_tokens, curr_tokens_mask, curr_pos, temperature, topk\n            )\n            # eos\n            if torch.all(sample == 0):\n                break\n            yield sample\n\n            # next AR\n            curr_tokens = torch.cat(\n                [sample, torch.zeros(1, 1).long().to(self.device)], dim=1\n            ).unsqueeze(1)\n            curr_tokens_mask = torch.cat(\n                [\n                    torch.ones_like(sample).bool(),\n                    torch.zeros(1, 1).bool().to(self.device),\n                ],\n                dim=1,\n            ).unsqueeze(1)\n            curr_pos = curr_pos[:, -1:] + 1"
  },
  {
    "id": "e2b88def42260e5aa3ff4eb6a2b7be7ec4ab8570",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/fireredtts2.py",
    "kind": "function",
    "name": "generate",
    "signature": "generate(\n        self,\n        text: str,\n        speaker: str,\n        context: List[Segment],\n        max_audio_length_ms: float = 90_000,\n        temperature: float = 0.9,\n        topk: int = 20,\n    )",
    "docstring": "",
    "identifiers": [
      "_",
      "_audio_tokenizer",
      "_model",
      "_tokenize_segment",
      "_tokenize_text_segment",
      "all",
      "append",
      "arange",
      "audio_chunk",
      "bool",
      "cat",
      "codec_cache",
      "context",
      "curr_pos",
      "curr_tokens",
      "curr_tokens_mask",
      "decode_one_token",
      "device",
      "dim",
      "float",
      "gen_segment_tokens",
      "gen_segment_tokens_mask",
      "generate",
      "generate_frame",
      "int",
      "last_token",
      "list",
      "long",
      "max_audio_length_ms",
      "max_context_len",
      "max_generation_len",
      "max_seq_len",
      "ones_like",
      "prev_sample",
      "prompt_tokens",
      "prompt_tokens_mask",
      "range",
      "reset_caches",
      "sample",
      "segment",
      "segment_tokens",
      "segment_tokens_mask",
      "self",
      "size",
      "speaker",
      "squeeze",
      "str",
      "temperature",
      "text",
      "to",
      "tokens",
      "tokens_mask",
      "topk",
      "torch",
      "unsqueeze",
      "valueerror",
      "zeros"
    ],
    "start_line": 385,
    "end_line": 465,
    "text": "def generate(\n        self,\n        text: str,\n        speaker: str,\n        context: List[Segment],\n        max_audio_length_ms: float = 90_000,\n        temperature: float = 0.9,\n        topk: int = 20,\n    ):\n        self._model.reset_caches()\n\n        max_generation_len = int(max_audio_length_ms / 80)\n        tokens, tokens_mask = [], []\n        for segment in context:\n            segment_tokens, segment_tokens_mask = self._tokenize_segment(segment)\n            tokens.append(segment_tokens)\n            tokens_mask.append(segment_tokens_mask)\n\n        gen_segment_tokens, gen_segment_tokens_mask = self._tokenize_text_segment(\n            text, speaker\n        )\n        tokens.append(gen_segment_tokens)\n        tokens_mask.append(gen_segment_tokens_mask)\n\n        prompt_tokens = torch.cat(tokens, dim=0).long().to(self.device)\n        prompt_tokens_mask = torch.cat(tokens_mask, dim=0).bool().to(self.device)\n\n        curr_tokens = prompt_tokens.unsqueeze(0)\n        curr_tokens_mask = prompt_tokens_mask.unsqueeze(0)\n        curr_pos = (\n            torch.arange(0, prompt_tokens.size(0)).unsqueeze(0).long().to(self.device)\n        )\n\n        max_seq_len = 3100\n        max_context_len = max_seq_len - max_generation_len\n        if curr_tokens.size(1) >= max_context_len:\n            raise ValueError(\n                f\"Inputs too long, must be below max_seq_len - max_generation_len: {max_context_len}\"\n            )\n\n        # for streaming token2audio\n        codec_cache = {}\n        prev_sample = None\n        for _ in range(max_generation_len):\n            sample = self._model.generate_frame(\n                curr_tokens, curr_tokens_mask, curr_pos, temperature, topk\n            )\n            # eos\n            if torch.all(sample == 0):\n                break\n            \n            # token2audio one step\n            if prev_sample is None:\n                prev_sample = sample\n            else:\n                audio_chunk, codec_cache = self._audio_tokenizer.decode_one_token(\n                    prev_sample.unsqueeze(-1),\n                    codec_cache,\n                    last_token=False,\n                )\n                prev_sample = sample\n                yield audio_chunk.squeeze(0)\n\n            curr_tokens = torch.cat(\n                [sample, torch.zeros(1, 1).long().to(self.device)], dim=1\n            ).unsqueeze(1)\n            curr_tokens_mask = torch.cat(\n                [\n                    torch.ones_like(sample).bool(),\n                    torch.zeros(1, 1).bool().to(self.device),\n                ],\n                dim=1,\n            ).unsqueeze(1)\n            curr_pos = curr_pos[:, -1:] + 1\n\n        audio_chunk, codec_cache = self._audio_tokenizer.decode_one_token(\n            prev_sample.unsqueeze(-1),\n            codec_cache,\n            last_token=True,\n        )\n        yield audio_chunk.squeeze(0)"
  },
  {
    "id": "6586eed7a4cdcaa41f799724a57dd724abe93c48",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/fireredtts2.py",
    "kind": "class",
    "name": "FireRedTTS2",
    "signature": "class FireRedTTS2",
    "docstring": "",
    "identifiers": [
      "_",
      "__init__",
      "_audio_tokenizer",
      "_model",
      "_text_tokenizer",
      "_tokenize_audio",
      "_tokenize_segment",
      "_tokenize_text_segment",
      "all",
      "all_audio",
      "all_generated_segments",
      "all_storage_segments",
      "append",
      "arange",
      "audio",
      "audio16k",
      "audio_16k",
      "audio_frame",
      "audio_frame_mask",
      "audio_length",
      "audio_list",
      "audio_masks",
      "audio_path",
      "audio_sr",
      "audio_tensor",
      "audio_tokens",
      "batch_size",
      "bfloat16",
      "bool",
      "cat",
      "checkpoint_path",
      "clean_text",
      "codec_ckpt_path",
      "codec_config_path",
      "configs",
      "context",
      "cpu",
      "cuda",
      "curr_pos",
      "curr_tokens",
      "curr_tokens_mask",
      "decode",
      "device",
      "dim",
      "dtype",
      "duration",
      "encode",
      "end_time",
      "eos_frame",
      "eval",
      "exists",
      "fireredtts2",
      "float",
      "frame_masks",
      "frame_tokens",
      "from_pretrained",
      "functional",
      "gen_segment_tokens",
      "gen_segment_tokens_mask",
      "gen_tokens",
      "gen_type",
      "generate",
      "generate_dialogue",
      "generate_frame",
      "generate_monologue",
      "generate_single",
      "i",
      "inference_mode",
      "input_text",
      "int",
      "is_bf16_supported",
      "join",
      "json",
      "len",
      "length",
      "list",
      "llm_ckpt_path",
      "llm_config",
      "llm_config_path",
      "load",
      "load_custom_tokenizer",
      "load_llm_model",
      "load_prompt_audio",
      "long",
      "max_audio_length_ms",
      "max_context_len",
      "max_generation_len",
      "max_seq_len",
      "num_token",
      "ones_like",
      "open",
      "os",
      "path",
      "permute",
      "prepare_prompt",
      "pretrained_dir",
      "pretrained_qwen_path",
      "print",
      "process_text_list",
      "prompt_a",
      "prompt_segments",
      "prompt_text",
      "prompt_text_list",
      "prompt_tokens",
      "prompt_tokens_mask",
      "prompt_wav",
      "prompt_wav_list",
      "range",
      "redcodecinfer",
      "resample",
      "reset_caches",
      "sample",
      "sample_rate",
      "samples",
      "seg",
      "segment",
      "segment_tokens",
      "segment_tokens_mask",
      "self",
      "setup_caches",
      "shape",
      "size",
      "speaker",
      "split_text",
      "squeeze",
      "stack",
      "start_time",
      "str",
      "strip",
      "temperature",
      "tensor",
      "tensors",
      "text",
      "text_frame",
      "text_frame_mask",
      "text_list",
      "text_masks",
      "text_tokens",
      "time",
      "to",
      "token_length",
      "tokens",
      "tokens_mask",
      "topk",
      "torch",
      "torch_codec",
      "torchaudio",
      "tqdm",
      "transpose",
      "tuple",
      "unsqueeze",
      "use_bf16",
      "valueerror",
      "zeros"
    ],
    "start_line": 15,
    "end_line": 380,
    "text": "class FireRedTTS2:\n    def __init__(self, pretrained_dir, gen_type, device, use_bf16=False):\n        self.use_bf16 = use_bf16\n        self.device = device\n        self.sample_rate = 16000\n        self.max_seq_len = 3100\n\n        assert os.path.exists(pretrained_dir)\n        assert gen_type in [\"monologue\", \"dialogue\"]\n        llm_config_path = os.path.join(pretrained_dir, \"config_llm.json\")\n        if gen_type == \"monologue\":\n            llm_ckpt_path = os.path.join(pretrained_dir, \"llm_pretrain.pt\")\n        else:\n            llm_ckpt_path = os.path.join(pretrained_dir, \"llm_posttrain.pt\")\n        codec_config_path = os.path.join(pretrained_dir, \"config_codec.json\")\n        codec_ckpt_path = os.path.join(pretrained_dir, \"codec.pt\")\n        pretrained_qwen_path = os.path.join(pretrained_dir, \"Qwen2.5-1.5B\")\n\n        # check\n        assert os.path.exists(llm_config_path)\n        assert os.path.exists(llm_ckpt_path)\n        assert os.path.exists(codec_config_path)\n        assert os.path.exists(codec_ckpt_path)\n        assert os.path.exists(pretrained_qwen_path)\n\n        # ==== Load Torch LLM ====\n        llm_config = json.load(open(llm_config_path))\n        self._model = load_llm_model(\n            configs=llm_config, checkpoint_path=llm_ckpt_path, device=device\n        )\n        if use_bf16:\n            if torch.cuda.is_bf16_supported():\n                print(\"bf16 supported\")\n                self._model.to(dtype=torch.bfloat16)\n            else:\n                self.use_bf16 = False\n                print(\"bf16 not supported\")\n\n        self._model.eval()\n        self._model.setup_caches(1)\n        print(\"[INFO] LLM Loaded...\")\n\n        # ==== Load Qwen2.5 Text Tokenizer ====\n        self._text_tokenizer = load_custom_tokenizer(pretrained_qwen_path)\n        print(\"[INFO] Text Tokenizer Loaded...\")\n\n        # ==== Load Torch Audio Tokenizer ====\n        torch_codec = RedCodecInfer.from_pretrained(codec_config_path, codec_ckpt_path)\n        torch_codec.eval()\n        self._audio_tokenizer = torch_codec.to(device)\n        print(\"[INFO] Codec Loaded...\")\n\n    def load_prompt_audio(self, audio_path) -> torch.Tensor:\n        audio, audio_sr = torchaudio.load(audio_path)\n        # Audio must be single channel\n        if audio.shape[0] > 1:\n            audio = audio[0, :].unsqueeze(0)\n        audio16k = torchaudio.functional.resample(audio, audio_sr, 16000)\n        return audio16k\n\n    def prepare_prompt(self, text, speaker, audio_path) -> Segment:\n        audio_tensor = self.load_prompt_audio(audio_path)\n        return Segment(text=text, speaker=speaker, audio=audio_tensor)\n\n    def _tokenize_text_segment(\n        self, text: str, speaker: str\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        frame_tokens = []\n        frame_masks = []\n\n        text = speaker + \"<|text_start|>\" + text + \"<|text_end|>\"\n        text_tokens = self._text_tokenizer.encode(text)\n        text_frame = torch.zeros(len(text_tokens), 17).long()\n        text_frame_mask = torch.zeros(len(text_tokens), 17).bool()\n        text_frame[:, -1] = torch.tensor(text_tokens)\n        text_frame_mask[:, -1] = True\n\n        frame_tokens.append(text_frame.to(self.device))\n        frame_masks.append(text_frame_mask.to(self.device))\n\n        return torch.cat(frame_tokens, dim=0), torch.cat(frame_masks, dim=0)\n\n    def _tokenize_audio(self, audio: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        frame_tokens = []\n        frame_masks = []\n\n        # (K, T)\n        audio_length = torch.tensor([audio.shape[1]], dtype=torch.long)\n        audio_tokens, token_length = self._audio_tokenizer.encode(\n            audio.to(self.device),\n            audio_length.to(self.device),\n            batch_size=48,\n        )\n\n        audio_tokens = audio_tokens.squeeze(0)\n        # add EOS frame\n        eos_frame = torch.zeros(audio_tokens.size(0), 1).to(self.device)\n        audio_tokens = torch.cat([audio_tokens, eos_frame], dim=1)\n\n        audio_frame = torch.zeros(audio_tokens.size(1), 17)"
  },
  {
    "id": "152583faf5e2a6246dc01de8e5d4ca6cb41868f3",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/fireredtts2.py",
    "kind": "function",
    "name": "generate_monologue",
    "signature": "generate_monologue(\n        self, text, prompt_wav=None, prompt_text=None, temperature=0.75, topk=20\n    )",
    "docstring": "",
    "identifiers": [
      "_audio_tokenizer",
      "all_audio",
      "all_generated_segments",
      "all_storage_segments",
      "append",
      "audio",
      "audio_list",
      "audio_path",
      "audio_tensor",
      "cat",
      "clean_text",
      "context",
      "decode",
      "dim",
      "exists",
      "gen_tokens",
      "generate",
      "generate_monologue",
      "generate_single",
      "input_text",
      "length",
      "max_audio_length_ms",
      "os",
      "path",
      "prepare_prompt",
      "prompt_a",
      "prompt_segments",
      "prompt_text",
      "prompt_wav",
      "self",
      "shape",
      "speaker",
      "split_text",
      "squeeze",
      "strip",
      "temperature",
      "tensors",
      "text",
      "text_list",
      "topk",
      "torch",
      "unsqueeze"
    ],
    "start_line": 327,
    "end_line": 380,
    "text": "def generate_monologue(\n        self, text, prompt_wav=None, prompt_text=None, temperature=0.75, topk=20\n    ):\n        # step1. construct context\n        if prompt_wav is not None:\n            assert os.path.exists(prompt_wav)\n            assert prompt_text is not None\n\n            all_generated_segments = []\n            all_storage_segments = []\n            prompt_segments = []\n            prompt_text = clean_text(text=prompt_text)\n            text = clean_text(text=text)\n            text_list = split_text(text=text, length=400)\n\n            audio_list = []\n            for text in text_list:\n                text = clean_text(text=text)\n                input_text = prompt_text[:-1] + \",\" + text\n                prompt_a = self.prepare_prompt(\n                    text=input_text, speaker=\"[S1]\", audio_path=prompt_wav\n                )\n\n                context = [prompt_a]\n\n                while True:\n                    gen_tokens = self.generate_single(\n                        context=context, temperature=temperature, topk=topk\n                    )\n                    if gen_tokens.shape[2] > 18:\n                        break\n                    # else:\n                    #     print(\"生成结果小于1s,重新跑\")\n\n                gen_tokens = gen_tokens[:, :, 2:]  # cut leading silence\n                audio = self._audio_tokenizer.decode(gen_tokens).squeeze(0).squeeze(0)\n                audio_list.append(audio.unsqueeze(0))\n\n            all_audio = torch.cat(tensors=audio_list, dim=1)\n\n            return all_audio\n\n        else:\n            # random speaker\n            text = clean_text(text=text.strip())\n            audio_tensor = self.generate(\n                text=text,\n                speaker=\"[S1]\",\n                context=[],\n                max_audio_length_ms=30_000,\n                temperature=temperature,\n                topk=topk,\n            )\n            return audio_tensor.unsqueeze(0)"
  },
  {
    "id": "1fffd9b3356c57a5e2cea3879c296dc6f7467b75",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/fireredtts2.py",
    "kind": "function",
    "name": "generate_dialogue",
    "signature": "generate_dialogue(\n        self,\n        text_list,\n        prompt_wav_list=None,\n        prompt_text_list=None,\n        temperature=0.9,\n        topk=20,\n    )",
    "docstring": "",
    "identifiers": [
      "all_audio",
      "all_generated_segments",
      "all_storage_segments",
      "append",
      "audio",
      "audio_16k",
      "audio_path",
      "audio_tensor",
      "cat",
      "context",
      "cpu",
      "dim",
      "functional",
      "generate",
      "generate_dialogue",
      "i",
      "len",
      "max_audio_length_ms",
      "prepare_prompt",
      "process_text_list",
      "prompt_segments",
      "prompt_text",
      "prompt_text_list",
      "prompt_wav",
      "prompt_wav_list",
      "range",
      "resample",
      "seg",
      "segment",
      "self",
      "speaker",
      "temperature",
      "text",
      "text_list",
      "topk",
      "torch",
      "torchaudio",
      "tqdm",
      "unsqueeze"
    ],
    "start_line": 267,
    "end_line": 324,
    "text": "def generate_dialogue(\n        self,\n        text_list,\n        prompt_wav_list=None,\n        prompt_text_list=None,\n        temperature=0.9,\n        topk=20,\n    ):\n        all_generated_segments = []\n        all_storage_segments = []\n        prompt_segments = []\n        text_list = process_text_list(text_list=text_list)\n        if prompt_wav_list is not None:\n            assert len(prompt_wav_list) == len(prompt_text_list)\n            # Prepare prompts\n            for i in range(len(prompt_wav_list)):\n                prompt_wav = prompt_wav_list[i]\n                prompt_text = prompt_text_list[i]\n                speaker = prompt_text[:4]\n                assert speaker in [\"[S1]\", \"[S2]\", \"[S3]\", \"[S4]\"]\n                prompt_segments.append(\n                    self.prepare_prompt(\n                        text=prompt_text, speaker=speaker, audio_path=prompt_wav\n                    )\n                )\n\n        for text in tqdm(text_list):\n            speaker = text[:4]\n            text = text[4:]\n            # print(\"---speaker:\", speaker)\n            # print(\"---text:\", text)\n            assert speaker in [\"[S1]\", \"[S2]\", \"[S3]\", \"[S4]\"]\n\n            audio_tensor = self.generate(\n                text=text,\n                speaker=speaker,\n                context=prompt_segments + all_generated_segments,\n                max_audio_length_ms=30_000,\n                temperature=temperature,\n                topk=topk,\n            )\n\n            # 做上下文管理的时候需要将audio 转到16k\n            audio_16k = torchaudio.functional.resample(\n                audio_tensor.unsqueeze(0), 24000, 16000\n            )\n            all_generated_segments.append(\n                Segment(text=text, speaker=speaker, audio=audio_16k)\n            )\n\n            all_storage_segments.append(\n                Segment(text=text, speaker=speaker, audio=audio_tensor.unsqueeze(0))\n            )\n\n        # Concatenate all generations\n        all_audio = torch.cat([seg.audio for seg in all_storage_segments], dim=1)\n        all_audio = all_audio.cpu()\n        return all_audio"
  },
  {
    "id": "baead92c96028d374eb886797dca651bb5404591",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/fireredtts2.py",
    "kind": "function",
    "name": "generate_single",
    "signature": "generate_single(\n        self, context: List[Segment], temperature: float = 0.9, topk: int = 20\n    )",
    "docstring": "",
    "identifiers": [
      "_",
      "_model",
      "_tokenize_segment",
      "all",
      "append",
      "arange",
      "bool",
      "cat",
      "context",
      "curr_pos",
      "curr_tokens",
      "curr_tokens_mask",
      "device",
      "dim",
      "duration",
      "end_time",
      "float",
      "gen_tokens",
      "generate_frame",
      "generate_single",
      "int",
      "list",
      "long",
      "max_generation_len",
      "num_token",
      "ones_like",
      "permute",
      "print",
      "prompt_tokens",
      "prompt_tokens_mask",
      "range",
      "reset_caches",
      "sample",
      "samples",
      "segment",
      "segment_tokens",
      "segment_tokens_mask",
      "self",
      "size",
      "stack",
      "start_time",
      "temperature",
      "time",
      "to",
      "tokens",
      "tokens_mask",
      "topk",
      "torch",
      "unsqueeze",
      "zeros"
    ],
    "start_line": 210,
    "end_line": 264,
    "text": "def generate_single(\n        self, context: List[Segment], temperature: float = 0.9, topk: int = 20\n    ):\n        self._model.reset_caches()\n        max_generation_len = 400\n        tokens, tokens_mask = [], []\n        for segment in context:\n            segment_tokens, segment_tokens_mask = self._tokenize_segment(segment)\n            tokens.append(segment_tokens)\n            tokens_mask.append(segment_tokens_mask)\n\n        prompt_tokens = torch.cat(tokens, dim=0).long().to(self.device)\n        prompt_tokens_mask = torch.cat(tokens_mask, dim=0).bool().to(self.device)\n        prompt_tokens = prompt_tokens[:-3, :]\n        prompt_tokens_mask = prompt_tokens_mask[:-3, :]\n\n        samples = []\n        curr_tokens = prompt_tokens.unsqueeze(0)\n        curr_tokens_mask = prompt_tokens_mask.unsqueeze(0)\n        curr_pos = (\n            torch.arange(0, prompt_tokens.size(0)).unsqueeze(0).long().to(self.device)\n        )\n\n        num_token = 0\n        start_time = time.time()\n        for _ in range(max_generation_len):\n            sample = self._model.generate_frame(\n                curr_tokens, curr_tokens_mask, curr_pos, temperature, topk\n            )\n            # eos\n            if torch.all(sample == 0):\n                break\n\n            samples.append(sample)\n\n            curr_tokens = torch.cat(\n                [sample, torch.zeros(1, 1).long().to(self.device)], dim=1\n            ).unsqueeze(1)\n            curr_tokens_mask = torch.cat(\n                [\n                    torch.ones_like(sample).bool(),\n                    torch.zeros(1, 1).bool().to(self.device),\n                ],\n                dim=1,\n            ).unsqueeze(1)\n            curr_pos = curr_pos[:, -1:] + 1\n            num_token += 1\n            if num_token == 2:\n                end_time = time.time()\n                duration = end_time - start_time\n                print(\"---first pack duration:\", duration)\n\n        gen_tokens = torch.stack(samples).permute(1, 2, 0)\n\n        return gen_tokens"
  },
  {
    "id": "7acb658a92e9022959f668860d19e98b6c50b19e",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/fireredtts2.py",
    "kind": "function",
    "name": "generate",
    "signature": "generate(\n        self,\n        text: str,\n        speaker: str,\n        context: List[Segment],\n        max_audio_length_ms: float = 90_000,\n        temperature: float = 0.9,\n        topk: int = 20,\n    )",
    "docstring": "",
    "identifiers": [
      "_",
      "_audio_tokenizer",
      "_model",
      "_tokenize_segment",
      "_tokenize_text_segment",
      "all",
      "append",
      "arange",
      "audio",
      "bool",
      "cat",
      "context",
      "curr_pos",
      "curr_tokens",
      "curr_tokens_mask",
      "decode",
      "device",
      "dim",
      "float",
      "gen_segment_tokens",
      "gen_segment_tokens_mask",
      "generate",
      "generate_frame",
      "int",
      "list",
      "long",
      "max_audio_length_ms",
      "max_context_len",
      "max_generation_len",
      "max_seq_len",
      "ones_like",
      "permute",
      "prompt_tokens",
      "prompt_tokens_mask",
      "range",
      "reset_caches",
      "sample",
      "samples",
      "segment",
      "segment_tokens",
      "segment_tokens_mask",
      "self",
      "size",
      "speaker",
      "squeeze",
      "stack",
      "str",
      "temperature",
      "tensor",
      "text",
      "to",
      "tokens",
      "tokens_mask",
      "topk",
      "torch",
      "unsqueeze",
      "valueerror",
      "zeros"
    ],
    "start_line": 139,
    "end_line": 208,
    "text": "def generate(\n        self,\n        text: str,\n        speaker: str,\n        context: List[Segment],\n        max_audio_length_ms: float = 90_000,\n        temperature: float = 0.9,\n        topk: int = 20,\n    ) -> torch.Tensor:\n        self._model.reset_caches()\n\n        max_generation_len = int(max_audio_length_ms / 80)\n        tokens, tokens_mask = [], []\n        for segment in context:\n            segment_tokens, segment_tokens_mask = self._tokenize_segment(segment)\n            tokens.append(segment_tokens)\n            tokens_mask.append(segment_tokens_mask)\n\n        gen_segment_tokens, gen_segment_tokens_mask = self._tokenize_text_segment(\n            text, speaker\n        )\n        tokens.append(gen_segment_tokens)\n        tokens_mask.append(gen_segment_tokens_mask)\n\n        prompt_tokens = torch.cat(tokens, dim=0).long().to(self.device)\n        prompt_tokens_mask = torch.cat(tokens_mask, dim=0).bool().to(self.device)\n\n        samples = []\n        curr_tokens = prompt_tokens.unsqueeze(0)\n        curr_tokens_mask = prompt_tokens_mask.unsqueeze(0)\n        curr_pos = (\n            torch.arange(0, prompt_tokens.size(0)).unsqueeze(0).long().to(self.device)\n        )\n\n        max_seq_len = 3100\n        max_context_len = max_seq_len - max_generation_len\n        if curr_tokens.size(1) >= max_context_len:\n            raise ValueError(\n                f\"Inputs too long, must be below max_seq_len - max_generation_len: {max_context_len}\"\n            )\n\n        for _ in range(max_generation_len):\n            sample = self._model.generate_frame(\n                curr_tokens, curr_tokens_mask, curr_pos, temperature, topk\n            )\n            # eos\n            if torch.all(sample == 0):\n                break\n\n            samples.append(sample)\n\n            curr_tokens = torch.cat(\n                [sample, torch.zeros(1, 1).long().to(self.device)], dim=1\n            ).unsqueeze(1)\n            curr_tokens_mask = torch.cat(\n                [\n                    torch.ones_like(sample).bool(),\n                    torch.zeros(1, 1).bool().to(self.device),\n                ],\n                dim=1,\n            ).unsqueeze(1)\n            curr_pos = curr_pos[:, -1:] + 1\n\n        audio = (\n            self._audio_tokenizer.decode(torch.stack(samples).permute(1, 2, 0))\n            .squeeze(0)\n            .squeeze(0)\n        )\n\n        return audio"
  },
  {
    "id": "5a48a6e36ef935df0459e495f57543b7562d863b",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/fireredtts2.py",
    "kind": "function",
    "name": "_tokenize_segment",
    "signature": "_tokenize_segment(self, segment: Segment)",
    "docstring": "Returns:\n            (seq_len,17), (seq_len, 17)",
    "identifiers": [
      "_tokenize_audio",
      "_tokenize_segment",
      "_tokenize_text_segment",
      "audio",
      "audio_masks",
      "audio_tokens",
      "cat",
      "dim",
      "segment",
      "self",
      "speaker",
      "tensor",
      "text",
      "text_masks",
      "text_tokens",
      "torch",
      "tuple"
    ],
    "start_line": 124,
    "end_line": 136,
    "text": "def _tokenize_segment(self, segment: Segment) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Returns:\n            (seq_len,17), (seq_len, 17)\n        \"\"\"\n        text_tokens, text_masks = self._tokenize_text_segment(\n            segment.text, segment.speaker\n        )\n        audio_tokens, audio_masks = self._tokenize_audio(segment.audio)\n\n        return torch.cat([text_tokens, audio_tokens], dim=0), torch.cat(\n            [text_masks, audio_masks], dim=0\n        )"
  },
  {
    "id": "ae5febe1b9c6716111a76037feb50d1384f5c539",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/fireredtts2.py",
    "kind": "function",
    "name": "_tokenize_audio",
    "signature": "_tokenize_audio(self, audio: torch.Tensor)",
    "docstring": "",
    "identifiers": [
      "_audio_tokenizer",
      "_tokenize_audio",
      "append",
      "audio",
      "audio_frame",
      "audio_frame_mask",
      "audio_length",
      "audio_tokens",
      "batch_size",
      "bool",
      "cat",
      "device",
      "dim",
      "dtype",
      "encode",
      "eos_frame",
      "frame_masks",
      "frame_tokens",
      "long",
      "self",
      "shape",
      "size",
      "squeeze",
      "tensor",
      "to",
      "token_length",
      "torch",
      "transpose",
      "tuple",
      "zeros"
    ],
    "start_line": 97,
    "end_line": 122,
    "text": "def _tokenize_audio(self, audio: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        frame_tokens = []\n        frame_masks = []\n\n        # (K, T)\n        audio_length = torch.tensor([audio.shape[1]], dtype=torch.long)\n        audio_tokens, token_length = self._audio_tokenizer.encode(\n            audio.to(self.device),\n            audio_length.to(self.device),\n            batch_size=48,\n        )\n\n        audio_tokens = audio_tokens.squeeze(0)\n        # add EOS frame\n        eos_frame = torch.zeros(audio_tokens.size(0), 1).to(self.device)\n        audio_tokens = torch.cat([audio_tokens, eos_frame], dim=1)\n\n        audio_frame = torch.zeros(audio_tokens.size(1), 17).long().to(self.device)\n        audio_frame_mask = torch.zeros(audio_tokens.size(1), 17).bool().to(self.device)\n        audio_frame[:, :-1] = audio_tokens.transpose(0, 1)\n        audio_frame_mask[:, :-1] = True\n\n        frame_tokens.append(audio_frame)\n        frame_masks.append(audio_frame_mask)\n\n        return torch.cat(frame_tokens, dim=0), torch.cat(frame_masks, dim=0)"
  },
  {
    "id": "9fd1fefb4dd794000e4ab094b662229346cad417",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/fireredtts2.py",
    "kind": "function",
    "name": "_tokenize_text_segment",
    "signature": "_tokenize_text_segment(\n        self, text: str, speaker: str\n    )",
    "docstring": "",
    "identifiers": [
      "_text_tokenizer",
      "_tokenize_text_segment",
      "append",
      "bool",
      "cat",
      "device",
      "dim",
      "encode",
      "frame_masks",
      "frame_tokens",
      "len",
      "long",
      "self",
      "speaker",
      "str",
      "tensor",
      "text",
      "text_frame",
      "text_frame_mask",
      "text_tokens",
      "to",
      "torch",
      "tuple",
      "zeros"
    ],
    "start_line": 79,
    "end_line": 95,
    "text": "def _tokenize_text_segment(\n        self, text: str, speaker: str\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        frame_tokens = []\n        frame_masks = []\n\n        text = speaker + \"<|text_start|>\" + text + \"<|text_end|>\"\n        text_tokens = self._text_tokenizer.encode(text)\n        text_frame = torch.zeros(len(text_tokens), 17).long()\n        text_frame_mask = torch.zeros(len(text_tokens), 17).bool()\n        text_frame[:, -1] = torch.tensor(text_tokens)\n        text_frame_mask[:, -1] = True\n\n        frame_tokens.append(text_frame.to(self.device))\n        frame_masks.append(text_frame_mask.to(self.device))\n\n        return torch.cat(frame_tokens, dim=0), torch.cat(frame_masks, dim=0)"
  },
  {
    "id": "58cbd61cf1310066c1d8ba06980787c9b10098e3",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/fireredtts2.py",
    "kind": "function",
    "name": "prepare_prompt",
    "signature": "prepare_prompt(self, text, speaker, audio_path)",
    "docstring": "",
    "identifiers": [
      "audio",
      "audio_path",
      "audio_tensor",
      "load_prompt_audio",
      "prepare_prompt",
      "segment",
      "self",
      "speaker",
      "text"
    ],
    "start_line": 75,
    "end_line": 77,
    "text": "def prepare_prompt(self, text, speaker, audio_path) -> Segment:\n        audio_tensor = self.load_prompt_audio(audio_path)\n        return Segment(text=text, speaker=speaker, audio=audio_tensor)"
  },
  {
    "id": "40909839fcda08c540f3dc37e24a7b77d92e0e7a",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/fireredtts2.py",
    "kind": "function",
    "name": "load_prompt_audio",
    "signature": "load_prompt_audio(self, audio_path)",
    "docstring": "",
    "identifiers": [
      "audio",
      "audio16k",
      "audio_path",
      "audio_sr",
      "functional",
      "load",
      "load_prompt_audio",
      "resample",
      "self",
      "shape",
      "tensor",
      "torch",
      "torchaudio",
      "unsqueeze"
    ],
    "start_line": 67,
    "end_line": 73,
    "text": "def load_prompt_audio(self, audio_path) -> torch.Tensor:\n        audio, audio_sr = torchaudio.load(audio_path)\n        # Audio must be single channel\n        if audio.shape[0] > 1:\n            audio = audio[0, :].unsqueeze(0)\n        audio16k = torchaudio.functional.resample(audio, audio_sr, 16000)\n        return audio16k"
  },
  {
    "id": "c9fe7af7848cc8995e3b7b3ea5bfd8914805662c",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/fireredtts2.py",
    "kind": "function",
    "name": "__init__",
    "signature": "__init__(self, pretrained_dir, gen_type, device, use_bf16=False)",
    "docstring": "",
    "identifiers": [
      "__init__",
      "_audio_tokenizer",
      "_model",
      "_text_tokenizer",
      "bfloat16",
      "checkpoint_path",
      "codec_ckpt_path",
      "codec_config_path",
      "configs",
      "cuda",
      "device",
      "dtype",
      "eval",
      "exists",
      "from_pretrained",
      "gen_type",
      "is_bf16_supported",
      "join",
      "json",
      "llm_ckpt_path",
      "llm_config",
      "llm_config_path",
      "load",
      "load_custom_tokenizer",
      "load_llm_model",
      "max_seq_len",
      "open",
      "os",
      "path",
      "pretrained_dir",
      "pretrained_qwen_path",
      "print",
      "redcodecinfer",
      "sample_rate",
      "self",
      "setup_caches",
      "to",
      "torch",
      "torch_codec",
      "use_bf16"
    ],
    "start_line": 16,
    "end_line": 65,
    "text": "def __init__(self, pretrained_dir, gen_type, device, use_bf16=False):\n        self.use_bf16 = use_bf16\n        self.device = device\n        self.sample_rate = 16000\n        self.max_seq_len = 3100\n\n        assert os.path.exists(pretrained_dir)\n        assert gen_type in [\"monologue\", \"dialogue\"]\n        llm_config_path = os.path.join(pretrained_dir, \"config_llm.json\")\n        if gen_type == \"monologue\":\n            llm_ckpt_path = os.path.join(pretrained_dir, \"llm_pretrain.pt\")\n        else:\n            llm_ckpt_path = os.path.join(pretrained_dir, \"llm_posttrain.pt\")\n        codec_config_path = os.path.join(pretrained_dir, \"config_codec.json\")\n        codec_ckpt_path = os.path.join(pretrained_dir, \"codec.pt\")\n        pretrained_qwen_path = os.path.join(pretrained_dir, \"Qwen2.5-1.5B\")\n\n        # check\n        assert os.path.exists(llm_config_path)\n        assert os.path.exists(llm_ckpt_path)\n        assert os.path.exists(codec_config_path)\n        assert os.path.exists(codec_ckpt_path)\n        assert os.path.exists(pretrained_qwen_path)\n\n        # ==== Load Torch LLM ====\n        llm_config = json.load(open(llm_config_path))\n        self._model = load_llm_model(\n            configs=llm_config, checkpoint_path=llm_ckpt_path, device=device\n        )\n        if use_bf16:\n            if torch.cuda.is_bf16_supported():\n                print(\"bf16 supported\")\n                self._model.to(dtype=torch.bfloat16)\n            else:\n                self.use_bf16 = False\n                print(\"bf16 not supported\")\n\n        self._model.eval()\n        self._model.setup_caches(1)\n        print(\"[INFO] LLM Loaded...\")\n\n        # ==== Load Qwen2.5 Text Tokenizer ====\n        self._text_tokenizer = load_custom_tokenizer(pretrained_qwen_path)\n        print(\"[INFO] Text Tokenizer Loaded...\")\n\n        # ==== Load Torch Audio Tokenizer ====\n        torch_codec = RedCodecInfer.from_pretrained(codec_config_path, codec_ckpt_path)\n        torch_codec.eval()\n        self._audio_tokenizer = torch_codec.to(device)\n        print(\"[INFO] Codec Loaded...\")"
  },
  {
    "id": "2f3fbed619426aa3fe017d9f9908b0b59db989bb",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/utils/spliter.py",
    "kind": "function",
    "name": "process_text_list",
    "signature": "process_text_list(text_list)",
    "docstring": "",
    "identifiers": [
      "append",
      "chunk",
      "new_text_list",
      "process_text",
      "process_text_list",
      "result",
      "speaker",
      "text",
      "text_list"
    ],
    "start_line": 279,
    "end_line": 289,
    "text": "def process_text_list(text_list):\n    new_text_list = []\n    for text in text_list:\n        speaker = text[:4]\n        # print(\"---speaker:\", speaker)\n        assert speaker in [\"[S1]\", \"[S2]\", \"[S3]\", \"[S4]\"]\n        result = process_text(text=text)\n        # print(\"---result:\\n\", result, len(result))\n        for chunk in result:\n            new_text_list.append(speaker + chunk)\n    return new_text_list"
  },
  {
    "id": "b1d0d148c9b4b9383ff99bf5403809736d776281",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/utils/spliter.py",
    "kind": "function",
    "name": "process_text",
    "signature": "process_text(text)",
    "docstring": "",
    "identifiers": [
      "chinese_max_limit",
      "contains_chinese",
      "count_characters_chinese",
      "count_words_english",
      "english_max_limit",
      "is_chinese",
      "merge_sentences_chinese",
      "merge_sentences_english",
      "process_text",
      "re",
      "result",
      "sentences",
      "split_by_punctuation_chinese",
      "split_by_punctuation_english",
      "strip",
      "sub",
      "text"
    ],
    "start_line": 259,
    "end_line": 276,
    "text": "def process_text(text):\n    chinese_max_limit = 150\n    english_max_limit = 80\n    # 移除开头的标记如[S2]\n    text = re.sub(r\"^\\[S\\d+\\]\", \"\", text).strip()\n    is_chinese = contains_chinese(text)\n    if is_chinese:\n        if count_characters_chinese(text) <= chinese_max_limit:\n            return [text]\n        sentences = split_by_punctuation_chinese(text)\n        result = merge_sentences_chinese(sentences, chinese_max_limit)\n    else:\n        if count_words_english(text) <= english_max_limit:\n            return [text]\n        sentences = split_by_punctuation_english(text)\n        result = merge_sentences_english(sentences, english_max_limit)\n\n    return result"
  },
  {
    "id": "b10399782510b9f31f6abda56ac9984667eb431a",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/utils/spliter.py",
    "kind": "function",
    "name": "merge_sentences_chinese",
    "signature": "merge_sentences_chinese(sentences, max_chars=100)",
    "docstring": "合并中文句子",
    "identifiers": [
      "append",
      "count_characters_chinese",
      "current_chunk",
      "max_chars",
      "merge_sentences_chinese",
      "result",
      "sentence",
      "sentences",
      "test_chunk"
    ],
    "start_line": 237,
    "end_line": 256,
    "text": "def merge_sentences_chinese(sentences, max_chars=100):\n    \"\"\"合并中文句子\"\"\"\n    result = []\n    current_chunk = \"\"\n\n    for sentence in sentences:\n        if not current_chunk:\n            current_chunk = sentence\n        else:\n            test_chunk = current_chunk + sentence\n            if count_characters_chinese(test_chunk) <= max_chars:\n                current_chunk = test_chunk\n            else:\n                result.append(current_chunk)\n                current_chunk = sentence\n\n    if current_chunk:\n        result.append(current_chunk)\n\n    return result"
  },
  {
    "id": "c900d3db0a7d37e3f3ceb9716ffff4edbd902d16",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/utils/spliter.py",
    "kind": "function",
    "name": "merge_sentences_english",
    "signature": "merge_sentences_english(sentences, max_words=80)",
    "docstring": "合并英文句子",
    "identifiers": [
      "append",
      "count_words_english",
      "current_chunk",
      "max_words",
      "merge_sentences_english",
      "result",
      "sentence",
      "sentences",
      "test_chunk"
    ],
    "start_line": 215,
    "end_line": 234,
    "text": "def merge_sentences_english(sentences, max_words=80):\n    \"\"\"合并英文句子\"\"\"\n    result = []\n    current_chunk = \"\"\n\n    for sentence in sentences:\n        if not current_chunk:\n            current_chunk = sentence\n        else:\n            test_chunk = current_chunk + \" \" + sentence\n            if count_words_english(test_chunk) <= max_words:\n                current_chunk = test_chunk\n            else:\n                result.append(current_chunk)\n                current_chunk = sentence\n\n    if current_chunk:\n        result.append(current_chunk)\n\n    return result"
  },
  {
    "id": "f63ab90f944f64cbec40ce09958c7ed20dee973c",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/utils/spliter.py",
    "kind": "function",
    "name": "split_by_punctuation_chinese",
    "signature": "split_by_punctuation_chinese(text)",
    "docstring": "按中文标点符号分割",
    "identifiers": [
      "append",
      "i",
      "len",
      "range",
      "re",
      "result",
      "sentence",
      "sentences",
      "split",
      "split_by_punctuation_chinese",
      "strip",
      "text"
    ],
    "start_line": 198,
    "end_line": 212,
    "text": "def split_by_punctuation_chinese(text):\n    \"\"\"按中文标点符号分割\"\"\"\n    sentences = re.split(r\"([。！？])\", text)\n    result = []\n    for i in range(0, len(sentences) - 1, 2):\n        sentence = sentences[i].strip()\n        if sentence:\n            if i + 1 < len(sentences):\n                sentence += sentences[i + 1]\n            result.append(sentence)\n\n    if len(sentences) % 2 == 1 and sentences[-1].strip():\n        result.append(sentences[-1].strip())\n\n    return result"
  },
  {
    "id": "2a10c32af14d87f28b9e7469fe8f4b27c62baa40",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/utils/spliter.py",
    "kind": "function",
    "name": "split_by_punctuation_english",
    "signature": "split_by_punctuation_english(text)",
    "docstring": "按英文标点符号分割",
    "identifiers": [
      "append",
      "i",
      "len",
      "range",
      "re",
      "result",
      "sentence",
      "sentences",
      "split",
      "split_by_punctuation_english",
      "strip",
      "text"
    ],
    "start_line": 181,
    "end_line": 195,
    "text": "def split_by_punctuation_english(text):\n    \"\"\"按英文标点符号分割\"\"\"\n    sentences = re.split(r\"([.!?])\", text)\n    result = []\n    for i in range(0, len(sentences) - 1, 2):\n        sentence = sentences[i].strip()\n        if sentence:\n            if i + 1 < len(sentences):\n                sentence += sentences[i + 1]\n            result.append(sentence)\n\n    if len(sentences) % 2 == 1 and sentences[-1].strip():\n        result.append(sentences[-1].strip())\n\n    return result"
  },
  {
    "id": "239b754845e21944f82a0b8d8705afb2e08e91ff",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/utils/spliter.py",
    "kind": "function",
    "name": "count_characters_chinese",
    "signature": "count_characters_chinese(text)",
    "docstring": "统计中文字符数量",
    "identifiers": [
      "count_characters_chinese",
      "len",
      "text"
    ],
    "start_line": 176,
    "end_line": 178,
    "text": "def count_characters_chinese(text):\n    \"\"\"统计中文字符数量\"\"\"\n    return len(text)"
  },
  {
    "id": "8ace2d13d1ae43b87de326f24e0d52ff48ccaa71",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/utils/spliter.py",
    "kind": "function",
    "name": "count_words_english",
    "signature": "count_words_english(text)",
    "docstring": "统计英文单词数量",
    "identifiers": [
      "count_words_english",
      "len",
      "split",
      "text"
    ],
    "start_line": 171,
    "end_line": 173,
    "text": "def count_words_english(text):\n    \"\"\"统计英文单词数量\"\"\"\n    return len(text.split())"
  },
  {
    "id": "eb1e7819c3b2357d9efaab00894ddab9829753c8",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/utils/spliter.py",
    "kind": "function",
    "name": "contains_chinese",
    "signature": "contains_chinese(text)",
    "docstring": "检测文本是否包含中文字符",
    "identifiers": [
      "bool",
      "contains_chinese",
      "re",
      "search",
      "text"
    ],
    "start_line": 166,
    "end_line": 168,
    "text": "def contains_chinese(text):\n    \"\"\"检测文本是否包含中文字符\"\"\"\n    return bool(re.search(r\"[\\u4e00-\\u9fff]\", text))"
  },
  {
    "id": "b59568c2fbaff27af45667b3c8fb80fa7f82090a",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/utils/spliter.py",
    "kind": "function",
    "name": "split_text",
    "signature": "split_text(text, length)",
    "docstring": "",
    "identifiers": [
      "add_cleaned",
      "break_text",
      "break_text_by_length",
      "clean_text",
      "curr",
      "length",
      "list",
      "map",
      "protect_float",
      "segments",
      "split_text",
      "text",
      "texts",
      "unprotect_float",
      "utf_8_len"
    ],
    "start_line": 132,
    "end_line": 163,
    "text": "def split_text(text, length):\n    text = clean_text(text)\n\n    # Break the text into pieces with following rules:\n    # 1. Split the text at \".\", \"!\", \"?\" if text is NOT a float\n    # 2. If the text is longer than length, split at \",\"\n    # 3. If the text is still longer than length, split at \" \"\n    # 4. If the text is still longer than length, split at any character to length\n\n    texts = [text]\n    texts = map(protect_float, texts)\n    texts = break_text(texts, length, {\".\", \"!\", \"?\", \"。\", \"！\", \"？\"})\n    texts = map(unprotect_float, texts)\n    texts = break_text(texts, length, {\",\", \"，\"})\n    texts = break_text(texts, length, {\" \"})\n    texts = list(break_text_by_length(texts, length))\n\n    # Then, merge the texts into segments with length <= length\n    segments = []\n    curr = \"\"\n\n    for text in texts:\n        if utf_8_len(curr) + utf_8_len(text) <= length:\n            curr += text\n        else:\n            add_cleaned(curr, segments)\n            curr = text\n\n    if curr:\n        add_cleaned(curr, segments)\n\n    return segments"
  },
  {
    "id": "080dd747360fb4703da61456f377ec47e7e78d29",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/utils/spliter.py",
    "kind": "function",
    "name": "unprotect_float",
    "signature": "unprotect_float(text)",
    "docstring": "",
    "identifiers": [
      "re",
      "sub",
      "text",
      "unprotect_float"
    ],
    "start_line": 127,
    "end_line": 129,
    "text": "def unprotect_float(text):\n    # Turns <3_f_14> into 3.14\n    return re.sub(r\"<(\\d+)_f_(\\d+)>\", r\"\\1.\\2\", text)"
  },
  {
    "id": "2ac5acb3061b97977bf4d92b186f04cafccd70b9",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/utils/spliter.py",
    "kind": "function",
    "name": "protect_float",
    "signature": "protect_float(text)",
    "docstring": "",
    "identifiers": [
      "protect_float",
      "re",
      "sub",
      "text"
    ],
    "start_line": 122,
    "end_line": 124,
    "text": "def protect_float(text):\n    # Turns 3.14 into <3_f_14> to prevent splitting\n    return re.sub(r\"(\\d+)\\.(\\d+)\", r\"<\\1_f_\\2>\", text)"
  },
  {
    "id": "ffb881eb8089f46035ae853b8e247d692015f213",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/utils/spliter.py",
    "kind": "function",
    "name": "add_cleaned",
    "signature": "add_cleaned(curr, segments)",
    "docstring": "",
    "identifiers": [
      "add_cleaned",
      "all",
      "append",
      "c",
      "curr",
      "isspace",
      "punctuation",
      "segments",
      "string",
      "strip"
    ],
    "start_line": 116,
    "end_line": 119,
    "text": "def add_cleaned(curr, segments):\n    curr = curr.strip()\n    if curr and not all(c.isspace() or c in string.punctuation for c in curr):\n        segments.append(curr)"
  },
  {
    "id": "5e2fbad42beeae61e10923a0cfa61974548c8228",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/utils/spliter.py",
    "kind": "function",
    "name": "break_text_by_length",
    "signature": "break_text_by_length(texts, length)",
    "docstring": "",
    "identifiers": [
      "break_text_by_length",
      "char",
      "curr",
      "length",
      "text",
      "texts",
      "utf_8_len"
    ],
    "start_line": 98,
    "end_line": 113,
    "text": "def break_text_by_length(texts, length):\n    for text in texts:\n        if utf_8_len(text) <= length:\n            yield text\n            continue\n\n        curr = \"\"\n        for char in text:\n            curr += char\n\n            if utf_8_len(curr) >= length:\n                yield curr\n                curr = \"\"\n\n        if curr:\n            yield curr"
  },
  {
    "id": "a951b812a20e053c00a8cb8697efe607c28332d2",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/utils/spliter.py",
    "kind": "function",
    "name": "break_text",
    "signature": "break_text(texts, length, splits: set)",
    "docstring": "",
    "identifiers": [
      "break_text",
      "char",
      "curr",
      "length",
      "set",
      "splits",
      "text",
      "texts",
      "utf_8_len"
    ],
    "start_line": 80,
    "end_line": 95,
    "text": "def break_text(texts, length, splits: set):\n    for text in texts:\n        if utf_8_len(text) <= length:\n            yield text\n            continue\n\n        curr = \"\"\n        for char in text:\n            curr += char\n\n            if char in splits:\n                yield curr\n                curr = \"\"\n\n        if curr:\n            yield curr"
  },
  {
    "id": "4c5d7cf0d8342606c6aaca8ab837f48ee7518626",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/utils/spliter.py",
    "kind": "function",
    "name": "utf_8_len",
    "signature": "utf_8_len(text)",
    "docstring": "",
    "identifiers": [
      "encode",
      "len",
      "text",
      "utf_8_len"
    ],
    "start_line": 76,
    "end_line": 77,
    "text": "def utf_8_len(text):\n    return len(text.encode(\"utf-8\"))"
  },
  {
    "id": "1a12d9cf059a28e6015a4b9c1a4a246e52639578",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/utils/spliter.py",
    "kind": "function",
    "name": "clean_text",
    "signature": "clean_text(text)",
    "docstring": "",
    "identifiers": [
      "clean_text",
      "emoji_regex",
      "group",
      "m",
      "re",
      "replace",
      "replace_symbol_regex",
      "strip",
      "sub",
      "symbols_mapping",
      "text",
      "x"
    ],
    "start_line": 59,
    "end_line": 73,
    "text": "def clean_text(text):\n    # Clean the text\n    text = text.strip()\n    text = text.replace(\"\\xa0\", \"\")\n\n    # Replace all chinese symbols with their english counterparts\n    text = REPLACE_SYMBOL_REGEX.sub(lambda x: SYMBOLS_MAPPING[x.group()], text)\n\n    # Remove emojis\n    text = EMOJI_REGEX.sub(r\"\", text)\n\n    # Remove continuous periods (...) and commas (,,,)\n    text = re.sub(r\"[.,]{2,}\", lambda m: m.group()[0], text)\n\n    return text"
  },
  {
    "id": "7ffa9a5d4ff282fed5a75880f349be6da10586a5",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/llm.py",
    "kind": "class",
    "name": "Model",
    "signature": "class Model(nn.Module, PyTorchModelHubMixin)",
    "docstring": "",
    "identifiers": [
      "_",
      "__init__",
      "_create_causal_mask",
      "_embed_audio",
      "_embed_tokens",
      "_index_causal_mask",
      "_prepare_transformer",
      "arange",
      "audio_embeddings",
      "audio_embeds",
      "audio_h",
      "audio_head",
      "audio_mask",
      "audio_num_codebooks",
      "audio_tokens",
      "audio_vocab_size",
      "b",
      "backbone",
      "backbone_attn_mask",
      "backbone_causal_mask",
      "backbone_dim",
      "backbone_flavor",
      "bias",
      "bool",
      "bsz",
      "c0_embed",
      "c0_logits",
      "c0_loss",
      "c0_sample",
      "c0_target",
      "c_embeds",
      "c_logits",
      "c_loss",
      "c_pos",
      "caches_are_enabled",
      "cat",
      "ci_embed",
      "ci_logits",
      "ci_sample",
      "clone",
      "codebook",
      "codebook0_head",
      "config",
      "cross_entropy",
      "curr_backbone_mask",
      "curr_decoder_mask",
      "curr_h",
      "curr_pos",
      "curr_sample",
      "decoder",
      "decoder_causal_mask",
      "decoder_dim",
      "decoder_embeds",
      "decoder_flavor",
      "decoder_h",
      "decoder_loss_weight",
      "decoder_max_seq_len",
      "device",
      "dim",
      "dims",
      "dtype",
      "einsum",
      "embedding",
      "embeds",
      "empty",
      "expand",
      "eye",
      "f",
      "flavors",
      "float",
      "forward",
      "generate_frame",
      "h",
      "i",
      "ignore_index",
      "indices",
      "input",
      "input_pos",
      "int",
      "last_h",
      "linear",
      "long",
      "loss",
      "mask",
      "masked_embeds",
      "max_batch_size",
      "max_seq_len",
      "mm",
      "model",
      "modelargs",
      "module",
      "n",
      "n_codebooks",
      "next",
      "nn",
      "padding_3d",
      "padding_mask",
      "parameter",
      "parameters",
      "projection",
      "pytorchmodelhubmixin",
      "randperm",
      "range",
      "register_buffer",
      "repeat",
      "reset_caches",
      "reshape",
      "roll",
      "s",
      "sample_topk",
      "self",
      "seq_len",
      "setup_caches",
      "shifts",
      "size",
      "sum",
      "super",
      "target_tokens",
      "temperature",
      "tensor",
      "text_embeddings",
      "text_embeds",
      "text_h",
      "text_head",
      "text_logits",
      "text_loss",
      "text_mask",
      "text_target_mask",
      "text_target_tokens",
      "text_vocab_size",
      "to",
      "tokens",
      "tokens_mask",
      "topk",
      "torch",
      "unsqueeze",
      "use_text_loss",
      "view"
    ],
    "start_line": 86,
    "end_line": 325,
    "text": "class Model(nn.Module, PyTorchModelHubMixin):\n    def __init__(self, config: ModelArgs):\n        super().__init__()\n        self.config = config\n\n        self.backbone, backbone_dim = _prepare_transformer(\n            FLAVORS[config.backbone_flavor]()\n        )\n        self.decoder, decoder_dim = _prepare_transformer(\n            FLAVORS[config.decoder_flavor]()\n        )\n\n        self.text_embeddings = nn.Embedding(config.text_vocab_size, backbone_dim)\n        self.audio_embeddings = nn.Embedding(\n            config.audio_vocab_size * config.audio_num_codebooks, backbone_dim\n        )\n\n        self.projection = nn.Linear(backbone_dim, decoder_dim, bias=False)\n        self.text_head = nn.Linear(backbone_dim, config.text_vocab_size, bias=False)\n        self.codebook0_head = nn.Linear(\n            backbone_dim, config.audio_vocab_size, bias=False\n        )\n        self.audio_head = nn.Parameter(\n            torch.empty(\n                config.audio_num_codebooks - 1, decoder_dim, config.audio_vocab_size\n            )\n        )\n\n        self.decoder_loss_weight = config.decoder_loss_weight\n        self.use_text_loss = config.use_text_loss\n\n    def setup_caches(self, max_batch_size: int) -> torch.Tensor:\n        \"\"\"Setup KV caches and return a causal mask.\"\"\"\n        dtype = next(self.parameters()).dtype\n        device = next(self.parameters()).device\n\n        with device:\n            self.backbone.setup_caches(max_batch_size, dtype)\n            self.decoder.setup_caches(\n                max_batch_size,\n                dtype,\n                decoder_max_seq_len=self.config.audio_num_codebooks,\n            )\n\n        self.register_buffer(\n            \"backbone_causal_mask\",\n            _create_causal_mask(self.backbone.max_seq_len, device),\n        )\n        self.register_buffer(\n            \"decoder_causal_mask\",\n            _create_causal_mask(self.config.audio_num_codebooks, device),\n        )\n\n    def forward(self, tokens: torch.Tensor, tokens_mask: torch.Tensor):\n        \"\"\"\n        Forward pass for Sesame's CSM model.\n        This will be added to the model with `model.forward = types.MethodType(forward, model)`\n\n        Args:\n            tokens: (batch_size, seq_len, n_codebooks+1)\n            tokens_mask: (batch_size, seq_len, n_codebooks+1)\n        \"\"\"\n\n        dtype = next(self.parameters()).dtype\n        bsz, seq_len, _ = tokens.size()\n        device = tokens.device\n\n        # embed tokens\n        embeds = self._embed_tokens(tokens)  # (bsz,seq_len,17,2048)\n\n        # get targets and codebook embeddings corresponding to audio tokens\n        audio_mask = tokens_mask[:, :, 0]  # [bsz, seq_len]\n        target_tokens = tokens[audio_mask][:, :-1]  # [audio_len, n_codebooks]\n        # [audio_len, n_codebooks, embed_dim]\n        c_embeds = embeds[:, :, :-1, :][audio_mask]\n\n        # get targets corresponding to text tokens\n        text_mask = tokens_mask[:, :, -1]\n        text_target_mask = torch.roll(input=text_mask, shifts=1, dims=1)\n        text_target_tokens = tokens[text_target_mask][:, -1]\n\n        # retain just non-padding embeddings\n        masked_embeds = embeds * tokens_mask.unsqueeze(-1)\n        h = masked_embeds.sum(dim=2)\n\n        # backbone forward pass\n        # [bsz, seq_len]\n        padding_mask = tokens_mask[:, :, 0] | tokens_mask[:, :, -1]\n        # [seq_len, seq_len]\n        backbone_attn_mask = _create_causal_mask(seq_len, device)\n        # [bsz, seq_len, seq_len]\n        padding_3d = padding_mask.unsqueeze(-1) * padding_mask.unsqueeze(1)\n        backbone_attn_mask = backbone_attn_mask.unsqueeze(0) * padding_3d\n        backbone_attn_mask = backbone_attn_mask | torch.eye(\n            seq_len, device=device\n        ).bool().unsqueeze(0).expand(bsz, -1, -1)\n        input_pos = (\n            torch.arange(0, seq_len).unsqueeze(0).expand(bsz, seq_len).long().to(device)\n        )\n        h = self.backbone(h, input_pos=input_pos, mask=backbone_attn_mask).to(\n            dtype=dtype\n        )\n\n        # get backbone"
  },
  {
    "id": "7d5eabee16a54a44cf06e3c8c9b08df15db07295",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/llm.py",
    "kind": "function",
    "name": "_embed_tokens",
    "signature": "_embed_tokens(self, tokens: torch.Tensor)",
    "docstring": "",
    "identifiers": [
      "_embed_tokens",
      "arange",
      "audio_embeddings",
      "audio_embeds",
      "audio_num_codebooks",
      "audio_tokens",
      "audio_vocab_size",
      "cat",
      "config",
      "device",
      "dim",
      "reshape",
      "self",
      "size",
      "tensor",
      "text_embeddings",
      "text_embeds",
      "tokens",
      "torch",
      "unsqueeze",
      "view"
    ],
    "start_line": 314,
    "end_line": 325,
    "text": "def _embed_tokens(self, tokens: torch.Tensor) -> torch.Tensor:\n        text_embeds = self.text_embeddings(tokens[:, :, -1]).unsqueeze(-2)\n\n        audio_tokens = tokens[:, :, :-1] + (\n            self.config.audio_vocab_size\n            * torch.arange(self.config.audio_num_codebooks, device=tokens.device)\n        )\n        audio_embeds = self.audio_embeddings(audio_tokens.view(-1)).reshape(\n            tokens.size(0), tokens.size(1), self.config.audio_num_codebooks, -1\n        )\n\n        return torch.cat([audio_embeds, text_embeds], dim=-2)"
  },
  {
    "id": "1a4a583533015cd98d774a990aa55811c2abdc08",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/llm.py",
    "kind": "function",
    "name": "_embed_audio",
    "signature": "_embed_audio(self, codebook: int, tokens: torch.Tensor)",
    "docstring": "",
    "identifiers": [
      "_embed_audio",
      "audio_embeddings",
      "audio_vocab_size",
      "codebook",
      "config",
      "int",
      "self",
      "tensor",
      "tokens",
      "torch"
    ],
    "start_line": 311,
    "end_line": 312,
    "text": "def _embed_audio(self, codebook: int, tokens: torch.Tensor) -> torch.Tensor:\n        return self.audio_embeddings(tokens + codebook * self.config.audio_vocab_size)"
  },
  {
    "id": "5254010b836fd16183f0b6dfb9db79f6774c5343",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/llm.py",
    "kind": "function",
    "name": "reset_caches",
    "signature": "reset_caches(self)",
    "docstring": "",
    "identifiers": [
      "backbone",
      "decoder",
      "reset_caches",
      "self"
    ],
    "start_line": 307,
    "end_line": 309,
    "text": "def reset_caches(self):\n        self.backbone.reset_caches()\n        self.decoder.reset_caches()"
  },
  {
    "id": "33f362b1965f672e26e6554beacf009ecdcba87b",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/llm.py",
    "kind": "function",
    "name": "generate_frame",
    "signature": "generate_frame(\n        self,\n        tokens: torch.Tensor,\n        tokens_mask: torch.Tensor,\n        input_pos: torch.Tensor,\n        temperature: float,\n        topk: int,\n    )",
    "docstring": "Args:\n            tokens: (batch_size, seq_len, audio_num_codebooks+1)\n            tokens_mask: (batch_size, seq_len, audio_num_codebooks+1)\n            input_pos: (batch_size, seq_len) positions for each token\n            mask: (batch_size, seq_len, max_seq_len\n\n        Returns:\n            (batch_size, audio_num_codebooks) sampled tokens",
    "identifiers": [
      "_",
      "_embed_audio",
      "_embed_tokens",
      "_index_causal_mask",
      "arange",
      "audio_head",
      "audio_num_codebooks",
      "b",
      "backbone",
      "backbone_causal_mask",
      "c0_embed",
      "c0_logits",
      "c0_sample",
      "caches_are_enabled",
      "cat",
      "ci_embed",
      "ci_logits",
      "ci_sample",
      "clone",
      "codebook0_head",
      "config",
      "curr_backbone_mask",
      "curr_decoder_mask",
      "curr_h",
      "curr_pos",
      "curr_sample",
      "decoder",
      "decoder_causal_mask",
      "decoder_h",
      "device",
      "dim",
      "dtype",
      "embeds",
      "float",
      "generate_frame",
      "h",
      "i",
      "input_pos",
      "int",
      "last_h",
      "mask",
      "masked_embeds",
      "mm",
      "next",
      "parameters",
      "projection",
      "range",
      "repeat",
      "reset_caches",
      "s",
      "sample_topk",
      "self",
      "size",
      "sum",
      "temperature",
      "tensor",
      "to",
      "tokens",
      "tokens_mask",
      "topk",
      "torch",
      "unsqueeze"
    ],
    "start_line": 249,
    "end_line": 305,
    "text": "def generate_frame(\n        self,\n        tokens: torch.Tensor,\n        tokens_mask: torch.Tensor,\n        input_pos: torch.Tensor,\n        temperature: float,\n        topk: int,\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            tokens: (batch_size, seq_len, audio_num_codebooks+1)\n            tokens_mask: (batch_size, seq_len, audio_num_codebooks+1)\n            input_pos: (batch_size, seq_len) positions for each token\n            mask: (batch_size, seq_len, max_seq_len\n\n        Returns:\n            (batch_size, audio_num_codebooks) sampled tokens\n        \"\"\"\n        dtype = next(self.parameters()).dtype\n        b, s, _ = tokens.size()\n\n        assert self.backbone.caches_are_enabled(), \"backbone caches are not enabled\"\n        curr_backbone_mask = _index_causal_mask(self.backbone_causal_mask, input_pos)\n        embeds = self._embed_tokens(tokens)\n        masked_embeds = embeds * tokens_mask.unsqueeze(-1)\n        h = masked_embeds.sum(dim=2)\n        h = self.backbone(h, input_pos=input_pos, mask=curr_backbone_mask).to(\n            dtype=dtype\n        )\n\n        last_h = h[:, -1, :]\n        c0_logits = self.codebook0_head(last_h)\n        c0_sample = sample_topk(c0_logits, topk, temperature)\n        c0_embed = self._embed_audio(0, c0_sample)\n        curr_h = torch.cat([last_h.unsqueeze(1), c0_embed], dim=1)\n        curr_sample = c0_sample.clone()\n        curr_pos = (\n            torch.arange(0, curr_h.size(1), device=curr_h.device)\n            .unsqueeze(0)\n            .repeat(curr_h.size(0), 1)\n        )\n\n        # Decoder caches must be reset every frame.\n        self.decoder.reset_caches()\n        for i in range(1, self.config.audio_num_codebooks):\n            curr_decoder_mask = _index_causal_mask(self.decoder_causal_mask, curr_pos)\n            decoder_h = self.decoder(\n                self.projection(curr_h), input_pos=curr_pos, mask=curr_decoder_mask\n            ).to(dtype=dtype)\n            ci_logits = torch.mm(decoder_h[:, -1, :], self.audio_head[i - 1])\n            ci_sample = sample_topk(ci_logits, 10, 0.75)  # fix to 10 and 0.75\n            ci_embed = self._embed_audio(i, ci_sample)\n            curr_h = ci_embed\n            curr_sample = torch.cat([curr_sample, ci_sample], dim=1)\n            curr_pos = curr_pos[:, -1:] + 1\n\n        return curr_sample"
  },
  {
    "id": "f4d6ab6c91888f0be3030ab04f6a01e2f22ce7ad",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/llm.py",
    "kind": "function",
    "name": "forward",
    "signature": "forward(self, tokens: torch.Tensor, tokens_mask: torch.Tensor)",
    "docstring": "Forward pass for Sesame's CSM model.\n        This will be added to the model with `model.forward = types.MethodType(forward, model)`\n\n        Args:\n            tokens: (batch_size, seq_len, n_codebooks+1)\n            tokens_mask: (batch_size, seq_len, n_codebooks+1)",
    "identifiers": [
      "_",
      "_create_causal_mask",
      "_embed_tokens",
      "arange",
      "audio_h",
      "audio_head",
      "audio_mask",
      "backbone",
      "backbone_attn_mask",
      "bool",
      "bsz",
      "c0_logits",
      "c0_loss",
      "c0_target",
      "c_embeds",
      "c_logits",
      "c_loss",
      "c_pos",
      "cat",
      "codebook0_head",
      "cross_entropy",
      "decoder",
      "decoder_causal_mask",
      "decoder_embeds",
      "decoder_h",
      "decoder_loss_weight",
      "device",
      "dim",
      "dims",
      "dtype",
      "einsum",
      "embeds",
      "expand",
      "eye",
      "f",
      "forward",
      "h",
      "ignore_index",
      "indices",
      "input",
      "input_pos",
      "long",
      "loss",
      "mask",
      "masked_embeds",
      "n",
      "n_codebooks",
      "next",
      "padding_3d",
      "padding_mask",
      "parameters",
      "projection",
      "randperm",
      "reshape",
      "roll",
      "self",
      "seq_len",
      "shifts",
      "size",
      "sum",
      "target_tokens",
      "tensor",
      "text_h",
      "text_head",
      "text_logits",
      "text_loss",
      "text_mask",
      "text_target_mask",
      "text_target_tokens",
      "to",
      "tokens",
      "tokens_mask",
      "torch",
      "unsqueeze",
      "use_text_loss"
    ],
    "start_line": 139,
    "end_line": 247,
    "text": "def forward(self, tokens: torch.Tensor, tokens_mask: torch.Tensor):\n        \"\"\"\n        Forward pass for Sesame's CSM model.\n        This will be added to the model with `model.forward = types.MethodType(forward, model)`\n\n        Args:\n            tokens: (batch_size, seq_len, n_codebooks+1)\n            tokens_mask: (batch_size, seq_len, n_codebooks+1)\n        \"\"\"\n\n        dtype = next(self.parameters()).dtype\n        bsz, seq_len, _ = tokens.size()\n        device = tokens.device\n\n        # embed tokens\n        embeds = self._embed_tokens(tokens)  # (bsz,seq_len,17,2048)\n\n        # get targets and codebook embeddings corresponding to audio tokens\n        audio_mask = tokens_mask[:, :, 0]  # [bsz, seq_len]\n        target_tokens = tokens[audio_mask][:, :-1]  # [audio_len, n_codebooks]\n        # [audio_len, n_codebooks, embed_dim]\n        c_embeds = embeds[:, :, :-1, :][audio_mask]\n\n        # get targets corresponding to text tokens\n        text_mask = tokens_mask[:, :, -1]\n        text_target_mask = torch.roll(input=text_mask, shifts=1, dims=1)\n        text_target_tokens = tokens[text_target_mask][:, -1]\n\n        # retain just non-padding embeddings\n        masked_embeds = embeds * tokens_mask.unsqueeze(-1)\n        h = masked_embeds.sum(dim=2)\n\n        # backbone forward pass\n        # [bsz, seq_len]\n        padding_mask = tokens_mask[:, :, 0] | tokens_mask[:, :, -1]\n        # [seq_len, seq_len]\n        backbone_attn_mask = _create_causal_mask(seq_len, device)\n        # [bsz, seq_len, seq_len]\n        padding_3d = padding_mask.unsqueeze(-1) * padding_mask.unsqueeze(1)\n        backbone_attn_mask = backbone_attn_mask.unsqueeze(0) * padding_3d\n        backbone_attn_mask = backbone_attn_mask | torch.eye(\n            seq_len, device=device\n        ).bool().unsqueeze(0).expand(bsz, -1, -1)\n        input_pos = (\n            torch.arange(0, seq_len).unsqueeze(0).expand(bsz, seq_len).long().to(device)\n        )\n        h = self.backbone(h, input_pos=input_pos, mask=backbone_attn_mask).to(\n            dtype=dtype\n        )\n\n        # get backbone embeddings used for audio codebook prediction predict first codebook and compute loss\n        audio_mask = torch.roll(audio_mask, -1, 1)  # shift audio mask to the right by 1\n        audio_h = h[audio_mask]  # [audio_len, embed_dim]\n        c0_logits = self.codebook0_head(audio_h)  # [audio_len, audio_vocab_size]\n        c0_target = target_tokens[:, 0]  # [audio_len]\n        c0_loss = F.cross_entropy(c0_logits, c0_target)\n\n        # predict text loss\n        text_h = h[text_mask]\n        text_logits = self.text_head(text_h)\n        text_loss = F.cross_entropy(text_logits, text_target_tokens, ignore_index=0)\n\n        # \"compute amortization\" (train decoder on random 1/8 subset of audio tokens)\n        # important change to 1/8\n        indices = torch.randperm(c_embeds.size(0))[: c_embeds.size(0) // 8]\n        # [audio_len//16, n_codebooks-1, embed_dim]\n        c_embeds = c_embeds[indices][:, :-1, :]\n        audio_h = audio_h[indices]  # [audio_len//16, embed_dim]\n        target_tokens = target_tokens[indices][:, 1:]  # [audio_len//16, n_codebooks-1]\n\n        # concatenate backbone embeddings and codebook embeddings for decoder input\n        # [audio_len//16, n_codebooks, embed_dim]\n        decoder_embeds = torch.cat([audio_h.unsqueeze(1), c_embeds], dim=1)\n        N, n_codebooks, _ = decoder_embeds.size()\n        c_pos = (\n            torch.arange(0, n_codebooks)\n            .unsqueeze(0)\n            .expand(N, n_codebooks)\n            .long()\n            .to(device)\n        )\n\n        decoder_causal_mask = _create_causal_mask(\n            decoder_embeds.size(1), device\n        ).expand(N, -1, -1)\n        decoder_h = self.decoder(\n            self.projection(decoder_embeds), input_pos=c_pos, mask=decoder_causal_mask\n        ).to(dtype=dtype)\n        c_logits = torch.einsum(\"bsd,sdv->bsv\", decoder_h[:, 1:, :], self.audio_head)\n\n        c_loss = F.cross_entropy(\n            c_logits.reshape(-1, c_lo"
  },
  {
    "id": "96e939417d3524fdc45124d8305524bf8f21fdfa",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/llm.py",
    "kind": "function",
    "name": "setup_caches",
    "signature": "setup_caches(self, max_batch_size: int)",
    "docstring": "Setup KV caches and return a causal mask.",
    "identifiers": [
      "_create_causal_mask",
      "audio_num_codebooks",
      "backbone",
      "config",
      "decoder",
      "decoder_max_seq_len",
      "device",
      "dtype",
      "int",
      "max_batch_size",
      "max_seq_len",
      "next",
      "parameters",
      "register_buffer",
      "self",
      "setup_caches",
      "tensor",
      "torch"
    ],
    "start_line": 117,
    "end_line": 137,
    "text": "def setup_caches(self, max_batch_size: int) -> torch.Tensor:\n        \"\"\"Setup KV caches and return a causal mask.\"\"\"\n        dtype = next(self.parameters()).dtype\n        device = next(self.parameters()).device\n\n        with device:\n            self.backbone.setup_caches(max_batch_size, dtype)\n            self.decoder.setup_caches(\n                max_batch_size,\n                dtype,\n                decoder_max_seq_len=self.config.audio_num_codebooks,\n            )\n\n        self.register_buffer(\n            \"backbone_causal_mask\",\n            _create_causal_mask(self.backbone.max_seq_len, device),\n        )\n        self.register_buffer(\n            \"decoder_causal_mask\",\n            _create_causal_mask(self.config.audio_num_codebooks, device),\n        )"
  },
  {
    "id": "610bcfb5fae00fc3a479172e00b7d5ee22b51377",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/llm.py",
    "kind": "function",
    "name": "__init__",
    "signature": "__init__(self, config: ModelArgs)",
    "docstring": "",
    "identifiers": [
      "__init__",
      "_prepare_transformer",
      "audio_embeddings",
      "audio_head",
      "audio_num_codebooks",
      "audio_vocab_size",
      "backbone",
      "backbone_dim",
      "backbone_flavor",
      "bias",
      "codebook0_head",
      "config",
      "decoder",
      "decoder_dim",
      "decoder_flavor",
      "decoder_loss_weight",
      "embedding",
      "empty",
      "flavors",
      "linear",
      "modelargs",
      "nn",
      "parameter",
      "projection",
      "self",
      "super",
      "text_embeddings",
      "text_head",
      "text_vocab_size",
      "torch",
      "use_text_loss"
    ],
    "start_line": 87,
    "end_line": 115,
    "text": "def __init__(self, config: ModelArgs):\n        super().__init__()\n        self.config = config\n\n        self.backbone, backbone_dim = _prepare_transformer(\n            FLAVORS[config.backbone_flavor]()\n        )\n        self.decoder, decoder_dim = _prepare_transformer(\n            FLAVORS[config.decoder_flavor]()\n        )\n\n        self.text_embeddings = nn.Embedding(config.text_vocab_size, backbone_dim)\n        self.audio_embeddings = nn.Embedding(\n            config.audio_vocab_size * config.audio_num_codebooks, backbone_dim\n        )\n\n        self.projection = nn.Linear(backbone_dim, decoder_dim, bias=False)\n        self.text_head = nn.Linear(backbone_dim, config.text_vocab_size, bias=False)\n        self.codebook0_head = nn.Linear(\n            backbone_dim, config.audio_vocab_size, bias=False\n        )\n        self.audio_head = nn.Parameter(\n            torch.empty(\n                config.audio_num_codebooks - 1, decoder_dim, config.audio_vocab_size\n            )\n        )\n\n        self.decoder_loss_weight = config.decoder_loss_weight\n        self.use_text_loss = config.use_text_loss"
  },
  {
    "id": "6c1eb2d3fa2559247ca190a61091fbdb218afe1b",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/llm.py",
    "kind": "class",
    "name": "ModelArgs",
    "signature": "class ModelArgs",
    "docstring": "",
    "identifiers": [
      "audio_num_codebooks",
      "audio_vocab_size",
      "backbone_flavor",
      "bool",
      "decoder_flavor",
      "decoder_loss_weight",
      "float",
      "int",
      "modelargs",
      "str",
      "text_vocab_size",
      "use_text_loss"
    ],
    "start_line": 76,
    "end_line": 83,
    "text": "class ModelArgs:\n    backbone_flavor: str\n    decoder_flavor: str\n    text_vocab_size: int\n    audio_vocab_size: int\n    audio_num_codebooks: int\n    decoder_loss_weight: float\n    use_text_loss: bool"
  },
  {
    "id": "70efecca7694d37f0b7ea2d6be9a4017570fa992",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/llm.py",
    "kind": "function",
    "name": "sample_top_nsigma",
    "signature": "sample_top_nsigma(logits: torch.Tensor, n: float, temperature: float)",
    "docstring": "_summary_\n\n    Args:\n        logits (torch.Tensor): _description_\n        n (float): _description_\n        temperature (float): _description_\n\n    Returns:\n        _type_: _description_",
    "identifiers": [
      "_multinomial_sample_one_no_sync",
      "dim",
      "float",
      "functional",
      "keepdim",
      "logits",
      "max",
      "n",
      "nn",
      "probs",
      "sample_token",
      "sample_top_nsigma",
      "softmax",
      "std",
      "temperature",
      "tensor",
      "threshold",
      "torch",
      "values"
    ],
    "start_line": 52,
    "end_line": 72,
    "text": "def sample_top_nsigma(logits: torch.Tensor, n: float, temperature: float):\n    \"\"\"_summary_\n\n    Args:\n        logits (torch.Tensor): _description_\n        n (float): _description_\n        temperature (float): _description_\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n    logits = logits / temperature\n    threshold = logits.max(dim=-1, keepdim=True).values - n * logits.std(\n        dim=-1, keepdim=True\n    )\n    logits[logits < threshold] = float(\"-inf\")\n    # scores_processed = torch.nn.functional.log_softmax(logits, dim=-1)\n    probs = torch.nn.functional.softmax(logits, dim=-1)\n\n    sample_token = _multinomial_sample_one_no_sync(probs)\n    return sample_token"
  },
  {
    "id": "5e233f03221b229f1ff616608e60c6f5b8e13fa2",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/llm.py",
    "kind": "function",
    "name": "sample_topk",
    "signature": "sample_topk(logits: torch.Tensor, topk: int, temperature: float)",
    "docstring": "",
    "identifiers": [
      "_multinomial_sample_one_no_sync",
      "dim",
      "filter_value",
      "float",
      "functional",
      "indices_to_remove",
      "int",
      "log_softmax",
      "logits",
      "masked_fill",
      "nn",
      "probs",
      "sample_token",
      "sample_topk",
      "scores_processed",
      "softmax",
      "temperature",
      "tensor",
      "topk",
      "torch"
    ],
    "start_line": 39,
    "end_line": 49,
    "text": "def sample_topk(logits: torch.Tensor, topk: int, temperature: float):\n    logits = logits / temperature\n\n    filter_value: float = -float(\"Inf\")\n    indices_to_remove = logits < torch.topk(logits, topk)[0][..., -1, None]\n    scores_processed = logits.masked_fill(indices_to_remove, filter_value)\n    scores_processed = torch.nn.functional.log_softmax(scores_processed, dim=-1)\n    probs = torch.nn.functional.softmax(scores_processed, dim=-1)\n\n    sample_token = _multinomial_sample_one_no_sync(probs)\n    return sample_token"
  },
  {
    "id": "c3f9d7b03cb8f955a7b2cbfb2f0b01a7879327b9",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/llm.py",
    "kind": "function",
    "name": "_multinomial_sample_one_no_sync",
    "signature": "_multinomial_sample_one_no_sync(probs)",
    "docstring": "Does multinomial sampling without a cuda synchronization",
    "identifiers": [
      "_multinomial_sample_one_no_sync",
      "argmax",
      "dim",
      "dtype",
      "empty_like",
      "exponential_",
      "int",
      "keepdim",
      "probs",
      "q",
      "to",
      "torch"
    ],
    "start_line": 34,
    "end_line": 36,
    "text": "def _multinomial_sample_one_no_sync(probs):\n    q = torch.empty_like(probs).exponential_(1)\n    return torch.argmax(probs / q, dim=-1, keepdim=True).to(dtype=torch.int)"
  },
  {
    "id": "77bb2a258e5a005f3d456c0835cd6021a03e6708",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/llm.py",
    "kind": "function",
    "name": "_index_causal_mask",
    "signature": "_index_causal_mask(mask: torch.Tensor, input_pos: torch.Tensor)",
    "docstring": "Args:\n        mask: (max_seq_len, max_seq_len)\n        input_pos: (batch_size, seq_len)\n\n    Returns:\n        (batch_size, seq_len, max_seq_len)",
    "identifiers": [
      "_index_causal_mask",
      "input_pos",
      "mask",
      "r",
      "tensor",
      "torch"
    ],
    "start_line": 20,
    "end_line": 30,
    "text": "def _index_causal_mask(mask: torch.Tensor, input_pos: torch.Tensor):\n    \"\"\"\n    Args:\n        mask: (max_seq_len, max_seq_len)\n        input_pos: (batch_size, seq_len)\n\n    Returns:\n        (batch_size, seq_len, max_seq_len)\n    \"\"\"\n    r = mask[input_pos, :]\n    return r"
  },
  {
    "id": "3dab486b0b26c0b5b9041b5f176784060a0a8a40",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/llm.py",
    "kind": "function",
    "name": "_create_causal_mask",
    "signature": "_create_causal_mask(seq_len: int, device: torch.device)",
    "docstring": "",
    "identifiers": [
      "_create_causal_mask",
      "bool",
      "device",
      "dtype",
      "int",
      "ones",
      "seq_len",
      "torch",
      "tril"
    ],
    "start_line": 16,
    "end_line": 17,
    "text": "def _create_causal_mask(seq_len: int, device: torch.device):\n    return torch.tril(torch.ones(seq_len, seq_len, dtype=torch.bool, device=device))"
  },
  {
    "id": "7c0329252ce0979d9044d67ed5cdee5330382ede",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/llm.py",
    "kind": "function",
    "name": "_prepare_transformer",
    "signature": "_prepare_transformer(model)",
    "docstring": "",
    "identifiers": [
      "_prepare_transformer",
      "embed_dim",
      "embedding_dim",
      "identity",
      "model",
      "nn",
      "output",
      "tok_embeddings"
    ],
    "start_line": 9,
    "end_line": 13,
    "text": "def _prepare_transformer(model):\n    embed_dim = model.tok_embeddings.embedding_dim\n    model.tok_embeddings = nn.Identity()\n    model.output = nn.Identity()\n    return model, embed_dim"
  },
  {
    "id": "0ccf445fd02a4d00de291c974780ecf9b1623eed",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/utils.py",
    "kind": "function",
    "name": "read_jsonl",
    "signature": "read_jsonl(path)",
    "docstring": "",
    "identifiers": [
      "append",
      "data",
      "data_list",
      "expanduser",
      "f",
      "json",
      "json_str",
      "line",
      "loads",
      "open",
      "os",
      "path",
      "read",
      "read_jsonl",
      "splitlines"
    ],
    "start_line": 331,
    "end_line": 339,
    "text": "def read_jsonl(path):\n    path = os.path.expanduser(path)\n    with open(path, \"r\") as f:\n        json_str = f.read()\n    data_list = []\n    for line in json_str.splitlines():\n        data = json.loads(line)\n        data_list.append(data)\n    return data_list"
  },
  {
    "id": "b9d25ae766c0866ddc37fad445f246e596ccbe90",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/utils.py",
    "kind": "function",
    "name": "get_grad_norm",
    "signature": "get_grad_norm(model)",
    "docstring": "",
    "identifiers": [
      "data",
      "get_grad_norm",
      "grad",
      "item",
      "model",
      "name",
      "named_parameters",
      "norm",
      "num",
      "p",
      "param_norm",
      "print",
      "total_norm"
    ],
    "start_line": 316,
    "end_line": 328,
    "text": "def get_grad_norm(model):\n    total_norm = 0\n    num = 0\n    for name, p in model.named_parameters():\n        try:\n            param_norm = p.grad.data.norm(2)\n            total_norm += param_norm.item() ** 2\n            num += 1\n        except:\n            print(name)\n    total_norm = total_norm ** (1.0 / 2)\n    total_norm = total_norm / num\n    return total_norm"
  },
  {
    "id": "4e81ada5cfab9c033373d199ac8e84b75f992dca",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/utils.py",
    "kind": "function",
    "name": "summarize",
    "signature": "summarize(\n    writer,\n    global_step,\n    scalars={},\n    histograms={},\n    images={},\n    audios={},\n    audio_sampling_rate=22050,\n)",
    "docstring": "",
    "identifiers": [
      "add_audio",
      "add_histogram",
      "add_image",
      "add_scalar",
      "audio_sampling_rate",
      "audios",
      "dataformats",
      "global_step",
      "histograms",
      "images",
      "items",
      "k",
      "scalars",
      "summarize",
      "v",
      "writer"
    ],
    "start_line": 297,
    "end_line": 313,
    "text": "def summarize(\n    writer,\n    global_step,\n    scalars={},\n    histograms={},\n    images={},\n    audios={},\n    audio_sampling_rate=22050,\n):\n    for k, v in scalars.items():\n        writer.add_scalar(k, v, global_step)\n    for k, v in histograms.items():\n        writer.add_histogram(k, v, global_step)\n    for k, v in images.items():\n        writer.add_image(k, v, global_step, dataformats=\"HWC\")\n    for k, v in audios.items():\n        writer.add_audio(k, v, global_step, audio_sampling_rate)"
  },
  {
    "id": "a98f55993f3fb1e554f7e0741e5978834ae4bc3e",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/utils.py",
    "kind": "function",
    "name": "load_llm_model",
    "signature": "load_llm_model(\n    configs,\n    checkpoint_path: Union[str, Path] = None,\n    device: Union[str, torch.device] = \"cuda\",\n)",
    "docstring": "Load model, add forward method, and move to device.\n\n    Args:\n        model_name_or_checkpoint_path: Name or path of pretrained model or checkpoint.\n        device: Device to move the model to.\n        decoder_loss_weight: Decoder loss weight.",
    "identifiers": [
      "audio_num_codebooks",
      "audio_vocab_size",
      "backbone_flavor",
      "checkpoint_path",
      "configs",
      "decoder_flavor",
      "decoder_loss_weight",
      "device",
      "exists",
      "init_weights",
      "load",
      "load_llm_model",
      "load_state_dict",
      "map_location",
      "model",
      "model_arg",
      "modelargs",
      "os",
      "path",
      "state_dict",
      "str",
      "text_vocab_size",
      "to",
      "torch",
      "union",
      "use_text_loss",
      "weights_only"
    ],
    "start_line": 261,
    "end_line": 294,
    "text": "def load_llm_model(\n    configs,\n    checkpoint_path: Union[str, Path] = None,\n    device: Union[str, torch.device] = \"cuda\",\n) -> Model:\n    \"\"\"Load model, add forward method, and move to device.\n\n    Args:\n        model_name_or_checkpoint_path: Name or path of pretrained model or checkpoint.\n        device: Device to move the model to.\n        decoder_loss_weight: Decoder loss weight.\n    \"\"\"\n\n    model_arg = ModelArgs(\n        backbone_flavor=configs[\"llm_models\"][\"backbone_flavor\"],\n        decoder_flavor=configs[\"llm_models\"][\"decoder_flavor\"],\n        text_vocab_size=configs[\"llm_models\"][\"text_vocab_size\"],\n        audio_vocab_size=configs[\"llm_models\"][\"audio_vocab_size\"],\n        audio_num_codebooks=configs[\"llm_models\"][\"audio_num_codebooks\"],\n        decoder_loss_weight=configs[\"llm_models\"][\"decoder_loss_weight\"],\n        use_text_loss=True,\n    )\n    model = Model(model_arg)\n\n    if checkpoint_path and os.path.exists(checkpoint_path):\n        state_dict = torch.load(\n            checkpoint_path, map_location=\"cpu\", weights_only=False\n        )[\"model\"]\n        model.load_state_dict(state_dict)\n    else:\n        model = init_weights(model)\n\n    model = model.to(device=device)\n    return model"
  },
  {
    "id": "1ffc701cef908235286acb66ad683de3f73778d0",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/utils.py",
    "kind": "function",
    "name": "load_model",
    "signature": "load_model(\n    configs,\n    checkpoint_path: Union[str, Path] = None,\n    device: Union[str, torch.device] = \"cuda\",\n)",
    "docstring": "Load model, add forward method, and move to device.\n\n    Args:\n        model_name_or_checkpoint_path: Name or path of pretrained model or checkpoint.\n        device: Device to move the model to.\n        decoder_loss_weight: Decoder loss weight.",
    "identifiers": [
      "audio_num_codebooks",
      "audio_vocab_size",
      "backbone_flavor",
      "checkpoint_path",
      "configs",
      "decoder_flavor",
      "decoder_loss_weight",
      "device",
      "exists",
      "init_weights",
      "load",
      "load_model",
      "load_state_dict",
      "map_location",
      "model",
      "model_arg",
      "modelargs",
      "os",
      "path",
      "state_dict",
      "str",
      "text_vocab_size",
      "to",
      "torch",
      "union",
      "use_text_loss",
      "weights_only"
    ],
    "start_line": 225,
    "end_line": 258,
    "text": "def load_model(\n    configs,\n    checkpoint_path: Union[str, Path] = None,\n    device: Union[str, torch.device] = \"cuda\",\n) -> Model:\n    \"\"\"Load model, add forward method, and move to device.\n\n    Args:\n        model_name_or_checkpoint_path: Name or path of pretrained model or checkpoint.\n        device: Device to move the model to.\n        decoder_loss_weight: Decoder loss weight.\n    \"\"\"\n\n    model_arg = ModelArgs(\n        backbone_flavor=configs[\"models\"][\"backbone_flavor\"],\n        decoder_flavor=configs[\"models\"][\"decoder_flavor\"],\n        text_vocab_size=configs[\"models\"][\"text_vocab_size\"],\n        audio_vocab_size=configs[\"models\"][\"audio_vocab_size\"],\n        audio_num_codebooks=configs[\"models\"][\"audio_num_codebooks\"],\n        decoder_loss_weight=configs[\"models\"][\"decoder_loss_weight\"],\n        use_text_loss=True,\n    )\n    model = Model(model_arg)\n\n    if checkpoint_path and os.path.exists(checkpoint_path):\n        state_dict = torch.load(\n            checkpoint_path, map_location=\"cpu\", weights_only=False\n        )[\"model\"]\n        model.load_state_dict(state_dict)\n    else:\n        model = init_weights(model)\n\n    model = model.to(device=device)\n    return model"
  },
  {
    "id": "5afec1a76c1bebdb2d8edcaaff6364fd52679cf7",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/utils.py",
    "kind": "function",
    "name": "init_weights",
    "signature": "init_weights(model: nn.Module)",
    "docstring": "Initialize the weights of the model.\n    - Xavier uniform initialization for linear layers\n    - Normal initialization for embeddings\n    - Xavier uniform initialization for parameters",
    "identifiers": [
      "_init_weights",
      "apply",
      "audio_head",
      "bias",
      "data",
      "embedding",
      "init",
      "init_weights",
      "isinstance",
      "linear",
      "m",
      "mean",
      "model",
      "module",
      "nn",
      "normal_",
      "parameter",
      "std",
      "weight",
      "xavier_uniform_",
      "zeros_"
    ],
    "start_line": 199,
    "end_line": 222,
    "text": "def init_weights(model: nn.Module):\n    \"\"\"\n    Initialize the weights of the model.\n    - Xavier uniform initialization for linear layers\n    - Normal initialization for embeddings\n    - Xavier uniform initialization for parameters\n    \"\"\"\n\n    def _init_weights(m):\n        if isinstance(m, nn.Linear):\n            nn.init.xavier_uniform_(m.weight)\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n        elif isinstance(m, nn.Embedding):\n            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n        elif isinstance(m, nn.Parameter):\n            nn.init.xavier_uniform_(m.data)\n\n    model.apply(_init_weights)\n\n    # Special handling for audio_head because it's nn.Parameter directly\n    nn.init.xavier_uniform_(model.audio_head)\n\n    return model"
  },
  {
    "id": "6861cf779ee955cf8b3daef7de54ecda54ce7cdf",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/utils.py",
    "kind": "function",
    "name": "_init_weights",
    "signature": "_init_weights(m)",
    "docstring": "",
    "identifiers": [
      "_init_weights",
      "bias",
      "data",
      "embedding",
      "init",
      "isinstance",
      "linear",
      "m",
      "mean",
      "nn",
      "normal_",
      "parameter",
      "std",
      "weight",
      "xavier_uniform_",
      "zeros_"
    ],
    "start_line": 207,
    "end_line": 215,
    "text": "def _init_weights(m):\n        if isinstance(m, nn.Linear):\n            nn.init.xavier_uniform_(m.weight)\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n        elif isinstance(m, nn.Embedding):\n            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n        elif isinstance(m, nn.Parameter):\n            nn.init.xavier_uniform_(m.data)"
  },
  {
    "id": "fca285a82fba052d22447cf63401b40a59dbff3f",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/utils.py",
    "kind": "function",
    "name": "load_custom_tokenizer",
    "signature": "load_custom_tokenizer(qwen2_tokenizer_path: str)",
    "docstring": "",
    "identifiers": [
      "add_special_tokens",
      "additional_special_tokens",
      "autotokenizer",
      "from_pretrained",
      "load_custom_tokenizer",
      "qwen2_tokenizer_path",
      "special_tokens_dict",
      "str",
      "tok"
    ],
    "start_line": 190,
    "end_line": 196,
    "text": "def load_custom_tokenizer(qwen2_tokenizer_path: str):\n    tok = AutoTokenizer.from_pretrained(qwen2_tokenizer_path)\n    special_tokens_dict = {\n        \"additional_special_tokens\": additional_special_tokens,\n    }\n    tok.add_special_tokens(special_tokens_dict)\n    return tok"
  },
  {
    "id": "6c75e647e0021eb2a209a1f970b8f87819c254a6",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/utils.py",
    "kind": "class",
    "name": "WarmupDecayLR",
    "signature": "class WarmupDecayLR(LambdaLR)",
    "docstring": "Learning rate scheduler with a linear warmup and specificable decay.",
    "identifiers": [
      "__init__",
      "cos",
      "decay_type",
      "float",
      "int",
      "lambdalr",
      "last_epoch",
      "lr_lambda",
      "optimizer",
      "pi",
      "self",
      "step",
      "str",
      "super",
      "tensor",
      "torch",
      "total_steps",
      "valueerror",
      "warmup_steps",
      "warmupdecaylr"
    ],
    "start_line": 20,
    "end_line": 59,
    "text": "class WarmupDecayLR(LambdaLR):\n    \"\"\"\n    Learning rate scheduler with a linear warmup and specificable decay.\n    \"\"\"\n\n    def __init__(\n        self, optimizer, warmup_steps: int, total_steps: int, decay_type: str = \"linear\"\n    ):\n        self.warmup_steps = warmup_steps\n        self.total_steps = total_steps\n        self.decay_type = decay_type\n        super().__init__(optimizer, self.lr_lambda, last_epoch=-1)\n\n    def lr_lambda(self, step: int) -> float:\n        if step < self.warmup_steps:\n            return step / self.warmup_steps\n        else:\n            if self.decay_type == \"linear\":\n                return (self.total_steps - step) / (\n                    self.total_steps - self.warmup_steps\n                )\n            elif self.decay_type == \"constant\":\n                return 1.0\n            elif self.decay_type == \"exponential\":\n                return 0.1 ** (\n                    (step - self.warmup_steps) / (self.total_steps - self.warmup_steps)\n                )\n            elif self.decay_type == \"cosine\":\n                return 0.5 * (\n                    1\n                    + torch.cos(\n                        torch.pi\n                        * torch.tensor(\n                            (step - self.warmup_steps)\n                            / (self.total_steps - self.warmup_steps)\n                        )\n                    )\n                )\n            else:\n                raise ValueError(f\"Invalid decay type: {self.decay_type}\")"
  },
  {
    "id": "b0703a2949eb4871ee15e13ba57a40f219144dd6",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/utils.py",
    "kind": "function",
    "name": "lr_lambda",
    "signature": "lr_lambda(self, step: int)",
    "docstring": "",
    "identifiers": [
      "cos",
      "decay_type",
      "float",
      "int",
      "lr_lambda",
      "pi",
      "self",
      "step",
      "tensor",
      "torch",
      "total_steps",
      "valueerror",
      "warmup_steps"
    ],
    "start_line": 33,
    "end_line": 59,
    "text": "def lr_lambda(self, step: int) -> float:\n        if step < self.warmup_steps:\n            return step / self.warmup_steps\n        else:\n            if self.decay_type == \"linear\":\n                return (self.total_steps - step) / (\n                    self.total_steps - self.warmup_steps\n                )\n            elif self.decay_type == \"constant\":\n                return 1.0\n            elif self.decay_type == \"exponential\":\n                return 0.1 ** (\n                    (step - self.warmup_steps) / (self.total_steps - self.warmup_steps)\n                )\n            elif self.decay_type == \"cosine\":\n                return 0.5 * (\n                    1\n                    + torch.cos(\n                        torch.pi\n                        * torch.tensor(\n                            (step - self.warmup_steps)\n                            / (self.total_steps - self.warmup_steps)\n                        )\n                    )\n                )\n            else:\n                raise ValueError(f\"Invalid decay type: {self.decay_type}\")"
  },
  {
    "id": "256a1d8d12ed41ca25984be80d56d2beb5595e99",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/utils.py",
    "kind": "function",
    "name": "__init__",
    "signature": "__init__(\n        self, optimizer, warmup_steps: int, total_steps: int, decay_type: str = \"linear\"\n    )",
    "docstring": "",
    "identifiers": [
      "__init__",
      "decay_type",
      "int",
      "last_epoch",
      "lr_lambda",
      "optimizer",
      "self",
      "str",
      "super",
      "total_steps",
      "warmup_steps"
    ],
    "start_line": 25,
    "end_line": 31,
    "text": "def __init__(\n        self, optimizer, warmup_steps: int, total_steps: int, decay_type: str = \"linear\"\n    ):\n        self.warmup_steps = warmup_steps\n        self.total_steps = total_steps\n        self.decay_type = decay_type\n        super().__init__(optimizer, self.lr_lambda, last_epoch=-1)"
  },
  {
    "id": "dd1f286c9840556cef11d4f860d93eb1faa9af59",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/utils.py",
    "kind": "class",
    "name": "Segment",
    "signature": "class Segment",
    "docstring": "",
    "identifiers": [
      "audio",
      "segment",
      "speaker",
      "str",
      "tensor",
      "text",
      "torch"
    ],
    "start_line": 14,
    "end_line": 17,
    "text": "class Segment:\n    speaker: str\n    text: str\n    audio: torch.Tensor"
  },
  {
    "id": "bfee7545bd86eafc9b45f076cbcf485caff787c0",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/modules.py",
    "kind": "function",
    "name": "qwen2_7B",
    "signature": "qwen2_7B()",
    "docstring": "",
    "identifiers": [
      "attn_dropout",
      "embed_dim",
      "intermediate_dim",
      "max_seq_len",
      "norm_eps",
      "num_heads",
      "num_kv_heads",
      "num_layers",
      "qwen2",
      "qwen2_7b",
      "rope_base",
      "transformerdecoder",
      "vocab_size"
    ],
    "start_line": 69,
    "end_line": 81,
    "text": "def qwen2_7B() -> TransformerDecoder:\n    return qwen2(\n        vocab_size=152064,\n        num_layers=28,\n        num_heads=28,\n        num_kv_heads=4,\n        embed_dim=3584,\n        intermediate_dim=18944,\n        max_seq_len=4096,\n        attn_dropout=0.0,\n        norm_eps=1e-6,\n        rope_base=1000000.0,\n    )"
  },
  {
    "id": "8b37bd657d34f45fcd831b851b2607e3c80ccf2b",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/modules.py",
    "kind": "function",
    "name": "qwen2_3B",
    "signature": "qwen2_3B()",
    "docstring": "",
    "identifiers": [
      "attn_dropout",
      "embed_dim",
      "intermediate_dim",
      "max_seq_len",
      "norm_eps",
      "num_heads",
      "num_kv_heads",
      "num_layers",
      "qwen2",
      "qwen2_3b",
      "rope_base",
      "tie_word_embeddings",
      "transformerdecoder",
      "vocab_size"
    ],
    "start_line": 53,
    "end_line": 66,
    "text": "def qwen2_3B() -> TransformerDecoder:\n    return qwen2(\n        vocab_size=151936,\n        num_layers=36,\n        num_heads=16,\n        num_kv_heads=2,\n        embed_dim=2048,\n        intermediate_dim=11008,\n        max_seq_len=4096,\n        attn_dropout=0.0,\n        norm_eps=1e-6,\n        rope_base=1000000.0,\n        tie_word_embeddings=True,\n    )"
  },
  {
    "id": "a8516532c0a085de0869036cad13dacf24351a9c",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/modules.py",
    "kind": "function",
    "name": "qwen2_1_5B",
    "signature": "qwen2_1_5B()",
    "docstring": "",
    "identifiers": [
      "attn_dropout",
      "embed_dim",
      "intermediate_dim",
      "max_seq_len",
      "norm_eps",
      "num_heads",
      "num_kv_heads",
      "num_layers",
      "qwen2",
      "qwen2_1_5b",
      "rope_base",
      "tie_word_embeddings",
      "transformerdecoder",
      "vocab_size"
    ],
    "start_line": 37,
    "end_line": 50,
    "text": "def qwen2_1_5B() -> TransformerDecoder:\n    return qwen2(\n        vocab_size=151936,\n        num_layers=28,\n        num_heads=12,\n        num_kv_heads=2,\n        embed_dim=1536,\n        intermediate_dim=8960,\n        max_seq_len=4096,\n        attn_dropout=0.0,\n        norm_eps=1e-6,\n        rope_base=1000000.0,\n        tie_word_embeddings=True,\n    )"
  },
  {
    "id": "0cd196bed60a3327e563ffd91553a7e874f27748",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/modules.py",
    "kind": "function",
    "name": "qwen2_500M",
    "signature": "qwen2_500M()",
    "docstring": "",
    "identifiers": [
      "attn_dropout",
      "embed_dim",
      "intermediate_dim",
      "max_seq_len",
      "norm_eps",
      "num_heads",
      "num_kv_heads",
      "num_layers",
      "qwen2",
      "qwen2_500m",
      "rope_base",
      "tie_word_embeddings",
      "transformerdecoder",
      "vocab_size"
    ],
    "start_line": 21,
    "end_line": 34,
    "text": "def qwen2_500M() -> TransformerDecoder:\n    return qwen2(\n        vocab_size=151936,\n        num_layers=24,\n        num_heads=14,\n        num_kv_heads=2,\n        embed_dim=896,\n        intermediate_dim=4864,\n        max_seq_len=4096,\n        attn_dropout=0.0,\n        norm_eps=1e-6,\n        rope_base=1000000.0,\n        tie_word_embeddings=True,\n    )"
  },
  {
    "id": "59dedbd6911d026578343f3ab1f6c6180518b155",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/llm/modules.py",
    "kind": "function",
    "name": "qwen2_200M",
    "signature": "qwen2_200M()",
    "docstring": "",
    "identifiers": [
      "attn_dropout",
      "embed_dim",
      "intermediate_dim",
      "max_seq_len",
      "norm_eps",
      "num_heads",
      "num_kv_heads",
      "num_layers",
      "qwen2",
      "qwen2_200m",
      "rope_base",
      "tie_word_embeddings",
      "transformerdecoder",
      "vocab_size"
    ],
    "start_line": 5,
    "end_line": 18,
    "text": "def qwen2_200M() -> TransformerDecoder:\n    return qwen2(\n        vocab_size=151936,\n        num_layers=4,\n        num_heads=12,\n        num_kv_heads=2,\n        embed_dim=1536,\n        intermediate_dim=8960,\n        max_seq_len=4096,\n        attn_dropout=0.0,\n        norm_eps=1e-6,\n        rope_base=1000000.0,\n        tie_word_embeddings=True,\n    )"
  },
  {
    "id": "8cb02d45513dce4f4d892c553559f45f51f225e8",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/audio.py",
    "kind": "function",
    "name": "mel_filter_bank",
    "signature": "mel_filter_bank(\n    num_frequency_bins: int,\n    num_mel_filters: int,\n    min_frequency: float,\n    max_frequency: float,\n    sampling_rate: int,\n    norm: Optional[str] = None,\n    mel_scale: str = \"htk\",\n    triangularize_in_mel_space: bool = False,\n)",
    "docstring": "",
    "identifiers": [
      "_create_triangular_filter_bank",
      "any",
      "arange",
      "axis",
      "bool",
      "enorm",
      "expand_dims",
      "fft_bin_width",
      "fft_freqs",
      "filter_freqs",
      "float",
      "hertz_to_mel",
      "int",
      "linspace",
      "max",
      "max_frequency",
      "mel_filter_bank",
      "mel_filters",
      "mel_freqs",
      "mel_max",
      "mel_min",
      "mel_scale",
      "mel_to_hertz",
      "min_frequency",
      "ndarray",
      "norm",
      "np",
      "num_frequency_bins",
      "num_mel_filters",
      "optional",
      "sampling_rate",
      "str",
      "triangularize_in_mel_space",
      "valueerror",
      "warn",
      "warnings"
    ],
    "start_line": 102,
    "end_line": 148,
    "text": "def mel_filter_bank(\n    num_frequency_bins: int,\n    num_mel_filters: int,\n    min_frequency: float,\n    max_frequency: float,\n    sampling_rate: int,\n    norm: Optional[str] = None,\n    mel_scale: str = \"htk\",\n    triangularize_in_mel_space: bool = False,\n) -> np.ndarray:\n    if norm is not None and norm != \"slaney\":\n        raise ValueError('norm must be one of None or \"slaney\"')\n\n    # center points of the triangular mel filters\n    mel_min = hertz_to_mel(min_frequency, mel_scale=mel_scale)\n    mel_max = hertz_to_mel(max_frequency, mel_scale=mel_scale)\n    mel_freqs = np.linspace(mel_min, mel_max, num_mel_filters + 2)\n    filter_freqs = mel_to_hertz(mel_freqs, mel_scale=mel_scale)\n\n    if triangularize_in_mel_space:\n        # frequencies of FFT bins in Hz, but filters triangularized in mel space\n        fft_bin_width = sampling_rate / (num_frequency_bins * 2)\n        fft_freqs = hertz_to_mel(\n            fft_bin_width * np.arange(num_frequency_bins), mel_scale=mel_scale\n        )\n        filter_freqs = mel_freqs\n    else:\n        # frequencies of FFT bins in Hz\n        fft_freqs = np.linspace(0, sampling_rate // 2, num_frequency_bins)\n\n    mel_filters = _create_triangular_filter_bank(fft_freqs, filter_freqs)\n\n    if norm is not None and norm == \"slaney\":\n        # Slaney-style mel is scaled to be approx constant energy per channel\n        enorm = 2.0 / (\n            filter_freqs[2 : num_mel_filters + 2] - filter_freqs[:num_mel_filters]\n        )\n        mel_filters *= np.expand_dims(enorm, 0)\n\n    if (mel_filters.max(axis=0) == 0.0).any():\n        warnings.warn(\n            \"At least one mel filter has all zero values. \"\n            f\"The value for `num_mel_filters` ({num_mel_filters}) may be set too high. \"\n            f\"Or, the value for `num_frequency_bins` ({num_frequency_bins}) may be set too low.\"\n        )\n\n    return mel_filters"
  },
  {
    "id": "ae777c89dd73b4fb213788d97756800ec074e62a",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/audio.py",
    "kind": "function",
    "name": "_create_triangular_filter_bank",
    "signature": "_create_triangular_filter_bank(\n    fft_freqs: np.ndarray, filter_freqs: np.ndarray\n)",
    "docstring": "Creates a triangular filter bank.\n\n    Adapted from *torchaudio* and *librosa*.\n\n    Args:\n        fft_freqs (`np.ndarray` of shape `(num_frequency_bins,)`):\n            Discrete frequencies of the FFT bins in Hz.\n        filter_freqs (`np.ndarray` of shape `(num_mel_filters,)`):\n            Center frequencies of the triangular filters to create, in Hz.\n\n    Returns:\n        `np.ndarray` of shape `(num_frequency_bins, num_mel_filters)`",
    "identifiers": [
      "_create_triangular_filter_bank",
      "diff",
      "down_slopes",
      "expand_dims",
      "fft_freqs",
      "filter_diff",
      "filter_freqs",
      "maximum",
      "minimum",
      "ndarray",
      "np",
      "slopes",
      "up_slopes",
      "zeros"
    ],
    "start_line": 78,
    "end_line": 99,
    "text": "def _create_triangular_filter_bank(\n    fft_freqs: np.ndarray, filter_freqs: np.ndarray\n) -> np.ndarray:\n    \"\"\"\n    Creates a triangular filter bank.\n\n    Adapted from *torchaudio* and *librosa*.\n\n    Args:\n        fft_freqs (`np.ndarray` of shape `(num_frequency_bins,)`):\n            Discrete frequencies of the FFT bins in Hz.\n        filter_freqs (`np.ndarray` of shape `(num_mel_filters,)`):\n            Center frequencies of the triangular filters to create, in Hz.\n\n    Returns:\n        `np.ndarray` of shape `(num_frequency_bins, num_mel_filters)`\n    \"\"\"\n    filter_diff = np.diff(filter_freqs)\n    slopes = np.expand_dims(filter_freqs, 0) - np.expand_dims(fft_freqs, 1)\n    down_slopes = -slopes[:, :-2] / filter_diff[:-1]\n    up_slopes = slopes[:, 2:] / filter_diff[1:]\n    return np.maximum(np.zeros(1), np.minimum(down_slopes, up_slopes))"
  },
  {
    "id": "db8bd5047f6826a49f046db2485af2a74cc96dcd",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/audio.py",
    "kind": "function",
    "name": "mel_to_hertz",
    "signature": "mel_to_hertz(\n    mels: Union[float, np.ndarray], mel_scale: str = \"htk\"\n)",
    "docstring": "",
    "identifiers": [
      "exp",
      "float",
      "freq",
      "isinstance",
      "log",
      "log_region",
      "logstep",
      "mel_scale",
      "mel_to_hertz",
      "mels",
      "min_log_hertz",
      "min_log_mel",
      "ndarray",
      "np",
      "power",
      "str",
      "union",
      "valueerror"
    ],
    "start_line": 51,
    "end_line": 75,
    "text": "def mel_to_hertz(\n    mels: Union[float, np.ndarray], mel_scale: str = \"htk\"\n) -> Union[float, np.ndarray]:\n    if mel_scale not in [\"slaney\", \"htk\", \"kaldi\"]:\n        raise ValueError('mel_scale should be one of \"htk\", \"slaney\" or \"kaldi\".')\n\n    if mel_scale == \"htk\":\n        return 700.0 * (np.power(10, mels / 2595.0) - 1.0)\n    elif mel_scale == \"kaldi\":\n        return 700.0 * (np.exp(mels / 1127.0) - 1.0)\n\n    min_log_hertz = 1000.0\n    min_log_mel = 15.0\n    logstep = np.log(6.4) / 27.0\n    freq = 200.0 * mels / 3.0\n\n    if isinstance(mels, np.ndarray):\n        log_region = mels >= min_log_mel\n        freq[log_region] = min_log_hertz * np.exp(\n            logstep * (mels[log_region] - min_log_mel)\n        )\n    elif mels >= min_log_mel:\n        freq = min_log_hertz * np.exp(logstep * (mels - min_log_mel))\n\n    return freq"
  },
  {
    "id": "cd79a5973abe04b8b1ff88d82594fc8b93b0b63f",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/audio.py",
    "kind": "function",
    "name": "hertz_to_mel",
    "signature": "hertz_to_mel(\n    freq: Union[float, np.ndarray], mel_scale: str = \"htk\"\n)",
    "docstring": "",
    "identifiers": [
      "float",
      "freq",
      "hertz_to_mel",
      "isinstance",
      "log",
      "log10",
      "log_region",
      "logstep",
      "mel_scale",
      "mels",
      "min_log_hertz",
      "min_log_mel",
      "ndarray",
      "np",
      "str",
      "union",
      "valueerror"
    ],
    "start_line": 24,
    "end_line": 48,
    "text": "def hertz_to_mel(\n    freq: Union[float, np.ndarray], mel_scale: str = \"htk\"\n) -> Union[float, np.ndarray]:\n    if mel_scale not in [\"slaney\", \"htk\", \"kaldi\"]:\n        raise ValueError('mel_scale should be one of \"htk\", \"slaney\" or \"kaldi\".')\n\n    if mel_scale == \"htk\":\n        return 2595.0 * np.log10(1.0 + (freq / 700.0))\n    elif mel_scale == \"kaldi\":\n        return 1127.0 * np.log(1.0 + (freq / 700.0))\n\n    min_log_hertz = 1000.0\n    min_log_mel = 15.0\n    logstep = 27.0 / np.log(6.4)\n    mels = 3.0 * freq / 200.0\n\n    if isinstance(freq, np.ndarray):\n        log_region = freq >= min_log_hertz\n        mels[log_region] = (\n            min_log_mel + np.log(freq[log_region] / min_log_hertz) * logstep\n        )\n    elif freq >= min_log_hertz:\n        mels = min_log_mel + np.log(freq / min_log_hertz) * logstep\n\n    return mels"
  },
  {
    "id": "a6da3c38c80e0da3e5654200bfe61244fd6da82c",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/model.py",
    "kind": "class",
    "name": "RedCodecInfer",
    "signature": "class RedCodecInfer(RedCodec)",
    "docstring": "",
    "identifiers": [
      "__init__",
      "_encode_one_batch",
      "_pad_and_chunk",
      "aco_feats",
      "aco_length",
      "acoustic_decoder",
      "acoustic_encoder",
      "append",
      "audio",
      "audio16k",
      "audio16k_batch",
      "audio16k_length",
      "audio_chunks",
      "audio_length",
      "b",
      "batch_first",
      "batch_size",
      "batch_size_list",
      "bb_conv_cache1",
      "bb_conv_cache2",
      "bb_kv_cache",
      "bool",
      "cache_dict",
      "cat",
      "ceil",
      "chunk_size",
      "ckpt",
      "ckpt_path",
      "classmethod",
      "clone",
      "cls",
      "codec",
      "conf_path",
      "decode",
      "decode_codes",
      "decode_one_token",
      "device",
      "dict",
      "dim",
      "downsample",
      "dtype",
      "encode",
      "encode_codes",
      "f",
      "forward",
      "forward_chunk",
      "from_config",
      "from_pretrained",
      "get",
      "i",
      "indices",
      "inference_mode",
      "int",
      "is_cache",
      "last_token",
      "len",
      "list",
      "load",
      "load_state_dict",
      "long",
      "math",
      "max",
      "mode",
      "new_bb_conv_cache1",
      "new_bb_conv_cache2",
      "new_bb_kv_cache",
      "new_cache_dict",
      "new_is_cache",
      "new_up_conv_cache",
      "one_audio_batch",
      "one_audio_chunks",
      "one_token_batch",
      "open",
      "pad",
      "pad_len",
      "pad_sequence",
      "padding_value",
      "permute",
      "range",
      "redcodec",
      "redcodecinfer",
      "rvq",
      "self",
      "sem_feats",
      "sem_length",
      "semantic_decoder",
      "shape",
      "split",
      "squeeze",
      "ssl",
      "ssl_adaptor",
      "ssl_length",
      "staticmethod",
      "str",
      "super",
      "t",
      "tensor",
      "token",
      "token_batch",
      "token_length",
      "token_list",
      "token_ts",
      "tokens",
      "torch",
      "transpose",
      "ts",
      "up_conv_cache",
      "upsample",
      "value",
      "vq_in_feats",
      "vq_in_length",
      "vq_out_feats",
      "vq_out_length"
    ],
    "start_line": 197,
    "end_line": 376,
    "text": "class RedCodecInfer(RedCodec):\n    def __init__(self, codec: RedCodec):\n        super().__init__(\n            codec.ssl,\n            codec.ssl_adaptor,\n            codec.acoustic_encoder,\n            codec.downsample,\n            codec.rvq,\n            codec.upsample,\n            codec.semantic_decoder,\n            codec.acoustic_decoder,\n        )\n\n    @classmethod\n    def from_pretrained(cls, conf_path: str, ckpt_path: str) -> \"RedCodecInfer\":\n        with open(conf_path, \"r\") as f:\n            codec = RedCodec.from_config(conf_path)\n        ckpt = torch.load(ckpt_path)[\"generator\"]\n        codec.load_state_dict(ckpt)\n        return cls(codec)\n\n    def _encode_one_batch(self, audio16k: torch.Tensor):\n        B, T = audio16k.shape\n        audio16k_length = torch.tensor(\n            [T] * B, dtype=torch.long, device=audio16k.device\n        )\n        # Semantic\n        ssl, ssl_length = self.ssl.forward(audio16k, audio16k_length)\n        ssl = ssl.clone()  # For onnx export\n        sem_feats, sem_length = self.ssl_adaptor(ssl, ssl_length)\n        # Acoustic\n        aco_feats, aco_length = self.acoustic_encoder(audio16k, audio16k_length)\n        # VQ\n        vq_in_feats = torch.cat([sem_feats, aco_feats], dim=2)\n        vq_in_feats, vq_in_length = self.downsample(vq_in_feats, aco_length)\n        # RVQ,\n        indices = self.rvq.encode_codes(vq_in_feats.transpose(1, 2))  # (nq, B, L)\n        indices = indices.permute(1, 0, 2)\n        return indices  # (B, nq, L)\n\n    @staticmethod\n    def _pad_and_chunk(audio: torch.Tensor, chunk_size: int) -> List[torch.Tensor]:\n        pad_len = math.ceil(audio.shape[1] / chunk_size) * chunk_size - audio.shape[1]\n        audio = F.pad(audio, (0, pad_len), mode=\"constant\", value=0)\n        audio_chunks = audio.split(chunk_size, dim=1)\n        return audio_chunks\n\n    @torch.inference_mode()\n    def encode(\n        self,\n        audio16k: torch.Tensor,\n        audio16k_length: torch.Tensor = None,\n        batch_size: int = 96,\n    ):\n        \"\"\"\n        Args:\n            audio16k: shape (b, t)\n            audio16k_length: (b,)\n        Returns:\n            token: shape (b, nq, l)\n            token_length: (b,)\n        \"\"\"\n        if audio16k_length is None:\n            assert audio16k.shape[0] == 1\n            audio16k_length = torch.tensor(\n                [audio16k.shape[1]], dtype=torch.long, device=audio16k.device\n            )\n\n        CHUNK_SIZE = 6 * 16000\n        B, T = audio16k.shape\n        # Pad, chunk, and batch\n        audio16k_batch = []\n        batch_size_list = []\n        for i in range(B):\n            # Remove extra paddings\n            one_audio_chunks = self._pad_and_chunk(\n                audio16k[i : (i + 1), : audio16k_length[i]], CHUNK_SIZE\n            )\n            audio16k_batch += one_audio_chunks\n            batch_size_list.append(len(one_audio_chunks))\n        audio16k_batch = torch.cat(audio16k_batch, dim=0)\n        # Batch encode\n        token_batch = []\n        for i in range(0, audio16k_batch.shape[0], batch_size):\n            one_audio_batch = audio16k_batch[i : (i + batch_size)]\n            one_token_batch = self._encode_one_batch(one_audio_batch)\n            token_batch.append(one_token_batch)\n        token_batch = torch.cat(token_batch, dim=0)\n        # Recover & concat\n        token_list = torch.split(\n            token_batch, batch_size_list, dim=0\n        )  # [(B=1, nq, l), (B=3, nq, l), ...]\n        token_list = [\n            torch.cat(token_ts.split(1, dim=0), dim=-1)  # (B=1, nq, l)\n            for token_ts in token_list\n        ]\n        # Pad tokens\n        token = pad_sequence(\n            [ts.squeeze(0).transpose(1, 0) for ts in token_list],\n            batch_first=True,\n            padding_value=0,\n        ).transpose(\n            1, 2\n        )  # (B, nq, L)\n        token_length = (audio16k_length / 1280).ceil().long()\n        token = token[\n            ..., : token_length.max()\n        ]  # Remove extra paddings (we pad to multiples of 6s)\n       "
  },
  {
    "id": "30523076615829dd1922b539fa2a3ea13a40d6d3",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/model.py",
    "kind": "function",
    "name": "decode_one_token",
    "signature": "decode_one_token(\n        self, token: torch.Tensor, cache_dict: Dict[str, torch.Tensor], last_token: bool\n    )",
    "docstring": "Decode one single token to audio.\n\n        Args:\n            token: (B=1, nq, L=1)\n        Returns:\n            audio:  (B=1, t)",
    "identifiers": [
      "acoustic_decoder",
      "audio",
      "bb_conv_cache1",
      "bb_conv_cache2",
      "bb_kv_cache",
      "bool",
      "cache_dict",
      "decode_codes",
      "decode_one_token",
      "device",
      "dict",
      "dtype",
      "forward_chunk",
      "get",
      "is_cache",
      "last_token",
      "long",
      "new_bb_conv_cache1",
      "new_bb_conv_cache2",
      "new_bb_kv_cache",
      "new_cache_dict",
      "new_is_cache",
      "new_up_conv_cache",
      "permute",
      "rvq",
      "self",
      "shape",
      "str",
      "tensor",
      "token",
      "torch",
      "transpose",
      "up_conv_cache",
      "upsample",
      "vq_out_feats",
      "vq_out_length"
    ],
    "start_line": 327,
    "end_line": 376,
    "text": "def decode_one_token(\n        self, token: torch.Tensor, cache_dict: Dict[str, torch.Tensor], last_token: bool\n    ):\n        \"\"\"Decode one single token to audio.\n\n        Args:\n            token: (B=1, nq, L=1)\n        Returns:\n            audio:  (B=1, t)\n        \"\"\"\n        # token->latent->upsample, (naturally causal)\n        token = token.permute(1, 0, 2)  # (B, nq, L) -> (nq, B, L)\n        vq_out_feats = self.rvq.decode_codes(token)\n        vq_out_feats = vq_out_feats.transpose(1, 2)\n        vq_out_length = torch.tensor(\n            [vq_out_feats.shape[1]], dtype=torch.long, device=vq_out_feats.device\n        )\n        vq_out_feats, vq_out_length = self.upsample(vq_out_feats, vq_out_length)\n        # acoustic decoder\n        up_conv_cache = cache_dict.get(\"up_conv_cache\", None)\n        bb_conv_cache1 = cache_dict.get(\"bb_conv_cache1\", None)\n        bb_conv_cache2 = cache_dict.get(\"bb_conv_cache2\", None)\n        bb_kv_cache = cache_dict.get(\"bb_kv_cache\", None)\n        is_cache = cache_dict.get(\"is_cache\", None)\n\n        (\n            audio,\n            new_up_conv_cache,\n            new_bb_conv_cache1,\n            new_bb_conv_cache2,\n            new_bb_kv_cache,\n            new_is_cache,\n        ) = self.acoustic_decoder.forward_chunk(\n            vq_out_feats,\n            up_conv_cache,\n            bb_conv_cache1,\n            bb_conv_cache2,\n            bb_kv_cache,\n            is_cache,\n            last_token,\n        )\n\n        new_cache_dict = {\n            \"up_conv_cache\": new_up_conv_cache,\n            \"bb_conv_cache1\": new_bb_conv_cache1,\n            \"bb_conv_cache2\": new_bb_conv_cache2,\n            \"bb_kv_cache\": new_bb_kv_cache,\n            \"is_cache\": new_is_cache,\n        }\n        return audio, new_cache_dict"
  },
  {
    "id": "8ef8dd75ee4cfa0d87c5aaa31a2d46695829084e",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/model.py",
    "kind": "function",
    "name": "decode",
    "signature": "decode(self, tokens: torch.Tensor)",
    "docstring": "Args:\n            tokens: (B=1, nq, L)\n        Returns:\n            audio: (B=1, t)",
    "identifiers": [
      "acoustic_decoder",
      "audio",
      "audio_length",
      "decode",
      "decode_codes",
      "device",
      "dtype",
      "long",
      "permute",
      "rvq",
      "self",
      "shape",
      "tensor",
      "tokens",
      "torch",
      "transpose",
      "upsample",
      "vq_out_feats",
      "vq_out_length"
    ],
    "start_line": 308,
    "end_line": 324,
    "text": "def decode(self, tokens: torch.Tensor):\n        \"\"\"\n        Args:\n            tokens: (B=1, nq, L)\n        Returns:\n            audio: (B=1, t)\n        \"\"\"\n        tokens = tokens.permute(1, 0, 2)  # (B, nq, L) -> (nq, B, L)\n        vq_out_feats = self.rvq.decode_codes(tokens)\n        vq_out_feats = vq_out_feats.transpose(1, 2)\n        vq_out_length = torch.tensor(\n            [vq_out_feats.shape[1]], dtype=torch.long, device=vq_out_feats.device\n        )\n        vq_out_feats, vq_out_length = self.upsample(vq_out_feats, vq_out_length)\n        # audio: (b, t)\n        audio, audio_length = self.acoustic_decoder(vq_out_feats, vq_out_length)\n        return audio"
  },
  {
    "id": "810d6114c928a8ad7f55be73838564267e021778",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/model.py",
    "kind": "function",
    "name": "encode",
    "signature": "encode(\n        self,\n        audio16k: torch.Tensor,\n        audio16k_length: torch.Tensor = None,\n        batch_size: int = 96,\n    )",
    "docstring": "Args:\n            audio16k: shape (b, t)\n            audio16k_length: (b,)\n        Returns:\n            token: shape (b, nq, l)\n            token_length: (b,)",
    "identifiers": [
      "_encode_one_batch",
      "_pad_and_chunk",
      "append",
      "audio16k",
      "audio16k_batch",
      "audio16k_length",
      "b",
      "batch_first",
      "batch_size",
      "batch_size_list",
      "cat",
      "ceil",
      "chunk_size",
      "device",
      "dim",
      "dtype",
      "encode",
      "i",
      "int",
      "len",
      "long",
      "max",
      "one_audio_batch",
      "one_audio_chunks",
      "one_token_batch",
      "pad_sequence",
      "padding_value",
      "range",
      "self",
      "shape",
      "split",
      "squeeze",
      "t",
      "tensor",
      "token",
      "token_batch",
      "token_length",
      "token_list",
      "token_ts",
      "torch",
      "transpose",
      "ts"
    ],
    "start_line": 245,
    "end_line": 305,
    "text": "def encode(\n        self,\n        audio16k: torch.Tensor,\n        audio16k_length: torch.Tensor = None,\n        batch_size: int = 96,\n    ):\n        \"\"\"\n        Args:\n            audio16k: shape (b, t)\n            audio16k_length: (b,)\n        Returns:\n            token: shape (b, nq, l)\n            token_length: (b,)\n        \"\"\"\n        if audio16k_length is None:\n            assert audio16k.shape[0] == 1\n            audio16k_length = torch.tensor(\n                [audio16k.shape[1]], dtype=torch.long, device=audio16k.device\n            )\n\n        CHUNK_SIZE = 6 * 16000\n        B, T = audio16k.shape\n        # Pad, chunk, and batch\n        audio16k_batch = []\n        batch_size_list = []\n        for i in range(B):\n            # Remove extra paddings\n            one_audio_chunks = self._pad_and_chunk(\n                audio16k[i : (i + 1), : audio16k_length[i]], CHUNK_SIZE\n            )\n            audio16k_batch += one_audio_chunks\n            batch_size_list.append(len(one_audio_chunks))\n        audio16k_batch = torch.cat(audio16k_batch, dim=0)\n        # Batch encode\n        token_batch = []\n        for i in range(0, audio16k_batch.shape[0], batch_size):\n            one_audio_batch = audio16k_batch[i : (i + batch_size)]\n            one_token_batch = self._encode_one_batch(one_audio_batch)\n            token_batch.append(one_token_batch)\n        token_batch = torch.cat(token_batch, dim=0)\n        # Recover & concat\n        token_list = torch.split(\n            token_batch, batch_size_list, dim=0\n        )  # [(B=1, nq, l), (B=3, nq, l), ...]\n        token_list = [\n            torch.cat(token_ts.split(1, dim=0), dim=-1)  # (B=1, nq, l)\n            for token_ts in token_list\n        ]\n        # Pad tokens\n        token = pad_sequence(\n            [ts.squeeze(0).transpose(1, 0) for ts in token_list],\n            batch_first=True,\n            padding_value=0,\n        ).transpose(\n            1, 2\n        )  # (B, nq, L)\n        token_length = (audio16k_length / 1280).ceil().long()\n        token = token[\n            ..., : token_length.max()\n        ]  # Remove extra paddings (we pad to multiples of 6s)\n        return token, token_length"
  },
  {
    "id": "2d7a40f430bc777b199daa4c0c4a512db8d803a0",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/model.py",
    "kind": "function",
    "name": "_pad_and_chunk",
    "signature": "_pad_and_chunk(audio: torch.Tensor, chunk_size: int)",
    "docstring": "",
    "identifiers": [
      "_pad_and_chunk",
      "audio",
      "audio_chunks",
      "ceil",
      "chunk_size",
      "dim",
      "f",
      "int",
      "list",
      "math",
      "mode",
      "pad",
      "pad_len",
      "shape",
      "split",
      "tensor",
      "torch",
      "value"
    ],
    "start_line": 238,
    "end_line": 242,
    "text": "def _pad_and_chunk(audio: torch.Tensor, chunk_size: int) -> List[torch.Tensor]:\n        pad_len = math.ceil(audio.shape[1] / chunk_size) * chunk_size - audio.shape[1]\n        audio = F.pad(audio, (0, pad_len), mode=\"constant\", value=0)\n        audio_chunks = audio.split(chunk_size, dim=1)\n        return audio_chunks"
  },
  {
    "id": "a376d26d9814d1c877a9e941bb6342258e7c51f4",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/model.py",
    "kind": "function",
    "name": "_encode_one_batch",
    "signature": "_encode_one_batch(self, audio16k: torch.Tensor)",
    "docstring": "",
    "identifiers": [
      "_encode_one_batch",
      "aco_feats",
      "aco_length",
      "acoustic_encoder",
      "audio16k",
      "audio16k_length",
      "b",
      "cat",
      "clone",
      "device",
      "dim",
      "downsample",
      "dtype",
      "encode_codes",
      "forward",
      "indices",
      "long",
      "permute",
      "rvq",
      "self",
      "sem_feats",
      "sem_length",
      "shape",
      "ssl",
      "ssl_adaptor",
      "ssl_length",
      "t",
      "tensor",
      "torch",
      "transpose",
      "vq_in_feats",
      "vq_in_length"
    ],
    "start_line": 218,
    "end_line": 235,
    "text": "def _encode_one_batch(self, audio16k: torch.Tensor):\n        B, T = audio16k.shape\n        audio16k_length = torch.tensor(\n            [T] * B, dtype=torch.long, device=audio16k.device\n        )\n        # Semantic\n        ssl, ssl_length = self.ssl.forward(audio16k, audio16k_length)\n        ssl = ssl.clone()  # For onnx export\n        sem_feats, sem_length = self.ssl_adaptor(ssl, ssl_length)\n        # Acoustic\n        aco_feats, aco_length = self.acoustic_encoder(audio16k, audio16k_length)\n        # VQ\n        vq_in_feats = torch.cat([sem_feats, aco_feats], dim=2)\n        vq_in_feats, vq_in_length = self.downsample(vq_in_feats, aco_length)\n        # RVQ,\n        indices = self.rvq.encode_codes(vq_in_feats.transpose(1, 2))  # (nq, B, L)\n        indices = indices.permute(1, 0, 2)\n        return indices  # (B, nq, L)"
  },
  {
    "id": "c572dfb5dfb26572f40646e3f8d55d33dd1efe7a",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/model.py",
    "kind": "function",
    "name": "from_pretrained",
    "signature": "from_pretrained(cls, conf_path: str, ckpt_path: str)",
    "docstring": "",
    "identifiers": [
      "ckpt",
      "ckpt_path",
      "cls",
      "codec",
      "conf_path",
      "f",
      "from_config",
      "from_pretrained",
      "load",
      "load_state_dict",
      "open",
      "redcodec",
      "str",
      "torch"
    ],
    "start_line": 211,
    "end_line": 216,
    "text": "def from_pretrained(cls, conf_path: str, ckpt_path: str) -> \"RedCodecInfer\":\n        with open(conf_path, \"r\") as f:\n            codec = RedCodec.from_config(conf_path)\n        ckpt = torch.load(ckpt_path)[\"generator\"]\n        codec.load_state_dict(ckpt)\n        return cls(codec)"
  },
  {
    "id": "3c3ffb9ed282e39eaa9cbc1826d7869670fe61cf",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/model.py",
    "kind": "function",
    "name": "__init__",
    "signature": "__init__(self, codec: RedCodec)",
    "docstring": "",
    "identifiers": [
      "__init__",
      "acoustic_decoder",
      "acoustic_encoder",
      "codec",
      "downsample",
      "redcodec",
      "rvq",
      "self",
      "semantic_decoder",
      "ssl",
      "ssl_adaptor",
      "super",
      "upsample"
    ],
    "start_line": 198,
    "end_line": 208,
    "text": "def __init__(self, codec: RedCodec):\n        super().__init__(\n            codec.ssl,\n            codec.ssl_adaptor,\n            codec.acoustic_encoder,\n            codec.downsample,\n            codec.rvq,\n            codec.upsample,\n            codec.semantic_decoder,\n            codec.acoustic_decoder,\n        )"
  },
  {
    "id": "b10d37679c0b1e9891a7a67344c33e46cbbc3595",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/model.py",
    "kind": "class",
    "name": "RedCodec",
    "signature": "class RedCodec(nn.Module)",
    "docstring": "",
    "identifiers": [
      "__init__",
      "acoustic_decoder",
      "acoustic_encoder",
      "acousticdecoder",
      "classmethod",
      "cls",
      "config",
      "config_json",
      "downsample",
      "f",
      "from_config",
      "from_pretrained",
      "json",
      "load",
      "module",
      "nn",
      "open",
      "pretrainedwhisperencoder",
      "redcodec",
      "residualdownconv",
      "residualvq",
      "rvq",
      "self",
      "semantic_decoder",
      "ssl",
      "ssl_adaptor",
      "ssladaptor",
      "str",
      "super",
      "upconv",
      "upsample",
      "whisperacousticencoder"
    ],
    "start_line": 151,
    "end_line": 194,
    "text": "class RedCodec(nn.Module):\n    def __init__(\n        self,\n        ssl: PretrainedWhisperEncoder,\n        ssl_adaptor: SslAdaptor,\n        acoustic_encoder: WhisperAcousticEncoder,\n        downsample: ResidualDownConv,\n        rvq: ResidualVQ,\n        upsample: UpConv,\n        semantic_decoder: SslAdaptor,\n        acoustic_decoder: AcousticDecoder,\n    ):\n        super().__init__()\n        self.ssl = ssl\n        self.ssl_adaptor = ssl_adaptor\n        self.acoustic_encoder = acoustic_encoder\n        self.downsample = downsample\n        self.rvq = rvq\n        self.upsample = upsample\n        self.semantic_decoder = semantic_decoder\n        self.acoustic_decoder = acoustic_decoder\n\n    @classmethod\n    def from_config(cls, config_json: str) -> \"RedCodec\":\n        with open(config_json, \"rb\") as f:\n            config = json.load(f)[\"codec\"]\n        ssl = PretrainedWhisperEncoder.from_pretrained()\n        ssl_adaptor = SslAdaptor(**config[\"ssl_adaptor\"])\n        acoustic_encoder = WhisperAcousticEncoder(**config[\"acoustic_encoder\"])\n        downsample = ResidualDownConv(**config[\"downsample\"])\n        rvq = ResidualVQ(**config[\"rvq\"])\n        upsample = UpConv(**config[\"upsample\"])\n        semantic_decoder = SslAdaptor(**config[\"semantic_decoder\"])\n        acoustic_decoder = AcousticDecoder(**config[\"acoustic_decoder\"])\n        return cls(\n            ssl,\n            ssl_adaptor,\n            acoustic_encoder,\n            downsample,\n            rvq,\n            upsample,\n            semantic_decoder,\n            acoustic_decoder,\n        )"
  },
  {
    "id": "332da21dcb72a699afc8c39e3ffb98350787a844",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/model.py",
    "kind": "function",
    "name": "from_config",
    "signature": "from_config(cls, config_json: str)",
    "docstring": "",
    "identifiers": [
      "acoustic_decoder",
      "acoustic_encoder",
      "acousticdecoder",
      "cls",
      "config",
      "config_json",
      "downsample",
      "f",
      "from_config",
      "from_pretrained",
      "json",
      "load",
      "open",
      "pretrainedwhisperencoder",
      "residualdownconv",
      "residualvq",
      "rvq",
      "semantic_decoder",
      "ssl",
      "ssl_adaptor",
      "ssladaptor",
      "str",
      "upconv",
      "upsample",
      "whisperacousticencoder"
    ],
    "start_line": 174,
    "end_line": 194,
    "text": "def from_config(cls, config_json: str) -> \"RedCodec\":\n        with open(config_json, \"rb\") as f:\n            config = json.load(f)[\"codec\"]\n        ssl = PretrainedWhisperEncoder.from_pretrained()\n        ssl_adaptor = SslAdaptor(**config[\"ssl_adaptor\"])\n        acoustic_encoder = WhisperAcousticEncoder(**config[\"acoustic_encoder\"])\n        downsample = ResidualDownConv(**config[\"downsample\"])\n        rvq = ResidualVQ(**config[\"rvq\"])\n        upsample = UpConv(**config[\"upsample\"])\n        semantic_decoder = SslAdaptor(**config[\"semantic_decoder\"])\n        acoustic_decoder = AcousticDecoder(**config[\"acoustic_decoder\"])\n        return cls(\n            ssl,\n            ssl_adaptor,\n            acoustic_encoder,\n            downsample,\n            rvq,\n            upsample,\n            semantic_decoder,\n            acoustic_decoder,\n        )"
  },
  {
    "id": "58a99c06142877b644e956a4a7eda63f03501b75",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/model.py",
    "kind": "function",
    "name": "__init__",
    "signature": "__init__(\n        self,\n        ssl: PretrainedWhisperEncoder,\n        ssl_adaptor: SslAdaptor,\n        acoustic_encoder: WhisperAcousticEncoder,\n        downsample: ResidualDownConv,\n        rvq: ResidualVQ,\n        upsample: UpConv,\n        semantic_decoder: SslAdaptor,\n        acoustic_decoder: AcousticDecoder,\n    )",
    "docstring": "",
    "identifiers": [
      "__init__",
      "acoustic_decoder",
      "acoustic_encoder",
      "acousticdecoder",
      "downsample",
      "pretrainedwhisperencoder",
      "residualdownconv",
      "residualvq",
      "rvq",
      "self",
      "semantic_decoder",
      "ssl",
      "ssl_adaptor",
      "ssladaptor",
      "super",
      "upconv",
      "upsample",
      "whisperacousticencoder"
    ],
    "start_line": 152,
    "end_line": 171,
    "text": "def __init__(\n        self,\n        ssl: PretrainedWhisperEncoder,\n        ssl_adaptor: SslAdaptor,\n        acoustic_encoder: WhisperAcousticEncoder,\n        downsample: ResidualDownConv,\n        rvq: ResidualVQ,\n        upsample: UpConv,\n        semantic_decoder: SslAdaptor,\n        acoustic_decoder: AcousticDecoder,\n    ):\n        super().__init__()\n        self.ssl = ssl\n        self.ssl_adaptor = ssl_adaptor\n        self.acoustic_encoder = acoustic_encoder\n        self.downsample = downsample\n        self.rvq = rvq\n        self.upsample = upsample\n        self.semantic_decoder = semantic_decoder\n        self.acoustic_decoder = acoustic_decoder"
  },
  {
    "id": "755e88d631db4b280b256454a751f7739883ffd8",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/model.py",
    "kind": "class",
    "name": "UpConv",
    "signature": "class UpConv(nn.Module)",
    "docstring": "",
    "identifiers": [
      "__init__",
      "bias",
      "convtranspose1d",
      "embed_dim",
      "forward",
      "in_proj",
      "input_length",
      "int",
      "kernel_size",
      "linear",
      "module",
      "nn",
      "output_length",
      "res",
      "self",
      "stride",
      "super",
      "tensor",
      "torch",
      "transpose",
      "up_conv",
      "upconv",
      "x"
    ],
    "start_line": 123,
    "end_line": 148,
    "text": "class UpConv(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int = 768,\n        stride: int = 4,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.stride = stride\n        self.in_proj = nn.Linear(embed_dim, self.stride * embed_dim)\n        # Simple transpose convolution layer to keep channel number consistent\n        self.up_conv = nn.ConvTranspose1d(\n            self.stride * embed_dim,\n            embed_dim,\n            kernel_size=stride,\n            stride=stride,\n            bias=False,\n        )\n\n    def forward(self, x: torch.Tensor, input_length: torch.Tensor):\n        x = self.in_proj(x)\n        x = x.transpose(1, 2)\n        res = self.up_conv(x)\n        res = res.transpose(1, 2)\n        output_length = input_length * self.stride\n        return res, output_length"
  },
  {
    "id": "19b64166b979b4826b6879317c0e05504dde194b",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/model.py",
    "kind": "function",
    "name": "forward",
    "signature": "forward(self, x: torch.Tensor, input_length: torch.Tensor)",
    "docstring": "",
    "identifiers": [
      "forward",
      "in_proj",
      "input_length",
      "output_length",
      "res",
      "self",
      "stride",
      "tensor",
      "torch",
      "transpose",
      "up_conv",
      "x"
    ],
    "start_line": 142,
    "end_line": 148,
    "text": "def forward(self, x: torch.Tensor, input_length: torch.Tensor):\n        x = self.in_proj(x)\n        x = x.transpose(1, 2)\n        res = self.up_conv(x)\n        res = res.transpose(1, 2)\n        output_length = input_length * self.stride\n        return res, output_length"
  },
  {
    "id": "06c6ef8d911d787fbd70edc765c9aee8d129b992",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/model.py",
    "kind": "function",
    "name": "__init__",
    "signature": "__init__(\n        self,\n        embed_dim: int = 768,\n        stride: int = 4,\n    )",
    "docstring": "",
    "identifiers": [
      "__init__",
      "bias",
      "convtranspose1d",
      "embed_dim",
      "in_proj",
      "int",
      "kernel_size",
      "linear",
      "nn",
      "self",
      "stride",
      "super",
      "up_conv"
    ],
    "start_line": 124,
    "end_line": 140,
    "text": "def __init__(\n        self,\n        embed_dim: int = 768,\n        stride: int = 4,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.stride = stride\n        self.in_proj = nn.Linear(embed_dim, self.stride * embed_dim)\n        # Simple transpose convolution layer to keep channel number consistent\n        self.up_conv = nn.ConvTranspose1d(\n            self.stride * embed_dim,\n            embed_dim,\n            kernel_size=stride,\n            stride=stride,\n            bias=False,\n        )"
  },
  {
    "id": "91b4785dbdcc2383161d7cc63ae899917a12950d",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/model.py",
    "kind": "class",
    "name": "ResidualDownConv",
    "signature": "class ResidualDownConv(nn.Module)",
    "docstring": "",
    "identifiers": [
      "_",
      "__init__",
      "act_fn",
      "avg_pooler",
      "batch_size",
      "bias",
      "c",
      "conv1d",
      "down_proj",
      "embed_dim",
      "forward",
      "g",
      "gate_proj",
      "input_length",
      "int",
      "intermediate_dim",
      "layer_norm",
      "layernorm",
      "linear",
      "module",
      "nn",
      "out_proj",
      "output_length",
      "permute",
      "res",
      "reshape",
      "residualdownconv",
      "self",
      "seq_len",
      "shape",
      "silu",
      "super",
      "tensor",
      "torch",
      "u",
      "up_proj",
      "x",
      "xt"
    ],
    "start_line": 80,
    "end_line": 120,
    "text": "class ResidualDownConv(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int = 768,\n        avg_pooler=4,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.avg_pooler = avg_pooler\n        self.intermediate_dim = embed_dim * avg_pooler\n        # Convolution layer for downsampling\n        self.gate_proj = nn.Conv1d(\n            embed_dim, self.intermediate_dim, avg_pooler, avg_pooler, bias=False\n        )\n        self.up_proj = nn.Conv1d(\n            embed_dim, self.intermediate_dim, avg_pooler, avg_pooler, bias=False\n        )\n        # Downsampled linear projection\n        self.down_proj = nn.Linear(\n            self.intermediate_dim, self.intermediate_dim, bias=False\n        )\n        # Activation function and layer normalization\n        self.act_fn = nn.SiLU()\n        self.layer_norm = nn.LayerNorm(self.intermediate_dim)\n        # Final output projection\n        self.out_proj = nn.Linear(self.intermediate_dim, embed_dim)\n\n    def forward(self, x: torch.Tensor, input_length: torch.Tensor):\n        output_length = input_length // self.avg_pooler\n        batch_size, seq_len, _ = x.shape  # (B, T, D)\n\n        xt = x.permute(0, 2, 1)  # (B, D, T)\n        g = self.gate_proj(xt).permute(0, 2, 1)  # (B, T//4, D*4)\n        u = self.up_proj(xt).permute(0, 2, 1)  # (B, T//4, D*4)\n        x = x.reshape(batch_size, -1, self.intermediate_dim)  # (B, T//4, D*4)\n\n        c = self.down_proj(self.act_fn(g) * u)  # (B, T//4, D*4)\n        res = self.layer_norm(c + x)  # (B, T//4, D*4)\n\n        res = self.out_proj(res)\n        return res, output_length"
  },
  {
    "id": "b4b4d65649615471309c87731f5fd127e167d18e",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/model.py",
    "kind": "function",
    "name": "forward",
    "signature": "forward(self, x: torch.Tensor, input_length: torch.Tensor)",
    "docstring": "",
    "identifiers": [
      "_",
      "act_fn",
      "avg_pooler",
      "batch_size",
      "c",
      "down_proj",
      "forward",
      "g",
      "gate_proj",
      "input_length",
      "intermediate_dim",
      "layer_norm",
      "out_proj",
      "output_length",
      "permute",
      "res",
      "reshape",
      "self",
      "seq_len",
      "shape",
      "tensor",
      "torch",
      "u",
      "up_proj",
      "x",
      "xt"
    ],
    "start_line": 107,
    "end_line": 120,
    "text": "def forward(self, x: torch.Tensor, input_length: torch.Tensor):\n        output_length = input_length // self.avg_pooler\n        batch_size, seq_len, _ = x.shape  # (B, T, D)\n\n        xt = x.permute(0, 2, 1)  # (B, D, T)\n        g = self.gate_proj(xt).permute(0, 2, 1)  # (B, T//4, D*4)\n        u = self.up_proj(xt).permute(0, 2, 1)  # (B, T//4, D*4)\n        x = x.reshape(batch_size, -1, self.intermediate_dim)  # (B, T//4, D*4)\n\n        c = self.down_proj(self.act_fn(g) * u)  # (B, T//4, D*4)\n        res = self.layer_norm(c + x)  # (B, T//4, D*4)\n\n        res = self.out_proj(res)\n        return res, output_length"
  },
  {
    "id": "7dd0339b64f3e21af5289d264de7447c019d69ac",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/model.py",
    "kind": "function",
    "name": "__init__",
    "signature": "__init__(\n        self,\n        embed_dim: int = 768,\n        avg_pooler=4,\n    )",
    "docstring": "",
    "identifiers": [
      "__init__",
      "act_fn",
      "avg_pooler",
      "bias",
      "conv1d",
      "down_proj",
      "embed_dim",
      "gate_proj",
      "int",
      "intermediate_dim",
      "layer_norm",
      "layernorm",
      "linear",
      "nn",
      "out_proj",
      "self",
      "silu",
      "super",
      "up_proj"
    ],
    "start_line": 81,
    "end_line": 105,
    "text": "def __init__(\n        self,\n        embed_dim: int = 768,\n        avg_pooler=4,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.avg_pooler = avg_pooler\n        self.intermediate_dim = embed_dim * avg_pooler\n        # Convolution layer for downsampling\n        self.gate_proj = nn.Conv1d(\n            embed_dim, self.intermediate_dim, avg_pooler, avg_pooler, bias=False\n        )\n        self.up_proj = nn.Conv1d(\n            embed_dim, self.intermediate_dim, avg_pooler, avg_pooler, bias=False\n        )\n        # Downsampled linear projection\n        self.down_proj = nn.Linear(\n            self.intermediate_dim, self.intermediate_dim, bias=False\n        )\n        # Activation function and layer normalization\n        self.act_fn = nn.SiLU()\n        self.layer_norm = nn.LayerNorm(self.intermediate_dim)\n        # Final output projection\n        self.out_proj = nn.Linear(self.intermediate_dim, embed_dim)"
  },
  {
    "id": "38f9196fced3a4951948b285fe103df8c8a6bb15",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/model.py",
    "kind": "class",
    "name": "SslAdaptor",
    "signature": "class SslAdaptor(nn.Module)",
    "docstring": "",
    "identifiers": [
      "_",
      "__init__",
      "_init_weights",
      "apply",
      "attention_mask",
      "attn_dropout",
      "bias",
      "conv1d",
      "data",
      "dropout",
      "embed_dim",
      "embedding",
      "ffn_dim",
      "float",
      "forward",
      "hidden_length",
      "hidden_states",
      "in_dim",
      "in_proj",
      "int",
      "isinstance",
      "layer",
      "layer_norm",
      "layernorm",
      "layers",
      "linear",
      "make_nonpad_mask",
      "mean",
      "module",
      "modulelist",
      "nn",
      "normal_",
      "num_heads",
      "num_layers",
      "out_dim",
      "out_proj",
      "padding_idx",
      "range",
      "self",
      "ssladaptor",
      "std",
      "super",
      "tensor",
      "torch",
      "unsqueeze",
      "weight",
      "whisperencoderlayer",
      "zero_"
    ],
    "start_line": 19,
    "end_line": 77,
    "text": "class SslAdaptor(nn.Module):\n    def __init__(\n        self,\n        in_dim: int,\n        embed_dim: int,\n        out_dim: int,\n        num_layers: int,\n        num_heads: int,\n        ffn_dim: int = None,\n        attn_dropout: float = 0.0,\n        dropout: float = 0.0,\n    ):\n        super().__init__()\n        self.in_dim = in_dim\n        self.embed_dim = embed_dim\n        self.dropout = dropout\n        # Input Projection\n        self.in_proj = nn.Linear(in_dim, embed_dim)\n        # Transformer\n        self.layers = nn.ModuleList(\n            [\n                WhisperEncoderLayer(\n                    embed_dim, num_heads, ffn_dim, attn_dropout, dropout\n                )\n                for _ in range(num_layers)\n            ]\n        )\n        # Output norm\n        self.layer_norm = nn.LayerNorm(embed_dim)\n        # Output projection\n        self.out_proj = nn.Linear(embed_dim, out_dim)\n        # Init weight\n        self.apply(self._init_weights)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        hidden_length: torch.Tensor,\n    ):\n        # Downsampling\n        hidden_states = self.in_proj(hidden_states)\n        # Transformer\n        attention_mask = make_nonpad_mask(hidden_length).unsqueeze(1)  # (b, 1, t)\n        for layer in self.layers:\n            hidden_states = layer(hidden_states, attention_mask)\n        hidden_states = self.layer_norm(hidden_states)\n        hidden_states = self.out_proj(hidden_states)\n        return hidden_states, hidden_length\n\n    def _init_weights(self, module):\n        std = 0.02\n        if isinstance(module, (nn.Linear, nn.Conv1d)):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()"
  },
  {
    "id": "5bd41c39128be424e82c5862644b8cccda008254",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/model.py",
    "kind": "function",
    "name": "_init_weights",
    "signature": "_init_weights(self, module)",
    "docstring": "",
    "identifiers": [
      "_init_weights",
      "bias",
      "conv1d",
      "data",
      "embedding",
      "isinstance",
      "linear",
      "mean",
      "module",
      "nn",
      "normal_",
      "padding_idx",
      "self",
      "std",
      "weight",
      "zero_"
    ],
    "start_line": 68,
    "end_line": 77,
    "text": "def _init_weights(self, module):\n        std = 0.02\n        if isinstance(module, (nn.Linear, nn.Conv1d)):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()"
  },
  {
    "id": "e720bc2d88e627268fdb2e3c79387042a95a089a",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/model.py",
    "kind": "function",
    "name": "forward",
    "signature": "forward(\n        self,\n        hidden_states: torch.Tensor,\n        hidden_length: torch.Tensor,\n    )",
    "docstring": "",
    "identifiers": [
      "attention_mask",
      "forward",
      "hidden_length",
      "hidden_states",
      "in_proj",
      "layer",
      "layer_norm",
      "layers",
      "make_nonpad_mask",
      "out_proj",
      "self",
      "tensor",
      "torch",
      "unsqueeze"
    ],
    "start_line": 53,
    "end_line": 66,
    "text": "def forward(\n        self,\n        hidden_states: torch.Tensor,\n        hidden_length: torch.Tensor,\n    ):\n        # Downsampling\n        hidden_states = self.in_proj(hidden_states)\n        # Transformer\n        attention_mask = make_nonpad_mask(hidden_length).unsqueeze(1)  # (b, 1, t)\n        for layer in self.layers:\n            hidden_states = layer(hidden_states, attention_mask)\n        hidden_states = self.layer_norm(hidden_states)\n        hidden_states = self.out_proj(hidden_states)\n        return hidden_states, hidden_length"
  },
  {
    "id": "cb815d8d66eb5e239e3705b5aa1d92815a53eec3",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/model.py",
    "kind": "function",
    "name": "__init__",
    "signature": "__init__(\n        self,\n        in_dim: int,\n        embed_dim: int,\n        out_dim: int,\n        num_layers: int,\n        num_heads: int,\n        ffn_dim: int = None,\n        attn_dropout: float = 0.0,\n        dropout: float = 0.0,\n    )",
    "docstring": "",
    "identifiers": [
      "_",
      "__init__",
      "_init_weights",
      "apply",
      "attn_dropout",
      "dropout",
      "embed_dim",
      "ffn_dim",
      "float",
      "in_dim",
      "in_proj",
      "int",
      "layer_norm",
      "layernorm",
      "layers",
      "linear",
      "modulelist",
      "nn",
      "num_heads",
      "num_layers",
      "out_dim",
      "out_proj",
      "range",
      "self",
      "super",
      "whisperencoderlayer"
    ],
    "start_line": 20,
    "end_line": 51,
    "text": "def __init__(\n        self,\n        in_dim: int,\n        embed_dim: int,\n        out_dim: int,\n        num_layers: int,\n        num_heads: int,\n        ffn_dim: int = None,\n        attn_dropout: float = 0.0,\n        dropout: float = 0.0,\n    ):\n        super().__init__()\n        self.in_dim = in_dim\n        self.embed_dim = embed_dim\n        self.dropout = dropout\n        # Input Projection\n        self.in_proj = nn.Linear(in_dim, embed_dim)\n        # Transformer\n        self.layers = nn.ModuleList(\n            [\n                WhisperEncoderLayer(\n                    embed_dim, num_heads, ffn_dim, attn_dropout, dropout\n                )\n                for _ in range(num_layers)\n            ]\n        )\n        # Output norm\n        self.layer_norm = nn.LayerNorm(embed_dim)\n        # Output projection\n        self.out_proj = nn.Linear(embed_dim, out_dim)\n        # Init weight\n        self.apply(self._init_weights)"
  },
  {
    "id": "3f314b7d3a919bb44187a870165f6802f99fdc6c",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "class",
    "name": "AcousticDecoder",
    "signature": "class AcousticDecoder(nn.Module)",
    "docstring": "UpsampleConv(50->100Hz) + VocosBackbone + ISTFTHead",
    "identifiers": [
      "__init__",
      "_init_weights",
      "acousticdecoder",
      "apply",
      "backbone",
      "bb_conv_cache1",
      "bb_conv_cache2",
      "bb_kv_cache",
      "bias",
      "bool",
      "cache",
      "cache1",
      "cache2",
      "cat",
      "causal",
      "causalvocosbackbone",
      "constant_",
      "conv1d",
      "convtranspose1d",
      "dim",
      "dropout",
      "embed_dim",
      "float",
      "forward",
      "forward_chunk",
      "forward_upsample_conv_chunk",
      "gelu",
      "hop_length",
      "init",
      "int",
      "is_cache",
      "isift",
      "isinstance",
      "istfthead",
      "kernel_size",
      "last_chunk",
      "m",
      "module",
      "new_bb_conv_cache1",
      "new_bb_conv_cache2",
      "new_bb_kv_cache",
      "new_cache",
      "new_cache1",
      "new_cache2",
      "new_is_cache",
      "new_up_conv_cache",
      "nn",
      "num_heads",
      "num_layers",
      "output_padding",
      "padding",
      "self",
      "sequential",
      "shape",
      "split",
      "std",
      "stride",
      "super",
      "target_length",
      "tensor",
      "torch",
      "transpose",
      "trunc_normal_",
      "up_conv_cache",
      "upsample_conv",
      "vocosbackbone",
      "weight",
      "x",
      "x_lens",
      "y",
      "y_lens"
    ],
    "start_line": 550,
    "end_line": 700,
    "text": "class AcousticDecoder(nn.Module):\n    def __init__(\n        self,\n        # Transformer\n        embed_dim: int,\n        num_layers: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        # iSTFT\n        hop_length: int = 240,\n        # Causal\n        causal: bool = False,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_layers = num_layers\n        self.num_heads = num_heads\n        self.hop_length = hop_length\n        self.causal = causal\n\n        # Output upsample\n        self.upsample_conv = nn.Sequential(\n            nn.ConvTranspose1d(\n                embed_dim,\n                embed_dim,\n                kernel_size=3,\n                stride=2,\n                padding=0,  # Do not fill input side\n                output_padding=0,  # Can be adjusted to precisely control length\n            ),\n            nn.GELU(),\n            nn.ConvTranspose1d(\n                embed_dim,\n                embed_dim,\n                kernel_size=3,\n                stride=1,\n                padding=0,  # Do not fill input side\n            ),\n            nn.GELU(),\n        )\n        self.backbone = (\n            CausalVocosBackbone(embed_dim, num_layers, num_heads, dropout)\n            if causal\n            else VocosBackbone(embed_dim, num_layers, num_heads, dropout)\n        )\n        self.isift = ISTFTHead(embed_dim, hop_length * 4, hop_length, padding=\"same\")\n        # Init weights\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Conv1d):\n            nn.init.trunc_normal_(m.weight, std=0.02)\n            nn.init.constant_(m.bias, 0)\n\n    def forward(self, x: torch.Tensor, x_lens: torch.Tensor):\n        \"\"\"\n        Args:\n            x: shape (b, t, c)\n            x_lens: shape (b,)\n        \"\"\"\n        # Upsample\n        target_length = x.shape[1] * 2\n        x = x.transpose(1, 2)\n        x = self.upsample_conv(x)\n        x = x.transpose(1, 2)\n        # NOTE strict upsampling, trim the last 3 elements\n        x = x[:, :target_length]\n        x_lens = x_lens * 2\n        # Backbone\n        x = self.backbone(x, x_lens)\n        # iSTFT\n        y, y_lens = self.isift(x, x_lens)\n        return y, y_lens\n\n    def forward_upsample_conv_chunk(self, x: torch.Tensor, cache: torch.Tensor = None):\n        \"\"\"Stream forward upsample_conv module with previous block cache.\n\n        Args:\n            x: shape (B, C, T)\n            cache: shape (B, C, 3), where 3 denotes 1 history state for 1st conv and 2 for the rest conv.\n        \"\"\"\n        # Unpack cache\n        cache1, cache2 = (\n            (None, None) if cache is None else torch.split(cache, [1, 2], dim=2)\n        )\n        # 1st conv cache\n        if cache1 is not None:\n            x = torch.cat([cache1, x], dim=2)\n        new_cache1 = x[..., -1:]\n        # 1st conv\n        x = self.upsample_conv[0](x)[..., :-1]  # remove extra 1 frame\n        if cache1 is not None:\n            x = x[..., 2:]  # remove cache1 part\n        x = self.upsample_conv[1](x)\n        # 2nd conv cache\n        if cache2 is not None:\n            x = torch.cat([cache2, x], dim=2)\n        new_cache2 = x[..., -2:]\n        # 2nd conv\n        x = self.upsample_conv[2](x)[..., :-2]  # remove extra 2 frame\n        if cache2 is not None:\n            x = x[..., 2:]  # remove cache2 part\n        x = self.upsample_conv[3](x)\n\n        new_cache = torch.cat([new_cache1, new_cache2], dim=2)\n        return x, new_cache\n\n    def forward_chunk(\n        self,\n        x: torch.Tensor,\n        # Upsample conv cache\n        up_conv_cache: torch.Tensor = None,\n        # Backbone conv cache\n        bb_conv_cache1: torch.Tensor = None,\n        bb_conv_cache2: torch.Tensor = None,\n        # Backbone attention cache\n        bb_kv_cache: torch.Tensor = None,\n        # iSTFT cache\n        is_cache: torch.Tensor = None,\n        last_chunk: bool = False,\n    ):\n        \"\"\"\n        Args:\n            x: input sequence at 50Hz, length should be multiples of 4\n       "
  },
  {
    "id": "bf880e9ced996bc9962f7463fcfe52528e42f3a0",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "function",
    "name": "forward_chunk",
    "signature": "forward_chunk(\n        self,\n        x: torch.Tensor,\n        # Upsample conv cache\n        up_conv_cache: torch.Tensor = None,\n        # Backbone conv cache\n        bb_conv_cache1: torch.Tensor = None,\n        bb_conv_cache2: torch.Tensor = None,\n        # Backbone attention cache\n        bb_kv_cache: torch.Tensor = None,\n        # iSTFT cache\n        is_cache: torch.Tensor = None,\n        last_chunk: bool = False,\n    )",
    "docstring": "Args:\n            x: input sequence at 50Hz, length should be multiples of 4",
    "identifiers": [
      "backbone",
      "bb_conv_cache1",
      "bb_conv_cache2",
      "bb_kv_cache",
      "bool",
      "causal",
      "forward_chunk",
      "forward_upsample_conv_chunk",
      "is_cache",
      "isift",
      "last_chunk",
      "new_bb_conv_cache1",
      "new_bb_conv_cache2",
      "new_bb_kv_cache",
      "new_is_cache",
      "new_up_conv_cache",
      "self",
      "tensor",
      "torch",
      "transpose",
      "up_conv_cache",
      "x",
      "y"
    ],
    "start_line": 657,
    "end_line": 700,
    "text": "def forward_chunk(\n        self,\n        x: torch.Tensor,\n        # Upsample conv cache\n        up_conv_cache: torch.Tensor = None,\n        # Backbone conv cache\n        bb_conv_cache1: torch.Tensor = None,\n        bb_conv_cache2: torch.Tensor = None,\n        # Backbone attention cache\n        bb_kv_cache: torch.Tensor = None,\n        # iSTFT cache\n        is_cache: torch.Tensor = None,\n        last_chunk: bool = False,\n    ):\n        \"\"\"\n        Args:\n            x: input sequence at 50Hz, length should be multiples of 4\n        \"\"\"\n        assert (\n            self.causal\n        ), \"Only AcousticDecoder with causal=True supports forward_chunk method.\"\n\n        x = x.transpose(1, 2)\n        x, new_up_conv_cache = self.forward_upsample_conv_chunk(x, up_conv_cache)\n        x = x.transpose(1, 2)\n        # Backbone\n        x, new_bb_conv_cache1, new_bb_conv_cache2, new_bb_kv_cache = (\n            self.backbone.forward_chunk(\n                x,\n                bb_conv_cache1,\n                bb_conv_cache2,\n                bb_kv_cache,\n            )\n        )\n        # iSTFT\n        y, new_is_cache = self.isift.forward_chunk(x, is_cache, last_chunk)\n        return (\n            y,\n            new_up_conv_cache,\n            new_bb_conv_cache1,\n            new_bb_conv_cache2,\n            new_bb_kv_cache,\n            new_is_cache,\n        )"
  },
  {
    "id": "b47817ead8b8cc83fdf2af1f1580fc7462953495",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "function",
    "name": "forward_upsample_conv_chunk",
    "signature": "forward_upsample_conv_chunk(self, x: torch.Tensor, cache: torch.Tensor = None)",
    "docstring": "Stream forward upsample_conv module with previous block cache.\n\n        Args:\n            x: shape (B, C, T)\n            cache: shape (B, C, 3), where 3 denotes 1 history state for 1st conv and 2 for the rest conv.",
    "identifiers": [
      "cache",
      "cache1",
      "cache2",
      "cat",
      "dim",
      "forward_upsample_conv_chunk",
      "new_cache",
      "new_cache1",
      "new_cache2",
      "self",
      "split",
      "tensor",
      "torch",
      "upsample_conv",
      "x"
    ],
    "start_line": 624,
    "end_line": 655,
    "text": "def forward_upsample_conv_chunk(self, x: torch.Tensor, cache: torch.Tensor = None):\n        \"\"\"Stream forward upsample_conv module with previous block cache.\n\n        Args:\n            x: shape (B, C, T)\n            cache: shape (B, C, 3), where 3 denotes 1 history state for 1st conv and 2 for the rest conv.\n        \"\"\"\n        # Unpack cache\n        cache1, cache2 = (\n            (None, None) if cache is None else torch.split(cache, [1, 2], dim=2)\n        )\n        # 1st conv cache\n        if cache1 is not None:\n            x = torch.cat([cache1, x], dim=2)\n        new_cache1 = x[..., -1:]\n        # 1st conv\n        x = self.upsample_conv[0](x)[..., :-1]  # remove extra 1 frame\n        if cache1 is not None:\n            x = x[..., 2:]  # remove cache1 part\n        x = self.upsample_conv[1](x)\n        # 2nd conv cache\n        if cache2 is not None:\n            x = torch.cat([cache2, x], dim=2)\n        new_cache2 = x[..., -2:]\n        # 2nd conv\n        x = self.upsample_conv[2](x)[..., :-2]  # remove extra 2 frame\n        if cache2 is not None:\n            x = x[..., 2:]  # remove cache2 part\n        x = self.upsample_conv[3](x)\n\n        new_cache = torch.cat([new_cache1, new_cache2], dim=2)\n        return x, new_cache"
  },
  {
    "id": "40a98c151e7b7a59fa897017ca92e0dc403b060b",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "function",
    "name": "forward",
    "signature": "forward(self, x: torch.Tensor, x_lens: torch.Tensor)",
    "docstring": "Args:\n            x: shape (b, t, c)\n            x_lens: shape (b,)",
    "identifiers": [
      "backbone",
      "forward",
      "isift",
      "self",
      "shape",
      "target_length",
      "tensor",
      "torch",
      "transpose",
      "upsample_conv",
      "x",
      "x_lens",
      "y",
      "y_lens"
    ],
    "start_line": 604,
    "end_line": 622,
    "text": "def forward(self, x: torch.Tensor, x_lens: torch.Tensor):\n        \"\"\"\n        Args:\n            x: shape (b, t, c)\n            x_lens: shape (b,)\n        \"\"\"\n        # Upsample\n        target_length = x.shape[1] * 2\n        x = x.transpose(1, 2)\n        x = self.upsample_conv(x)\n        x = x.transpose(1, 2)\n        # NOTE strict upsampling, trim the last 3 elements\n        x = x[:, :target_length]\n        x_lens = x_lens * 2\n        # Backbone\n        x = self.backbone(x, x_lens)\n        # iSTFT\n        y, y_lens = self.isift(x, x_lens)\n        return y, y_lens"
  },
  {
    "id": "f17df20d852b9bf33285af0d46bd67a2034a5e3a",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "function",
    "name": "_init_weights",
    "signature": "_init_weights(self, m)",
    "docstring": "",
    "identifiers": [
      "_init_weights",
      "bias",
      "constant_",
      "conv1d",
      "init",
      "isinstance",
      "m",
      "nn",
      "self",
      "std",
      "trunc_normal_",
      "weight"
    ],
    "start_line": 599,
    "end_line": 602,
    "text": "def _init_weights(self, m):\n        if isinstance(m, nn.Conv1d):\n            nn.init.trunc_normal_(m.weight, std=0.02)\n            nn.init.constant_(m.bias, 0)"
  },
  {
    "id": "81bfbb43b4e05b9e37c5d14c78ae234768ce56c8",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "function",
    "name": "__init__",
    "signature": "__init__(\n        self,\n        # Transformer\n        embed_dim: int,\n        num_layers: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        # iSTFT\n        hop_length: int = 240,\n        # Causal\n        causal: bool = False,\n    )",
    "docstring": "",
    "identifiers": [
      "__init__",
      "_init_weights",
      "apply",
      "backbone",
      "bool",
      "causal",
      "causalvocosbackbone",
      "convtranspose1d",
      "dropout",
      "embed_dim",
      "float",
      "gelu",
      "hop_length",
      "int",
      "isift",
      "istfthead",
      "kernel_size",
      "nn",
      "num_heads",
      "num_layers",
      "output_padding",
      "padding",
      "self",
      "sequential",
      "stride",
      "super",
      "upsample_conv",
      "vocosbackbone"
    ],
    "start_line": 551,
    "end_line": 597,
    "text": "def __init__(\n        self,\n        # Transformer\n        embed_dim: int,\n        num_layers: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        # iSTFT\n        hop_length: int = 240,\n        # Causal\n        causal: bool = False,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_layers = num_layers\n        self.num_heads = num_heads\n        self.hop_length = hop_length\n        self.causal = causal\n\n        # Output upsample\n        self.upsample_conv = nn.Sequential(\n            nn.ConvTranspose1d(\n                embed_dim,\n                embed_dim,\n                kernel_size=3,\n                stride=2,\n                padding=0,  # Do not fill input side\n                output_padding=0,  # Can be adjusted to precisely control length\n            ),\n            nn.GELU(),\n            nn.ConvTranspose1d(\n                embed_dim,\n                embed_dim,\n                kernel_size=3,\n                stride=1,\n                padding=0,  # Do not fill input side\n            ),\n            nn.GELU(),\n        )\n        self.backbone = (\n            CausalVocosBackbone(embed_dim, num_layers, num_heads, dropout)\n            if causal\n            else VocosBackbone(embed_dim, num_layers, num_heads, dropout)\n        )\n        self.isift = ISTFTHead(embed_dim, hop_length * 4, hop_length, padding=\"same\")\n        # Init weights\n        self.apply(self._init_weights)"
  },
  {
    "id": "cf5fc8b9ab04b88947ab3b61d7a78e3172411dd4",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "class",
    "name": "ISTFTHead",
    "signature": "class ISTFTHead(nn.Module)",
    "docstring": "ISTFT Head module for predicting STFT complex coefficients.\n\n    Args:\n        dim (int): Hidden dimension of the model.\n        n_fft (int): Size of Fourier transform.\n        hop_length (int): The distance between neighboring sliding window frames, which should align with\n                          the resolution of the input features.\n        padding (str, optional): Type of padding. Options are \"center\" or \"same\". Defaults to \"same\".",
    "identifiers": [
      "__init__",
      "audio",
      "audio_length",
      "bool",
      "cache",
      "chunk",
      "clip",
      "cos",
      "dim",
      "exp",
      "forward",
      "forward_chunk",
      "hop_length",
      "int",
      "istft",
      "istfthead",
      "last_chunk",
      "linear",
      "mag",
      "max",
      "module",
      "n_fft",
      "new_cache",
      "nn",
      "out",
      "out_dim",
      "p",
      "padding",
      "s",
      "self",
      "sin",
      "str",
      "super",
      "tensor",
      "torch",
      "transpose",
      "win_length",
      "x",
      "x_len",
      "x_pred",
      "y"
    ],
    "start_line": 471,
    "end_line": 546,
    "text": "class ISTFTHead(nn.Module):\n    \"\"\"\n    ISTFT Head module for predicting STFT complex coefficients.\n\n    Args:\n        dim (int): Hidden dimension of the model.\n        n_fft (int): Size of Fourier transform.\n        hop_length (int): The distance between neighboring sliding window frames, which should align with\n                          the resolution of the input features.\n        padding (str, optional): Type of padding. Options are \"center\" or \"same\". Defaults to \"same\".\n    \"\"\"\n\n    def __init__(self, dim: int, n_fft: int, hop_length: int, padding: str = \"same\"):\n        super().__init__()\n        self.hop_length = hop_length\n        out_dim = n_fft + 2\n        self.out = torch.nn.Linear(dim, out_dim)\n        self.istft = ISTFT(\n            n_fft=n_fft, hop_length=hop_length, win_length=n_fft, padding=padding\n        )\n\n    def forward(self, x: torch.Tensor, x_len: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass of the ISTFTHead module.\n\n        Args:\n            x (Tensor): Input tensor of shape (B, L, H), where B is the batch size,\n                        L is the sequence length, and H denotes the model dimension.\n\n        Returns:\n            Tensor: Reconstructed time-domain audio signal of shape (B, T), where T is the length of the output signal.\n        \"\"\"\n        x_pred = self.out(x)\n        x_pred = x_pred.transpose(1, 2)\n        mag, p = x_pred.chunk(2, dim=1)\n        mag = torch.exp(mag)\n        mag = torch.clip(\n            mag, max=1e2\n        )  # safeguard to prevent excessively large magnitudes\n        # wrapping happens here. These two lines produce real and imaginary value\n        x = torch.cos(p)\n        y = torch.sin(p)\n        # recalculating phase here does not produce anything new\n        # only costs time\n        # phase = torch.atan2(y, x)\n        # S = mag * torch.exp(phase * 1j)\n        # better directly produce the complex value\n        S = mag * (x + 1j * y)\n        audio = self.istft(S)\n        audio_length = x_len * self.hop_length\n        return audio, audio_length\n\n    def forward_chunk(\n        self, x: torch.Tensor, cache: torch.Tensor = None, last_chunk: bool = False\n    ):\n        \"\"\"ISTFTHead can be adapted in streaming inference without retraining.\n\n        Args:\n            x: shape (B, T, C)\n            cache: shape (B, N, T=3), istft cache\n        Returns:\n            audio: shape (B, t)\n        \"\"\"\n        x_pred = self.out(x)\n        x_pred = x_pred.transpose(1, 2)\n        mag, p = x_pred.chunk(2, dim=1)\n        mag = torch.exp(mag)  # (B, C, T)\n        mag = torch.clip(\n            mag, max=1e2\n        )  # safeguard to prevent excessively large magnitudes\n        # wrapping happens here. These two lines produce real and imaginary value\n        x = torch.cos(p)\n        y = torch.sin(p)\n        S = mag * (x + 1j * y)  # (B, C, T)\n        audio, new_cache = self.istft.forward_chunk(S, cache, last_chunk)\n        return audio, new_cache"
  },
  {
    "id": "0b46a9eee310c7ba7f4155652966d5d66d5829f9",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "function",
    "name": "forward_chunk",
    "signature": "forward_chunk(\n        self, x: torch.Tensor, cache: torch.Tensor = None, last_chunk: bool = False\n    )",
    "docstring": "ISTFTHead can be adapted in streaming inference without retraining.\n\n        Args:\n            x: shape (B, T, C)\n            cache: shape (B, N, T=3), istft cache\n        Returns:\n            audio: shape (B, t)",
    "identifiers": [
      "audio",
      "bool",
      "cache",
      "chunk",
      "clip",
      "cos",
      "dim",
      "exp",
      "forward_chunk",
      "istft",
      "last_chunk",
      "mag",
      "max",
      "new_cache",
      "out",
      "p",
      "s",
      "self",
      "sin",
      "tensor",
      "torch",
      "transpose",
      "x",
      "x_pred",
      "y"
    ],
    "start_line": 523,
    "end_line": 546,
    "text": "def forward_chunk(\n        self, x: torch.Tensor, cache: torch.Tensor = None, last_chunk: bool = False\n    ):\n        \"\"\"ISTFTHead can be adapted in streaming inference without retraining.\n\n        Args:\n            x: shape (B, T, C)\n            cache: shape (B, N, T=3), istft cache\n        Returns:\n            audio: shape (B, t)\n        \"\"\"\n        x_pred = self.out(x)\n        x_pred = x_pred.transpose(1, 2)\n        mag, p = x_pred.chunk(2, dim=1)\n        mag = torch.exp(mag)  # (B, C, T)\n        mag = torch.clip(\n            mag, max=1e2\n        )  # safeguard to prevent excessively large magnitudes\n        # wrapping happens here. These two lines produce real and imaginary value\n        x = torch.cos(p)\n        y = torch.sin(p)\n        S = mag * (x + 1j * y)  # (B, C, T)\n        audio, new_cache = self.istft.forward_chunk(S, cache, last_chunk)\n        return audio, new_cache"
  },
  {
    "id": "6aad161c8228237d5a85c748f87df1e02117935d",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "function",
    "name": "forward",
    "signature": "forward(self, x: torch.Tensor, x_len: torch.Tensor)",
    "docstring": "Forward pass of the ISTFTHead module.\n\n        Args:\n            x (Tensor): Input tensor of shape (B, L, H), where B is the batch size,\n                        L is the sequence length, and H denotes the model dimension.\n\n        Returns:\n            Tensor: Reconstructed time-domain audio signal of shape (B, T), where T is the length of the output signal.",
    "identifiers": [
      "audio",
      "audio_length",
      "chunk",
      "clip",
      "cos",
      "dim",
      "exp",
      "forward",
      "hop_length",
      "istft",
      "mag",
      "max",
      "out",
      "p",
      "s",
      "self",
      "sin",
      "tensor",
      "torch",
      "transpose",
      "x",
      "x_len",
      "x_pred",
      "y"
    ],
    "start_line": 492,
    "end_line": 521,
    "text": "def forward(self, x: torch.Tensor, x_len: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass of the ISTFTHead module.\n\n        Args:\n            x (Tensor): Input tensor of shape (B, L, H), where B is the batch size,\n                        L is the sequence length, and H denotes the model dimension.\n\n        Returns:\n            Tensor: Reconstructed time-domain audio signal of shape (B, T), where T is the length of the output signal.\n        \"\"\"\n        x_pred = self.out(x)\n        x_pred = x_pred.transpose(1, 2)\n        mag, p = x_pred.chunk(2, dim=1)\n        mag = torch.exp(mag)\n        mag = torch.clip(\n            mag, max=1e2\n        )  # safeguard to prevent excessively large magnitudes\n        # wrapping happens here. These two lines produce real and imaginary value\n        x = torch.cos(p)\n        y = torch.sin(p)\n        # recalculating phase here does not produce anything new\n        # only costs time\n        # phase = torch.atan2(y, x)\n        # S = mag * torch.exp(phase * 1j)\n        # better directly produce the complex value\n        S = mag * (x + 1j * y)\n        audio = self.istft(S)\n        audio_length = x_len * self.hop_length\n        return audio, audio_length"
  },
  {
    "id": "a7e6d5faca2fd84b8a97f99b5cf189796d03fcf3",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "function",
    "name": "__init__",
    "signature": "__init__(self, dim: int, n_fft: int, hop_length: int, padding: str = \"same\")",
    "docstring": "",
    "identifiers": [
      "__init__",
      "dim",
      "hop_length",
      "int",
      "istft",
      "linear",
      "n_fft",
      "nn",
      "out",
      "out_dim",
      "padding",
      "self",
      "str",
      "super",
      "torch",
      "win_length"
    ],
    "start_line": 483,
    "end_line": 490,
    "text": "def __init__(self, dim: int, n_fft: int, hop_length: int, padding: str = \"same\"):\n        super().__init__()\n        self.hop_length = hop_length\n        out_dim = n_fft + 2\n        self.out = torch.nn.Linear(dim, out_dim)\n        self.istft = ISTFT(\n            n_fft=n_fft, hop_length=hop_length, win_length=n_fft, padding=padding\n        )"
  },
  {
    "id": "71a21c5957e675fed49cd77ec512713f19975dbe",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "class",
    "name": "ISTFT",
    "signature": "class ISTFT(nn.Module)",
    "docstring": "Custom implementation of ISTFT since torch.istft doesn't allow custom padding (other than `center=True`) with\n    windowing. This is because the NOLA (Nonzero Overlap Add) check fails at the edges.\n    See issue: https://github.com/pytorch/pytorch/issues/62323\n    Specifically, in the context of neural vocoding we are interested in \"same\" padding analogous to CNNs.\n    The NOLA constraint is met as we trim padded samples anyway.\n\n    Args:\n        n_fft (int): Size of Fourier transform.\n        hop_length (int): The distance between neighboring sliding window frames.\n        win_length (int): The size of window frame and STFT filter.\n        padding (str, optional): Type of padding. Options are \"center\" or \"same\". Defaults to \"same\".",
    "identifiers": [
      "__init__",
      "all",
      "b",
      "bool",
      "cache",
      "cat",
      "center",
      "dim",
      "expand",
      "fft",
      "fold",
      "forward",
      "forward_chunk",
      "functional",
      "hann_window",
      "hop_length",
      "ifft",
      "int",
      "irfft",
      "istft",
      "kernel_size",
      "last_chunk",
      "module",
      "n",
      "n_fft",
      "new_cache",
      "new_cache_t",
      "nn",
      "norm",
      "output_size",
      "pad",
      "padding",
      "register_buffer",
      "self",
      "shape",
      "spec",
      "square",
      "squeeze",
      "str",
      "stride",
      "super",
      "t",
      "tensor",
      "torch",
      "transpose",
      "valueerror",
      "win_length",
      "window",
      "window_envelope",
      "window_sq",
      "y"
    ],
    "start_line": 323,
    "end_line": 468,
    "text": "class ISTFT(nn.Module):\n    \"\"\"\n    Custom implementation of ISTFT since torch.istft doesn't allow custom padding (other than `center=True`) with\n    windowing. This is because the NOLA (Nonzero Overlap Add) check fails at the edges.\n    See issue: https://github.com/pytorch/pytorch/issues/62323\n    Specifically, in the context of neural vocoding we are interested in \"same\" padding analogous to CNNs.\n    The NOLA constraint is met as we trim padded samples anyway.\n\n    Args:\n        n_fft (int): Size of Fourier transform.\n        hop_length (int): The distance between neighboring sliding window frames.\n        win_length (int): The size of window frame and STFT filter.\n        padding (str, optional): Type of padding. Options are \"center\" or \"same\". Defaults to \"same\".\n    \"\"\"\n\n    def __init__(\n        self, n_fft: int, hop_length: int, win_length: int, padding: str = \"same\"\n    ):\n        super().__init__()\n        assert padding in [\"center\", \"same\"], \"Padding must be 'center' or 'same'.\"\n        self.padding = padding\n        self.n_fft = n_fft\n        self.hop_length = hop_length\n        self.win_length = win_length\n        window = torch.hann_window(win_length)\n        self.register_buffer(\"window\", window)\n\n    def forward(self, spec: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute the Inverse Short Time Fourier Transform (ISTFT) of a complex spectrogram.\n\n        Args:\n            spec (Tensor): Input complex spectrogram of shape (B, N, T), where B is the batch size,\n                            N is the number of frequency bins, and T is the number of time frames.\n\n        Returns:\n            Tensor: Reconstructed time-domain signal of shape (B, L), where L is the length of the output signal.\n        \"\"\"\n        if self.padding == \"center\":\n            # Fallback to pytorch native implementation\n            return torch.istft(\n                spec,\n                self.n_fft,\n                self.hop_length,\n                self.win_length,\n                self.window,\n                center=True,\n            )\n        elif self.padding == \"same\":\n            pad = (self.win_length - self.hop_length) // 2\n        else:\n            raise ValueError(\"Padding must be 'center' or 'same'.\")\n\n        assert spec.dim() == 3, \"Expected a 3D tensor as input\"\n        B, N, T = spec.shape\n\n        # Inverse FFT\n        ifft = torch.fft.irfft(spec, self.n_fft, dim=1, norm=\"backward\")\n        ifft = ifft * self.window[None, :, None]\n\n        # Overlap and Add\n        output_size = (T - 1) * self.hop_length + self.win_length\n        y = torch.nn.functional.fold(\n            ifft,\n            output_size=(1, output_size),\n            kernel_size=(1, self.win_length),\n            stride=(1, self.hop_length),\n        )[:, 0, 0, pad:-pad]\n\n        # Window envelope\n        window_sq = self.window.square().expand(1, T, -1).transpose(1, 2)\n        window_envelope = torch.nn.functional.fold(\n            window_sq,\n            output_size=(1, output_size),\n            kernel_size=(1, self.win_length),\n            stride=(1, self.hop_length),\n        ).squeeze()[pad:-pad]\n\n        # Normalize\n        assert (window_envelope > 1e-11).all()\n        y = y / window_envelope\n\n        return y\n\n    def forward_chunk(\n        self, spec: torch.Tensor, cache: torch.Tensor = None, last_chunk: bool = False\n    ):\n        \"\"\"Forward only one frame.\n\n        Args:\n            spec: shape (B, N, T=chunk_size)\n            cache: previous chunk's last ifft frame, shape (B, N, T=3)\n            last_chunk: if last_chunk, will not trim the last (win-hop) segment\n        Returns:\n            y: shape (B, T=effective_length)\n        \"\"\"\n        assert self.padding == \"same\", \"Padding must be same.\"\n        assert (\n            self.win_length % self.hop_length == 0\n        ), f\"{self.win_length} {self.hop_length}\"\n        pad = (self.win_length - self.hop_length) // 2\n\n        # Inverse FFT\n        ifft = torch.fft.irfft(spec, self.n_fft, dim=1, nor"
  },
  {
    "id": "fcf6c4cfd70727f02f2a8900bf1ea833d8a5ce24",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "function",
    "name": "forward_chunk",
    "signature": "forward_chunk(\n        self, spec: torch.Tensor, cache: torch.Tensor = None, last_chunk: bool = False\n    )",
    "docstring": "Forward only one frame.\n\n        Args:\n            spec: shape (B, N, T=chunk_size)\n            cache: previous chunk's last ifft frame, shape (B, N, T=3)\n            last_chunk: if last_chunk, will not trim the last (win-hop) segment\n        Returns:\n            y: shape (B, T=effective_length)",
    "identifiers": [
      "bool",
      "cache",
      "cat",
      "dim",
      "expand",
      "fft",
      "fold",
      "forward_chunk",
      "functional",
      "hop_length",
      "ifft",
      "irfft",
      "kernel_size",
      "last_chunk",
      "n_fft",
      "new_cache",
      "new_cache_t",
      "nn",
      "norm",
      "output_size",
      "pad",
      "padding",
      "self",
      "shape",
      "spec",
      "square",
      "squeeze",
      "stride",
      "tensor",
      "torch",
      "transpose",
      "win_length",
      "window",
      "window_envelope",
      "window_sq",
      "y"
    ],
    "start_line": 407,
    "end_line": 468,
    "text": "def forward_chunk(\n        self, spec: torch.Tensor, cache: torch.Tensor = None, last_chunk: bool = False\n    ):\n        \"\"\"Forward only one frame.\n\n        Args:\n            spec: shape (B, N, T=chunk_size)\n            cache: previous chunk's last ifft frame, shape (B, N, T=3)\n            last_chunk: if last_chunk, will not trim the last (win-hop) segment\n        Returns:\n            y: shape (B, T=effective_length)\n        \"\"\"\n        assert self.padding == \"same\", \"Padding must be same.\"\n        assert (\n            self.win_length % self.hop_length == 0\n        ), f\"{self.win_length} {self.hop_length}\"\n        pad = (self.win_length - self.hop_length) // 2\n\n        # Inverse FFT\n        ifft = torch.fft.irfft(spec, self.n_fft, dim=1, norm=\"backward\")\n        ifft = ifft * self.window[None, :, None]  # (B, N, T=chunk_size)\n\n        # Append previous cache\n        if cache is not None:\n            ifft = torch.cat([cache, ifft], dim=-1)\n        new_cache_t = self.win_length // self.hop_length - 1\n        new_cache = ifft[..., -new_cache_t:]\n\n        # Overlap and Add\n        output_size = (ifft.shape[-1] - 1) * self.hop_length + self.win_length\n        y = torch.nn.functional.fold(\n            ifft,\n            output_size=(1, output_size),\n            kernel_size=(1, self.win_length),\n            stride=(1, self.hop_length),\n        )[:, 0, 0, :]\n\n        # Window envelope\n        window_sq = (\n            self.window.square().expand(1, ifft.shape[-1], -1).transpose(1, 2)\n        )  # (B=1, N, T)\n        window_envelope = torch.nn.functional.fold(\n            window_sq,\n            output_size=(1, output_size),\n            kernel_size=(1, self.win_length),\n            stride=(1, self.hop_length),\n        ).squeeze()\n\n        # Normalize\n        # assert (window_envelope > 1e-11).all()\n        y = y / window_envelope\n\n        # Only take effective part\n        if cache is None:\n            y = y[:, pad:]\n        else:\n            y = y[:, (self.win_length - self.hop_length) :]\n        if last_chunk:\n            y = y[:, :-pad]\n        else:\n            y = y[:, : -(self.win_length - self.hop_length)]\n        return y, new_cache"
  },
  {
    "id": "0a2cf892bd912cd651e6c229b533e08f04417ef1",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "function",
    "name": "forward",
    "signature": "forward(self, spec: torch.Tensor)",
    "docstring": "Compute the Inverse Short Time Fourier Transform (ISTFT) of a complex spectrogram.\n\n        Args:\n            spec (Tensor): Input complex spectrogram of shape (B, N, T), where B is the batch size,\n                            N is the number of frequency bins, and T is the number of time frames.\n\n        Returns:\n            Tensor: Reconstructed time-domain signal of shape (B, L), where L is the length of the output signal.",
    "identifiers": [
      "all",
      "b",
      "center",
      "dim",
      "expand",
      "fft",
      "fold",
      "forward",
      "functional",
      "hop_length",
      "ifft",
      "irfft",
      "istft",
      "kernel_size",
      "n",
      "n_fft",
      "nn",
      "norm",
      "output_size",
      "pad",
      "padding",
      "self",
      "shape",
      "spec",
      "square",
      "squeeze",
      "stride",
      "t",
      "tensor",
      "torch",
      "transpose",
      "valueerror",
      "win_length",
      "window",
      "window_envelope",
      "window_sq",
      "y"
    ],
    "start_line": 350,
    "end_line": 405,
    "text": "def forward(self, spec: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute the Inverse Short Time Fourier Transform (ISTFT) of a complex spectrogram.\n\n        Args:\n            spec (Tensor): Input complex spectrogram of shape (B, N, T), where B is the batch size,\n                            N is the number of frequency bins, and T is the number of time frames.\n\n        Returns:\n            Tensor: Reconstructed time-domain signal of shape (B, L), where L is the length of the output signal.\n        \"\"\"\n        if self.padding == \"center\":\n            # Fallback to pytorch native implementation\n            return torch.istft(\n                spec,\n                self.n_fft,\n                self.hop_length,\n                self.win_length,\n                self.window,\n                center=True,\n            )\n        elif self.padding == \"same\":\n            pad = (self.win_length - self.hop_length) // 2\n        else:\n            raise ValueError(\"Padding must be 'center' or 'same'.\")\n\n        assert spec.dim() == 3, \"Expected a 3D tensor as input\"\n        B, N, T = spec.shape\n\n        # Inverse FFT\n        ifft = torch.fft.irfft(spec, self.n_fft, dim=1, norm=\"backward\")\n        ifft = ifft * self.window[None, :, None]\n\n        # Overlap and Add\n        output_size = (T - 1) * self.hop_length + self.win_length\n        y = torch.nn.functional.fold(\n            ifft,\n            output_size=(1, output_size),\n            kernel_size=(1, self.win_length),\n            stride=(1, self.hop_length),\n        )[:, 0, 0, pad:-pad]\n\n        # Window envelope\n        window_sq = self.window.square().expand(1, T, -1).transpose(1, 2)\n        window_envelope = torch.nn.functional.fold(\n            window_sq,\n            output_size=(1, output_size),\n            kernel_size=(1, self.win_length),\n            stride=(1, self.hop_length),\n        ).squeeze()[pad:-pad]\n\n        # Normalize\n        assert (window_envelope > 1e-11).all()\n        y = y / window_envelope\n\n        return y"
  },
  {
    "id": "2cdf0dc7cfd5760656ac498bc2f41d562aba552d",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "function",
    "name": "__init__",
    "signature": "__init__(\n        self, n_fft: int, hop_length: int, win_length: int, padding: str = \"same\"\n    )",
    "docstring": "",
    "identifiers": [
      "__init__",
      "hann_window",
      "hop_length",
      "int",
      "n_fft",
      "padding",
      "register_buffer",
      "self",
      "str",
      "super",
      "torch",
      "win_length",
      "window"
    ],
    "start_line": 338,
    "end_line": 348,
    "text": "def __init__(\n        self, n_fft: int, hop_length: int, win_length: int, padding: str = \"same\"\n    ):\n        super().__init__()\n        assert padding in [\"center\", \"same\"], \"Padding must be 'center' or 'same'.\"\n        self.padding = padding\n        self.n_fft = n_fft\n        self.hop_length = hop_length\n        self.win_length = win_length\n        window = torch.hann_window(win_length)\n        self.register_buffer(\"window\", window)"
  },
  {
    "id": "3ea3b7929d83d3d9d66a535e7c74eb2ef0901cc8",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "class",
    "name": "CausalVocosBackbone",
    "signature": "class CausalVocosBackbone(nn.Module)",
    "docstring": "Streaming Vocos backbone based on Transformer layers",
    "identifiers": [
      "_",
      "__init__",
      "append",
      "attention_mask",
      "cache1",
      "cache2",
      "cache3",
      "cache4",
      "cache5",
      "cat",
      "causalconv1d",
      "causalresnetblock",
      "causalvocosbackbone",
      "chunk",
      "chunk_size",
      "conv_cache1",
      "conv_cache2",
      "dim",
      "dropout",
      "embed_dim",
      "enumerate",
      "eps",
      "final_norm",
      "float",
      "forward",
      "forward_chunk",
      "idx",
      "in_proj",
      "int",
      "kernel_size",
      "kv_cache",
      "kv_cache_i",
      "layer",
      "layernorm",
      "make_block_causal_mask",
      "module",
      "modulelist",
      "new_cache1",
      "new_cache2",
      "new_cache3",
      "new_cache4",
      "new_cache5",
      "new_conv_cache1",
      "new_conv_cache2",
      "new_kv_cache",
      "new_kv_cache_i",
      "nn",
      "num_heads",
      "num_layers",
      "post_net",
      "prior_net",
      "range",
      "self",
      "sequential",
      "stack",
      "super",
      "tensor",
      "torch",
      "transformers",
      "transpose",
      "whisperencoderlayer",
      "x",
      "x_lens"
    ],
    "start_line": 225,
    "end_line": 320,
    "text": "class CausalVocosBackbone(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int = 1024,\n        num_layers: int = 12,\n        num_heads: int = 16,\n        dropout: float = 0.1,\n    ):\n        super().__init__()\n        self.in_proj = CausalConv1d(embed_dim, embed_dim, kernel_size=7)\n        self.prior_net = nn.Sequential(\n            CausalResnetBlock(embed_dim, embed_dim, dropout=dropout),\n            CausalResnetBlock(embed_dim, embed_dim, dropout=dropout),\n        )\n        self.transformers = nn.ModuleList(\n            [WhisperEncoderLayer(embed_dim, num_heads) for _ in range(num_layers)]\n        )\n        self.post_net = nn.Sequential(\n            CausalResnetBlock(embed_dim, embed_dim, dropout=dropout),\n            CausalResnetBlock(embed_dim, embed_dim, dropout=dropout),\n        )\n        self.final_norm = nn.LayerNorm(embed_dim, eps=1e-6)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        x_lens: torch.Tensor,\n    ):\n        \"\"\"\n        Args:\n            x: shape (b, t, c)\n            x_lens: shape (b,)\n        \"\"\"\n        x = x.transpose(1, 2)\n        x = self.in_proj(x)\n        x = self.prior_net(x)\n        x = x.transpose(1, 2)\n\n        # NOTE(sfy): We have no padding in training, so safe for sdpa attention, no Nan.\n        # Also, 1 token(12.5Hz) -> 4 latents(50Hz) -> 8 latents(100Hz),\n        # so we design a 8 block causal attention mask instead of fully causal to improve performance\n        attention_mask = make_block_causal_mask(x_lens, chunk_size=8)\n        for layer in self.transformers:\n            x = layer(x, attention_mask)\n\n        x = x.transpose(1, 2)\n        x = self.post_net(x)\n        x = x.transpose(1, 2)\n        x = self.final_norm(x)\n        return x\n\n    def forward_chunk(\n        self,\n        x: torch.Tensor,\n        conv_cache1: torch.Tensor = None,\n        conv_cache2: torch.Tensor = None,\n        kv_cache: torch.Tensor = None,\n    ):\n        # Unpack cache\n        cache1 = conv_cache1\n        cache2, cache3, cache4, cache5 = (\n            (None, None, None, None)\n            if conv_cache2 is None\n            else conv_cache2.chunk(4, dim=1)\n        )\n\n        # cache1: shape (b, c=embed_dim, t=6)\n        x = x.transpose(1, 2)\n        x, new_cache1 = self.in_proj.forward_chunk(x, cache1)\n        # cache2: shape (b, c=embed_dim*2, t=2)\n        x, new_cache2 = self.prior_net[0].forward_chunk(x, cache2)\n        # cache3: shape (b, c=embed_dim*2, t=2)\n        x, new_cache3 = self.prior_net[1].forward_chunk(x, cache3)\n        x = x.transpose(1, 2)\n\n        # k,v-cache: shape (b, nlayer, nh, t, c*2)\n        new_kv_cache = []\n        for idx, layer in enumerate(self.transformers):\n            kv_cache_i = None if kv_cache is None else kv_cache[:, idx]\n            x, new_kv_cache_i = layer.forward_chunk(x, kv_cache=kv_cache_i)\n            new_kv_cache.append(new_kv_cache_i)\n        new_kv_cache = torch.stack(new_kv_cache, dim=1)\n\n        x = x.transpose(1, 2)\n        # cache4: shape (b, c=embed_dim*2, t=2)\n        x, new_cache4 = self.post_net[0].forward_chunk(x, cache4)\n        # cache5: shape (b, c=embed_dim*2, t=2)\n        x, new_cache5 = self.post_net[1].forward_chunk(x, cache5)\n        x = x.transpose(1, 2)\n        x = self.final_norm(x)\n\n        new_conv_cache1 = new_cache1\n        new_conv_cache2 = torch.cat(\n            [new_cache2, new_cache3, new_cache4, new_cache5], dim=1\n        )\n        return x, new_conv_cache1, new_conv_cache2, new_kv_cache"
  },
  {
    "id": "4882f70dde4022ed6d489451748d4e85da84057f",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "function",
    "name": "forward_chunk",
    "signature": "forward_chunk(\n        self,\n        x: torch.Tensor,\n        conv_cache1: torch.Tensor = None,\n        conv_cache2: torch.Tensor = None,\n        kv_cache: torch.Tensor = None,\n    )",
    "docstring": "",
    "identifiers": [
      "append",
      "cache1",
      "cache2",
      "cache3",
      "cache4",
      "cache5",
      "cat",
      "chunk",
      "conv_cache1",
      "conv_cache2",
      "dim",
      "enumerate",
      "final_norm",
      "forward_chunk",
      "idx",
      "in_proj",
      "kv_cache",
      "kv_cache_i",
      "layer",
      "new_cache1",
      "new_cache2",
      "new_cache3",
      "new_cache4",
      "new_cache5",
      "new_conv_cache1",
      "new_conv_cache2",
      "new_kv_cache",
      "new_kv_cache_i",
      "post_net",
      "prior_net",
      "self",
      "stack",
      "tensor",
      "torch",
      "transformers",
      "transpose",
      "x"
    ],
    "start_line": 276,
    "end_line": 320,
    "text": "def forward_chunk(\n        self,\n        x: torch.Tensor,\n        conv_cache1: torch.Tensor = None,\n        conv_cache2: torch.Tensor = None,\n        kv_cache: torch.Tensor = None,\n    ):\n        # Unpack cache\n        cache1 = conv_cache1\n        cache2, cache3, cache4, cache5 = (\n            (None, None, None, None)\n            if conv_cache2 is None\n            else conv_cache2.chunk(4, dim=1)\n        )\n\n        # cache1: shape (b, c=embed_dim, t=6)\n        x = x.transpose(1, 2)\n        x, new_cache1 = self.in_proj.forward_chunk(x, cache1)\n        # cache2: shape (b, c=embed_dim*2, t=2)\n        x, new_cache2 = self.prior_net[0].forward_chunk(x, cache2)\n        # cache3: shape (b, c=embed_dim*2, t=2)\n        x, new_cache3 = self.prior_net[1].forward_chunk(x, cache3)\n        x = x.transpose(1, 2)\n\n        # k,v-cache: shape (b, nlayer, nh, t, c*2)\n        new_kv_cache = []\n        for idx, layer in enumerate(self.transformers):\n            kv_cache_i = None if kv_cache is None else kv_cache[:, idx]\n            x, new_kv_cache_i = layer.forward_chunk(x, kv_cache=kv_cache_i)\n            new_kv_cache.append(new_kv_cache_i)\n        new_kv_cache = torch.stack(new_kv_cache, dim=1)\n\n        x = x.transpose(1, 2)\n        # cache4: shape (b, c=embed_dim*2, t=2)\n        x, new_cache4 = self.post_net[0].forward_chunk(x, cache4)\n        # cache5: shape (b, c=embed_dim*2, t=2)\n        x, new_cache5 = self.post_net[1].forward_chunk(x, cache5)\n        x = x.transpose(1, 2)\n        x = self.final_norm(x)\n\n        new_conv_cache1 = new_cache1\n        new_conv_cache2 = torch.cat(\n            [new_cache2, new_cache3, new_cache4, new_cache5], dim=1\n        )\n        return x, new_conv_cache1, new_conv_cache2, new_kv_cache"
  },
  {
    "id": "323fd0267a5398870333904799cd5c2f9c989ed5",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "function",
    "name": "forward",
    "signature": "forward(\n        self,\n        x: torch.Tensor,\n        x_lens: torch.Tensor,\n    )",
    "docstring": "Args:\n            x: shape (b, t, c)\n            x_lens: shape (b,)",
    "identifiers": [
      "attention_mask",
      "chunk_size",
      "final_norm",
      "forward",
      "in_proj",
      "layer",
      "make_block_causal_mask",
      "post_net",
      "prior_net",
      "self",
      "tensor",
      "torch",
      "transformers",
      "transpose",
      "x",
      "x_lens"
    ],
    "start_line": 248,
    "end_line": 274,
    "text": "def forward(\n        self,\n        x: torch.Tensor,\n        x_lens: torch.Tensor,\n    ):\n        \"\"\"\n        Args:\n            x: shape (b, t, c)\n            x_lens: shape (b,)\n        \"\"\"\n        x = x.transpose(1, 2)\n        x = self.in_proj(x)\n        x = self.prior_net(x)\n        x = x.transpose(1, 2)\n\n        # NOTE(sfy): We have no padding in training, so safe for sdpa attention, no Nan.\n        # Also, 1 token(12.5Hz) -> 4 latents(50Hz) -> 8 latents(100Hz),\n        # so we design a 8 block causal attention mask instead of fully causal to improve performance\n        attention_mask = make_block_causal_mask(x_lens, chunk_size=8)\n        for layer in self.transformers:\n            x = layer(x, attention_mask)\n\n        x = x.transpose(1, 2)\n        x = self.post_net(x)\n        x = x.transpose(1, 2)\n        x = self.final_norm(x)\n        return x"
  },
  {
    "id": "7184e72be0e3d6700e894f91d78032ce0331d9da",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "function",
    "name": "__init__",
    "signature": "__init__(\n        self,\n        embed_dim: int = 1024,\n        num_layers: int = 12,\n        num_heads: int = 16,\n        dropout: float = 0.1,\n    )",
    "docstring": "",
    "identifiers": [
      "_",
      "__init__",
      "causalconv1d",
      "causalresnetblock",
      "dropout",
      "embed_dim",
      "eps",
      "final_norm",
      "float",
      "in_proj",
      "int",
      "kernel_size",
      "layernorm",
      "modulelist",
      "nn",
      "num_heads",
      "num_layers",
      "post_net",
      "prior_net",
      "range",
      "self",
      "sequential",
      "super",
      "transformers",
      "whisperencoderlayer"
    ],
    "start_line": 226,
    "end_line": 246,
    "text": "def __init__(\n        self,\n        embed_dim: int = 1024,\n        num_layers: int = 12,\n        num_heads: int = 16,\n        dropout: float = 0.1,\n    ):\n        super().__init__()\n        self.in_proj = CausalConv1d(embed_dim, embed_dim, kernel_size=7)\n        self.prior_net = nn.Sequential(\n            CausalResnetBlock(embed_dim, embed_dim, dropout=dropout),\n            CausalResnetBlock(embed_dim, embed_dim, dropout=dropout),\n        )\n        self.transformers = nn.ModuleList(\n            [WhisperEncoderLayer(embed_dim, num_heads) for _ in range(num_layers)]\n        )\n        self.post_net = nn.Sequential(\n            CausalResnetBlock(embed_dim, embed_dim, dropout=dropout),\n            CausalResnetBlock(embed_dim, embed_dim, dropout=dropout),\n        )\n        self.final_norm = nn.LayerNorm(embed_dim, eps=1e-6)"
  },
  {
    "id": "01164d5a72c962b3fa1639ff8024163074ea675e",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "class",
    "name": "VocosBackbone",
    "signature": "class VocosBackbone(nn.Module)",
    "docstring": "Nonstreaming Vocos backbone based on Transformer layers",
    "identifiers": [
      "_",
      "__init__",
      "attention_mask",
      "conv1d",
      "dropout",
      "embed_dim",
      "eps",
      "final_norm",
      "float",
      "forward",
      "in_proj",
      "int",
      "kernel_size",
      "layer",
      "layernorm",
      "make_nonpad_mask",
      "module",
      "modulelist",
      "nn",
      "num_heads",
      "num_layers",
      "padding",
      "post_net",
      "prior_net",
      "range",
      "resnetblock",
      "self",
      "sequential",
      "super",
      "tensor",
      "torch",
      "transformers",
      "transpose",
      "unsqueeze",
      "vocosbackbone",
      "whisperencoderlayer",
      "x",
      "x_lens"
    ],
    "start_line": 175,
    "end_line": 221,
    "text": "class VocosBackbone(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int = 1024,\n        num_layers: int = 12,\n        num_heads: int = 16,\n        dropout: float = 0.1,\n    ):\n        super().__init__()\n        self.in_proj = nn.Conv1d(embed_dim, embed_dim, kernel_size=7, padding=3)\n        self.prior_net = nn.Sequential(\n            ResnetBlock(embed_dim, embed_dim, dropout=dropout),\n            ResnetBlock(embed_dim, embed_dim, dropout=dropout),\n        )\n        self.transformers = nn.ModuleList(\n            [WhisperEncoderLayer(embed_dim, num_heads) for _ in range(num_layers)]\n        )\n        self.post_net = nn.Sequential(\n            ResnetBlock(embed_dim, embed_dim, dropout=dropout),\n            ResnetBlock(embed_dim, embed_dim, dropout=dropout),\n        )\n        self.final_norm = nn.LayerNorm(embed_dim, eps=1e-6)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        x_lens: torch.Tensor,\n    ):\n        \"\"\"\n        Args:\n            x: shape (b, t, c)\n            x_lens: shape (b,)\n        \"\"\"\n        x = x.transpose(1, 2)\n        x = self.in_proj(x)\n        x = self.prior_net(x)\n        x = x.transpose(1, 2)\n\n        attention_mask = make_nonpad_mask(x_lens).unsqueeze(1)  # (b, 1, t)\n        # NOTE(sfy): I think positional embedding is unnecessary\n        for layer in self.transformers:\n            x = layer(x, attention_mask)\n        x = x.transpose(1, 2)\n        x = self.post_net(x)\n        x = x.transpose(1, 2)\n        x = self.final_norm(x)\n        return x"
  },
  {
    "id": "2d6b0f95b3a7bf2c27aa91a6c7ebdcd6572e05ea",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "function",
    "name": "forward",
    "signature": "forward(\n        self,\n        x: torch.Tensor,\n        x_lens: torch.Tensor,\n    )",
    "docstring": "Args:\n            x: shape (b, t, c)\n            x_lens: shape (b,)",
    "identifiers": [
      "attention_mask",
      "final_norm",
      "forward",
      "in_proj",
      "layer",
      "make_nonpad_mask",
      "post_net",
      "prior_net",
      "self",
      "tensor",
      "torch",
      "transformers",
      "transpose",
      "unsqueeze",
      "x",
      "x_lens"
    ],
    "start_line": 198,
    "end_line": 221,
    "text": "def forward(\n        self,\n        x: torch.Tensor,\n        x_lens: torch.Tensor,\n    ):\n        \"\"\"\n        Args:\n            x: shape (b, t, c)\n            x_lens: shape (b,)\n        \"\"\"\n        x = x.transpose(1, 2)\n        x = self.in_proj(x)\n        x = self.prior_net(x)\n        x = x.transpose(1, 2)\n\n        attention_mask = make_nonpad_mask(x_lens).unsqueeze(1)  # (b, 1, t)\n        # NOTE(sfy): I think positional embedding is unnecessary\n        for layer in self.transformers:\n            x = layer(x, attention_mask)\n        x = x.transpose(1, 2)\n        x = self.post_net(x)\n        x = x.transpose(1, 2)\n        x = self.final_norm(x)\n        return x"
  },
  {
    "id": "0fa3617f8c84dac2376f544d68c74624c4e2a20a",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "function",
    "name": "__init__",
    "signature": "__init__(\n        self,\n        embed_dim: int = 1024,\n        num_layers: int = 12,\n        num_heads: int = 16,\n        dropout: float = 0.1,\n    )",
    "docstring": "",
    "identifiers": [
      "_",
      "__init__",
      "conv1d",
      "dropout",
      "embed_dim",
      "eps",
      "final_norm",
      "float",
      "in_proj",
      "int",
      "kernel_size",
      "layernorm",
      "modulelist",
      "nn",
      "num_heads",
      "num_layers",
      "padding",
      "post_net",
      "prior_net",
      "range",
      "resnetblock",
      "self",
      "sequential",
      "super",
      "transformers",
      "whisperencoderlayer"
    ],
    "start_line": 176,
    "end_line": 196,
    "text": "def __init__(\n        self,\n        embed_dim: int = 1024,\n        num_layers: int = 12,\n        num_heads: int = 16,\n        dropout: float = 0.1,\n    ):\n        super().__init__()\n        self.in_proj = nn.Conv1d(embed_dim, embed_dim, kernel_size=7, padding=3)\n        self.prior_net = nn.Sequential(\n            ResnetBlock(embed_dim, embed_dim, dropout=dropout),\n            ResnetBlock(embed_dim, embed_dim, dropout=dropout),\n        )\n        self.transformers = nn.ModuleList(\n            [WhisperEncoderLayer(embed_dim, num_heads) for _ in range(num_layers)]\n        )\n        self.post_net = nn.Sequential(\n            ResnetBlock(embed_dim, embed_dim, dropout=dropout),\n            ResnetBlock(embed_dim, embed_dim, dropout=dropout),\n        )\n        self.final_norm = nn.LayerNorm(embed_dim, eps=1e-6)"
  },
  {
    "id": "c03e8287d4a95761b92f9d5b3a623adac5bde60f",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "class",
    "name": "CausalResnetBlock",
    "signature": "class CausalResnetBlock(nn.Module)",
    "docstring": "A causal variant of ResnetBlock",
    "identifiers": [
      "__init__",
      "block1",
      "block2",
      "cache",
      "cache1",
      "cache2",
      "cat",
      "causalconv1d",
      "causalresnetblock",
      "conv1d",
      "dim",
      "dropout",
      "float",
      "forward",
      "forward_chunk",
      "h",
      "in_channels",
      "int",
      "kernel_size",
      "layernorm",
      "module",
      "new_cache",
      "new_cache1",
      "new_cache2",
      "nin_shortcut",
      "nn",
      "out_channels",
      "padding",
      "self",
      "sequential",
      "silu",
      "split",
      "stride",
      "super",
      "tensor",
      "torch",
      "transpose",
      "x"
    ],
    "start_line": 105,
    "end_line": 171,
    "text": "class CausalResnetBlock(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int = None,\n        dropout: float = 0.0,\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        out_channels = in_channels if out_channels is None else out_channels\n        self.out_channels = out_channels\n\n        self.block1 = nn.Sequential(\n            Transpose(1, 2),\n            nn.LayerNorm(in_channels),\n            Transpose(1, 2),\n            nn.SiLU(),\n            CausalConv1d(in_channels, out_channels, kernel_size=3),\n        )\n\n        self.block2 = nn.Sequential(\n            Transpose(1, 2),\n            nn.LayerNorm(out_channels),\n            Transpose(1, 2),\n            nn.SiLU(),\n            nn.Dropout(dropout),\n            CausalConv1d(out_channels, out_channels, kernel_size=3),\n        )\n        if self.in_channels != self.out_channels:\n            self.nin_shortcut = torch.nn.Conv1d(\n                in_channels, out_channels, kernel_size=1, stride=1, padding=0\n            )\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        Args:\n            x: shape (b, c, t)\n        \"\"\"\n        h = x\n        h = self.block1(h)\n        h = self.block2(h)\n        if self.in_channels != self.out_channels:\n            x = self.nin_shortcut(x)\n        return x + h\n\n    def forward_chunk(self, x: torch.Tensor, cache: torch.Tensor = None):\n        \"\"\"\n        Args:\n            x: shape (b, c, t)\n            cache: shape (b, c_in+c_out, t=2)\n        \"\"\"\n        cache1, cache2 = (\n            (None, None)\n            if cache is None\n            else cache.split((self.in_channels, self.out_channels), dim=1)\n        )\n        h = x\n        # block1\n        h = self.block1[:4](h)\n        h, new_cache1 = self.block1[4].forward_chunk(h, cache1)\n        # block2\n        h = self.block2[:5](h)\n        h, new_cache2 = self.block2[5].forward_chunk(h, cache2)\n        if self.in_channels != self.out_channels:\n            x = self.nin_shortcut(x)\n        new_cache = torch.cat([new_cache1, new_cache2], dim=1)\n        return x + h, new_cache"
  },
  {
    "id": "211653064e6cb0a0ef347dee1eb25d350fe84198",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "function",
    "name": "forward_chunk",
    "signature": "forward_chunk(self, x: torch.Tensor, cache: torch.Tensor = None)",
    "docstring": "Args:\n            x: shape (b, c, t)\n            cache: shape (b, c_in+c_out, t=2)",
    "identifiers": [
      "block1",
      "block2",
      "cache",
      "cache1",
      "cache2",
      "cat",
      "dim",
      "forward_chunk",
      "h",
      "in_channels",
      "new_cache",
      "new_cache1",
      "new_cache2",
      "nin_shortcut",
      "out_channels",
      "self",
      "split",
      "tensor",
      "torch",
      "x"
    ],
    "start_line": 150,
    "end_line": 171,
    "text": "def forward_chunk(self, x: torch.Tensor, cache: torch.Tensor = None):\n        \"\"\"\n        Args:\n            x: shape (b, c, t)\n            cache: shape (b, c_in+c_out, t=2)\n        \"\"\"\n        cache1, cache2 = (\n            (None, None)\n            if cache is None\n            else cache.split((self.in_channels, self.out_channels), dim=1)\n        )\n        h = x\n        # block1\n        h = self.block1[:4](h)\n        h, new_cache1 = self.block1[4].forward_chunk(h, cache1)\n        # block2\n        h = self.block2[:5](h)\n        h, new_cache2 = self.block2[5].forward_chunk(h, cache2)\n        if self.in_channels != self.out_channels:\n            x = self.nin_shortcut(x)\n        new_cache = torch.cat([new_cache1, new_cache2], dim=1)\n        return x + h, new_cache"
  },
  {
    "id": "578240bcba75f57915e357d39e832d933ac63263",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "function",
    "name": "forward",
    "signature": "forward(self, x: torch.Tensor)",
    "docstring": "Args:\n            x: shape (b, c, t)",
    "identifiers": [
      "block1",
      "block2",
      "forward",
      "h",
      "in_channels",
      "nin_shortcut",
      "out_channels",
      "self",
      "tensor",
      "torch",
      "x"
    ],
    "start_line": 138,
    "end_line": 148,
    "text": "def forward(self, x: torch.Tensor):\n        \"\"\"\n        Args:\n            x: shape (b, c, t)\n        \"\"\"\n        h = x\n        h = self.block1(h)\n        h = self.block2(h)\n        if self.in_channels != self.out_channels:\n            x = self.nin_shortcut(x)\n        return x + h"
  },
  {
    "id": "e0e9039313caaa455016910cac6ce11732898e8a",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "function",
    "name": "__init__",
    "signature": "__init__(\n        self,\n        in_channels: int,\n        out_channels: int = None,\n        dropout: float = 0.0,\n    )",
    "docstring": "",
    "identifiers": [
      "__init__",
      "block1",
      "block2",
      "causalconv1d",
      "conv1d",
      "dropout",
      "float",
      "in_channels",
      "int",
      "kernel_size",
      "layernorm",
      "nin_shortcut",
      "nn",
      "out_channels",
      "padding",
      "self",
      "sequential",
      "silu",
      "stride",
      "super",
      "torch",
      "transpose"
    ],
    "start_line": 106,
    "end_line": 136,
    "text": "def __init__(\n        self,\n        in_channels: int,\n        out_channels: int = None,\n        dropout: float = 0.0,\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        out_channels = in_channels if out_channels is None else out_channels\n        self.out_channels = out_channels\n\n        self.block1 = nn.Sequential(\n            Transpose(1, 2),\n            nn.LayerNorm(in_channels),\n            Transpose(1, 2),\n            nn.SiLU(),\n            CausalConv1d(in_channels, out_channels, kernel_size=3),\n        )\n\n        self.block2 = nn.Sequential(\n            Transpose(1, 2),\n            nn.LayerNorm(out_channels),\n            Transpose(1, 2),\n            nn.SiLU(),\n            nn.Dropout(dropout),\n            CausalConv1d(out_channels, out_channels, kernel_size=3),\n        )\n        if self.in_channels != self.out_channels:\n            self.nin_shortcut = torch.nn.Conv1d(\n                in_channels, out_channels, kernel_size=1, stride=1, padding=0\n            )"
  },
  {
    "id": "509d4e3707888a358d2c9cb6a62db215f712a0f7",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "class",
    "name": "CausalConv1d",
    "signature": "class CausalConv1d(torch.nn.Conv1d)",
    "docstring": "A causal variant of Conv1d",
    "identifiers": [
      "__init__",
      "cat",
      "causal_padding",
      "causalconv1d",
      "cnn_cache",
      "conv1d",
      "dim",
      "f",
      "forward",
      "forward_chunk",
      "in_channels",
      "int",
      "kernel_size",
      "new_cnn_cache",
      "new_zeros",
      "nn",
      "out_channels",
      "pad",
      "self",
      "shape",
      "super",
      "tensor",
      "torch",
      "x"
    ],
    "start_line": 78,
    "end_line": 101,
    "text": "class CausalConv1d(torch.nn.Conv1d):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int,\n    ) -> None:\n        super(CausalConv1d, self).__init__(in_channels, out_channels, kernel_size)\n        self.causal_padding = (kernel_size - 1, 0)\n\n    def forward(self, x: torch.Tensor):\n        x = F.pad(x, self.causal_padding)\n        x = super(CausalConv1d, self).forward(x)\n        return x\n\n    def forward_chunk(self, x: torch.Tensor, cnn_cache: torch.Tensor = None):\n        if cnn_cache is None:\n            cnn_cache = x.new_zeros(\n                (x.shape[0], self.in_channels, self.causal_padding[0])\n            )\n        x = torch.cat([cnn_cache, x], dim=2)\n        new_cnn_cache = x[..., -self.causal_padding[0] :]\n        x = super(CausalConv1d, self).forward(x)\n        return x, new_cnn_cache"
  },
  {
    "id": "33c8424102f12e1bf3b0c5d54b7dcc9a0e3689c7",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "function",
    "name": "forward_chunk",
    "signature": "forward_chunk(self, x: torch.Tensor, cnn_cache: torch.Tensor = None)",
    "docstring": "",
    "identifiers": [
      "cat",
      "causal_padding",
      "causalconv1d",
      "cnn_cache",
      "dim",
      "forward",
      "forward_chunk",
      "in_channels",
      "new_cnn_cache",
      "new_zeros",
      "self",
      "shape",
      "super",
      "tensor",
      "torch",
      "x"
    ],
    "start_line": 93,
    "end_line": 101,
    "text": "def forward_chunk(self, x: torch.Tensor, cnn_cache: torch.Tensor = None):\n        if cnn_cache is None:\n            cnn_cache = x.new_zeros(\n                (x.shape[0], self.in_channels, self.causal_padding[0])\n            )\n        x = torch.cat([cnn_cache, x], dim=2)\n        new_cnn_cache = x[..., -self.causal_padding[0] :]\n        x = super(CausalConv1d, self).forward(x)\n        return x, new_cnn_cache"
  },
  {
    "id": "ffdbb2bcc0deaa801fc1bd1ed39d479e9d225186",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "function",
    "name": "forward",
    "signature": "forward(self, x: torch.Tensor)",
    "docstring": "",
    "identifiers": [
      "causal_padding",
      "causalconv1d",
      "f",
      "forward",
      "pad",
      "self",
      "super",
      "tensor",
      "torch",
      "x"
    ],
    "start_line": 88,
    "end_line": 91,
    "text": "def forward(self, x: torch.Tensor):\n        x = F.pad(x, self.causal_padding)\n        x = super(CausalConv1d, self).forward(x)\n        return x"
  },
  {
    "id": "d53d96ec9b317c5d44c9f3f3e5c6324a1eda7cc7",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "function",
    "name": "__init__",
    "signature": "__init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int,\n    )",
    "docstring": "",
    "identifiers": [
      "__init__",
      "causal_padding",
      "causalconv1d",
      "in_channels",
      "int",
      "kernel_size",
      "out_channels",
      "self",
      "super"
    ],
    "start_line": 79,
    "end_line": 86,
    "text": "def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int,\n    ) -> None:\n        super(CausalConv1d, self).__init__(in_channels, out_channels, kernel_size)\n        self.causal_padding = (kernel_size - 1, 0)"
  },
  {
    "id": "7c38fd6697196b89c6a89c77ed4e8e64c44a274e",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "class",
    "name": "Transpose",
    "signature": "class Transpose(torch.nn.Module)",
    "docstring": "",
    "identifiers": [
      "__init__",
      "dim0",
      "dim1",
      "forward",
      "int",
      "module",
      "nn",
      "self",
      "super",
      "tensor",
      "torch",
      "transpose",
      "x"
    ],
    "start_line": 66,
    "end_line": 74,
    "text": "class Transpose(torch.nn.Module):\n    def __init__(self, dim0: int, dim1: int):\n        super().__init__()\n        self.dim0 = dim0\n        self.dim1 = dim1\n\n    def forward(self, x: torch.Tensor):\n        x = torch.transpose(x, self.dim0, self.dim1)\n        return x"
  },
  {
    "id": "f7da6fd816ac7b4586e317196d1c5b2d204604b5",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "function",
    "name": "forward",
    "signature": "forward(self, x: torch.Tensor)",
    "docstring": "",
    "identifiers": [
      "dim0",
      "dim1",
      "forward",
      "self",
      "tensor",
      "torch",
      "transpose",
      "x"
    ],
    "start_line": 72,
    "end_line": 74,
    "text": "def forward(self, x: torch.Tensor):\n        x = torch.transpose(x, self.dim0, self.dim1)\n        return x"
  },
  {
    "id": "bb2fd6868506bf07ff0591a317caff4c3f71c36b",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "function",
    "name": "__init__",
    "signature": "__init__(self, dim0: int, dim1: int)",
    "docstring": "",
    "identifiers": [
      "__init__",
      "dim0",
      "dim1",
      "int",
      "self",
      "super"
    ],
    "start_line": 67,
    "end_line": 70,
    "text": "def __init__(self, dim0: int, dim1: int):\n        super().__init__()\n        self.dim0 = dim0\n        self.dim1 = dim1"
  },
  {
    "id": "6b66043268c99545b3062246f801439f5bae3d0c",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "class",
    "name": "ResnetBlock",
    "signature": "class ResnetBlock(nn.Module)",
    "docstring": "",
    "identifiers": [
      "__init__",
      "affine",
      "block1",
      "block2",
      "bool",
      "conv1d",
      "conv_shortcut",
      "dropout",
      "eps",
      "float",
      "forward",
      "groupnorm",
      "h",
      "in_channels",
      "int",
      "kernel_size",
      "module",
      "nin_shortcut",
      "nn",
      "num_channels",
      "num_groups",
      "out_channels",
      "padding",
      "resnetblock",
      "self",
      "sequential",
      "silu",
      "stride",
      "super",
      "tensor",
      "torch",
      "use_conv_shortcut",
      "x"
    ],
    "start_line": 8,
    "end_line": 63,
    "text": "class ResnetBlock(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int = None,\n        conv_shortcut: bool = False,\n        dropout: float = 0.0,\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        out_channels = in_channels if out_channels is None else out_channels\n        self.out_channels = out_channels\n        self.use_conv_shortcut = conv_shortcut\n\n        self.block1 = nn.Sequential(\n            nn.GroupNorm(\n                num_groups=32, num_channels=in_channels, eps=1e-6, affine=True\n            ),\n            nn.SiLU(),\n            nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n        )\n\n        self.block2 = nn.Sequential(\n            nn.GroupNorm(\n                num_groups=32, num_channels=out_channels, eps=1e-6, affine=True\n            ),\n            nn.SiLU(),\n            nn.Dropout(dropout),\n            nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n        )\n\n        if self.in_channels != self.out_channels:\n            if self.use_conv_shortcut:\n                self.conv_shortcut = torch.nn.Conv1d(\n                    in_channels, out_channels, kernel_size=3, stride=1, padding=1\n                )\n            else:\n                self.nin_shortcut = torch.nn.Conv1d(\n                    in_channels, out_channels, kernel_size=1, stride=1, padding=0\n                )\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        Args:\n            x: shape (b, c, t)\n        \"\"\"\n        h = x\n        h = self.block1(h)\n        h = self.block2(h)\n\n        if self.in_channels != self.out_channels:\n            if self.use_conv_shortcut:\n                x = self.conv_shortcut(x)\n            else:\n                x = self.nin_shortcut(x)\n        return x + h"
  },
  {
    "id": "dc7a898d17cfbb9febbf1715c02ebeb9fa6dd960",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "function",
    "name": "forward",
    "signature": "forward(self, x: torch.Tensor)",
    "docstring": "Args:\n            x: shape (b, c, t)",
    "identifiers": [
      "block1",
      "block2",
      "conv_shortcut",
      "forward",
      "h",
      "in_channels",
      "nin_shortcut",
      "out_channels",
      "self",
      "tensor",
      "torch",
      "use_conv_shortcut",
      "x"
    ],
    "start_line": 49,
    "end_line": 63,
    "text": "def forward(self, x: torch.Tensor):\n        \"\"\"\n        Args:\n            x: shape (b, c, t)\n        \"\"\"\n        h = x\n        h = self.block1(h)\n        h = self.block2(h)\n\n        if self.in_channels != self.out_channels:\n            if self.use_conv_shortcut:\n                x = self.conv_shortcut(x)\n            else:\n                x = self.nin_shortcut(x)\n        return x + h"
  },
  {
    "id": "1f4c5dfe2c8a00de601ae3a4087fe5e1e24def74",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/decoder.py",
    "kind": "function",
    "name": "__init__",
    "signature": "__init__(\n        self,\n        in_channels: int,\n        out_channels: int = None,\n        conv_shortcut: bool = False,\n        dropout: float = 0.0,\n    )",
    "docstring": "",
    "identifiers": [
      "__init__",
      "affine",
      "block1",
      "block2",
      "bool",
      "conv1d",
      "conv_shortcut",
      "dropout",
      "eps",
      "float",
      "groupnorm",
      "in_channels",
      "int",
      "kernel_size",
      "nin_shortcut",
      "nn",
      "num_channels",
      "num_groups",
      "out_channels",
      "padding",
      "self",
      "sequential",
      "silu",
      "stride",
      "super",
      "torch",
      "use_conv_shortcut"
    ],
    "start_line": 9,
    "end_line": 47,
    "text": "def __init__(\n        self,\n        in_channels: int,\n        out_channels: int = None,\n        conv_shortcut: bool = False,\n        dropout: float = 0.0,\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        out_channels = in_channels if out_channels is None else out_channels\n        self.out_channels = out_channels\n        self.use_conv_shortcut = conv_shortcut\n\n        self.block1 = nn.Sequential(\n            nn.GroupNorm(\n                num_groups=32, num_channels=in_channels, eps=1e-6, affine=True\n            ),\n            nn.SiLU(),\n            nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n        )\n\n        self.block2 = nn.Sequential(\n            nn.GroupNorm(\n                num_groups=32, num_channels=out_channels, eps=1e-6, affine=True\n            ),\n            nn.SiLU(),\n            nn.Dropout(dropout),\n            nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n        )\n\n        if self.in_channels != self.out_channels:\n            if self.use_conv_shortcut:\n                self.conv_shortcut = torch.nn.Conv1d(\n                    in_channels, out_channels, kernel_size=3, stride=1, padding=1\n                )\n            else:\n                self.nin_shortcut = torch.nn.Conv1d(\n                    in_channels, out_channels, kernel_size=1, stride=1, padding=0\n                )"
  },
  {
    "id": "ab6f5f04aa8223055609f397b5761cb65706b62b",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/utils.py",
    "kind": "function",
    "name": "make_block_causal_mask",
    "signature": "make_block_causal_mask(\n    lengths: torch.Tensor, max_len: int = 0, chunk_size: int = 4\n)",
    "docstring": "",
    "identifiers": [
      "_",
      "attn_mask",
      "block_diag",
      "block_mask",
      "ceil",
      "chunk_size",
      "diag_mask",
      "fill_value",
      "int",
      "lengths",
      "logical_and",
      "logical_or",
      "make_block_causal_mask",
      "make_nonpad_mask",
      "mask",
      "math",
      "max_len",
      "new_full",
      "num_blocks",
      "ones",
      "range",
      "shape",
      "tensor",
      "to",
      "torch",
      "tril",
      "unsqueeze"
    ],
    "start_line": 19,
    "end_line": 38,
    "text": "def make_block_causal_mask(\n    lengths: torch.Tensor, max_len: int = 0, chunk_size: int = 4\n) -> torch.Tensor:\n    mask = make_nonpad_mask(lengths, max_len)  # (b, t)\n    attn_mask = torch.logical_and(mask.unsqueeze(1), mask.unsqueeze(2))  # (b, t, t)\n\n    num_blocks = math.ceil(attn_mask.shape[1] / chunk_size)\n    block_mask = torch.block_diag(\n        *[torch.ones(chunk_size, chunk_size) for _ in range(num_blocks)]\n    )\n    block_mask = block_mask[: attn_mask.shape[1], : attn_mask.shape[1]].to(\n        attn_mask\n    )  # (t, t)\n\n    diag_mask = attn_mask.new_full(\n        (1, attn_mask.shape[1], attn_mask.shape[2]), fill_value=True\n    ).tril()  # (1, t, t)\n    diag_mask = diag_mask.logical_or(block_mask)\n    attn_mask = attn_mask.logical_and(diag_mask)\n    return attn_mask"
  },
  {
    "id": "bfda720de90a0c239ea02ad25272a4923fb8574d",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/utils.py",
    "kind": "function",
    "name": "make_nonpad_mask",
    "signature": "make_nonpad_mask(lengths: torch.Tensor, max_len: int = 0)",
    "docstring": "",
    "identifiers": [
      "int",
      "lengths",
      "make_nonpad_mask",
      "make_pad_mask",
      "max_len",
      "tensor",
      "torch"
    ],
    "start_line": 15,
    "end_line": 16,
    "text": "def make_nonpad_mask(lengths: torch.Tensor, max_len: int = 0) -> torch.Tensor:\n    return ~make_pad_mask(lengths, max_len)"
  },
  {
    "id": "0f12f42f9edd13c72cb15d025d17a1e983041e96",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/utils.py",
    "kind": "function",
    "name": "make_pad_mask",
    "signature": "make_pad_mask(lengths: torch.Tensor, max_len: int = 0)",
    "docstring": "",
    "identifiers": [
      "arange",
      "batch_size",
      "device",
      "dtype",
      "expand",
      "int",
      "int64",
      "item",
      "lengths",
      "make_pad_mask",
      "mask",
      "max",
      "max_len",
      "seq_length_expand",
      "seq_range",
      "seq_range_expand",
      "size",
      "tensor",
      "torch",
      "unsqueeze"
    ],
    "start_line": 5,
    "end_line": 12,
    "text": "def make_pad_mask(lengths: torch.Tensor, max_len: int = 0) -> torch.Tensor:\n    batch_size = lengths.size(0)\n    max_len = max_len if max_len > 0 else lengths.max().item()\n    seq_range = torch.arange(0, max_len, dtype=torch.int64, device=lengths.device)\n    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n    seq_length_expand = lengths.unsqueeze(-1)\n    mask = seq_range_expand >= seq_length_expand\n    return mask  # (b, t)"
  },
  {
    "id": "f9e4dafa0aa8671eeb36d34b9beac4a7ba0a14cc",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/whisper.py",
    "kind": "class",
    "name": "WhisperAcousticEncoder",
    "signature": "class WhisperAcousticEncoder(WhisperEncoder)",
    "docstring": "",
    "identifiers": [
      "__init__",
      "apply_position",
      "attn_dropout",
      "audio16k",
      "audio16k_length",
      "dropout",
      "embed_dim",
      "feature_extractor",
      "ffn_dim",
      "float",
      "fmax",
      "fmin",
      "forward",
      "hidden_length",
      "hidden_states",
      "hop_length",
      "in_dim",
      "int",
      "max_positions",
      "mel",
      "mel_length",
      "n_fft",
      "no_grad",
      "num_heads",
      "num_layers",
      "num_mels",
      "sampling_rate",
      "self",
      "super",
      "tensor",
      "torch",
      "whisperacousticencoder",
      "whisperencoder",
      "whispermelextractor"
    ],
    "start_line": 374,
    "end_line": 420,
    "text": "class WhisperAcousticEncoder(WhisperEncoder):\n    def __init__(\n        self,\n        # Mel extraction params\n        num_mels: int = 128,\n        sampling_rate: int = 16000,\n        hop_length: int = 160,\n        n_fft: int = 400,\n        fmin: float = 0.0,\n        fmax: float = 8000,\n        # Encoder params\n        embed_dim: int = 768,\n        num_layers: int = 12,\n        num_heads: int = 8,\n        ffn_dim: int = None,\n        attn_dropout: float = 0.0,\n        dropout: float = 0.0,\n        max_positions: int = 1500,  # 50Hz * 30s\n    ):\n        super().__init__(\n            in_dim=num_mels,\n            embed_dim=embed_dim,\n            num_layers=num_layers,\n            num_heads=num_heads,\n            ffn_dim=ffn_dim,\n            attn_dropout=attn_dropout,\n            dropout=dropout,\n            max_positions=max_positions,\n        )\n        self.feature_extractor = WhisperMelExtractor(\n            num_mels=num_mels,\n            sampling_rate=sampling_rate,\n            hop_length=hop_length,\n            n_fft=n_fft,\n            fmin=fmin,\n            fmax=fmax,\n        )\n\n    def forward(self, audio16k: torch.Tensor, audio16k_length: torch.Tensor):\n        # Extract mel\n        with torch.no_grad():\n            mel, mel_length = self.feature_extractor(audio16k, audio16k_length)\n        # Forward model\n        hidden_states, hidden_length = super().forward(\n            mel, mel_length, apply_position=True\n        )\n        return hidden_states, hidden_length"
  },
  {
    "id": "f266c9c94014524f4aedd44a35b873681d4f4506",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/whisper.py",
    "kind": "function",
    "name": "forward",
    "signature": "forward(self, audio16k: torch.Tensor, audio16k_length: torch.Tensor)",
    "docstring": "",
    "identifiers": [
      "apply_position",
      "audio16k",
      "audio16k_length",
      "feature_extractor",
      "forward",
      "hidden_length",
      "hidden_states",
      "mel",
      "mel_length",
      "no_grad",
      "self",
      "super",
      "tensor",
      "torch"
    ],
    "start_line": 412,
    "end_line": 420,
    "text": "def forward(self, audio16k: torch.Tensor, audio16k_length: torch.Tensor):\n        # Extract mel\n        with torch.no_grad():\n            mel, mel_length = self.feature_extractor(audio16k, audio16k_length)\n        # Forward model\n        hidden_states, hidden_length = super().forward(\n            mel, mel_length, apply_position=True\n        )\n        return hidden_states, hidden_length"
  },
  {
    "id": "047bf2f75f68f6a89e771241b3233cefd7b48016",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/whisper.py",
    "kind": "function",
    "name": "__init__",
    "signature": "__init__(\n        self,\n        # Mel extraction params\n        num_mels: int = 128,\n        sampling_rate: int = 16000,\n        hop_length: int = 160,\n        n_fft: int = 400,\n        fmin: float = 0.0,\n        fmax: float = 8000,\n        # Encoder params\n        embed_dim: int = 768,\n        num_layers: int = 12,\n        num_heads: int = 8,\n        ffn_dim: int = None,\n        attn_dropout: float = 0.0,\n        dropout: float = 0.0,\n        max_positions: int = 1500,  # 50Hz * 30s\n    )",
    "docstring": "",
    "identifiers": [
      "__init__",
      "attn_dropout",
      "dropout",
      "embed_dim",
      "feature_extractor",
      "ffn_dim",
      "float",
      "fmax",
      "fmin",
      "hop_length",
      "in_dim",
      "int",
      "max_positions",
      "n_fft",
      "num_heads",
      "num_layers",
      "num_mels",
      "sampling_rate",
      "self",
      "super",
      "whispermelextractor"
    ],
    "start_line": 375,
    "end_line": 410,
    "text": "def __init__(\n        self,\n        # Mel extraction params\n        num_mels: int = 128,\n        sampling_rate: int = 16000,\n        hop_length: int = 160,\n        n_fft: int = 400,\n        fmin: float = 0.0,\n        fmax: float = 8000,\n        # Encoder params\n        embed_dim: int = 768,\n        num_layers: int = 12,\n        num_heads: int = 8,\n        ffn_dim: int = None,\n        attn_dropout: float = 0.0,\n        dropout: float = 0.0,\n        max_positions: int = 1500,  # 50Hz * 30s\n    ):\n        super().__init__(\n            in_dim=num_mels,\n            embed_dim=embed_dim,\n            num_layers=num_layers,\n            num_heads=num_heads,\n            ffn_dim=ffn_dim,\n            attn_dropout=attn_dropout,\n            dropout=dropout,\n            max_positions=max_positions,\n        )\n        self.feature_extractor = WhisperMelExtractor(\n            num_mels=num_mels,\n            sampling_rate=sampling_rate,\n            hop_length=hop_length,\n            n_fft=n_fft,\n            fmin=fmin,\n            fmax=fmax,\n        )"
  },
  {
    "id": "558503717a90093eb69d872cc3cb8b0a9ecd9921",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/whisper.py",
    "kind": "class",
    "name": "PretrainedWhisperEncoder",
    "signature": "class PretrainedWhisperEncoder(WhisperEncoder)",
    "docstring": "Pretrained encoder from whisper-large-v3",
    "identifiers": [
      "apply_position",
      "attn_dropout",
      "audio16k",
      "audio16k_length",
      "ckpt",
      "classmethod",
      "cls",
      "embed_dim",
      "encoder",
      "eval",
      "feature_extractor",
      "ffn_dim",
      "fmax",
      "fmin",
      "forward",
      "from_pretrained",
      "hop_length",
      "in_dim",
      "inference_mode",
      "load",
      "load_state_dict",
      "map_location",
      "max_positions",
      "mel",
      "mel_length",
      "n_fft",
      "num_heads",
      "num_layers",
      "num_mels",
      "p",
      "parameters",
      "pretrained_path",
      "pretrainedwhisperencoder",
      "requires_grad_",
      "sampling_rate",
      "self",
      "semantic_feats",
      "semantic_length",
      "str",
      "super",
      "tensor",
      "torch",
      "whisperencoder",
      "whispermelextractor"
    ],
    "start_line": 333,
    "end_line": 371,
    "text": "class PretrainedWhisperEncoder(WhisperEncoder):\n    @classmethod\n    def from_pretrained(cls, pretrained_path: str = None):\n        encoder = cls(\n            in_dim=128,\n            embed_dim=1280,\n            num_layers=32,\n            num_heads=20,\n            ffn_dim=5120,\n            attn_dropout=0.0,\n            max_positions=1500,\n        )\n        if pretrained_path is not None:\n            ckpt = torch.load(pretrained_path, map_location=\"cpu\")\n            encoder.load_state_dict(ckpt)\n        encoder.eval()\n        # Disable grad\n        for p in encoder.parameters():\n            p.requires_grad_(False)\n        # Add Mel extractor\n        encoder.feature_extractor = WhisperMelExtractor(\n            num_mels=128,\n            sampling_rate=16000,\n            hop_length=160,\n            n_fft=400,\n            fmin=0,\n            fmax=8000,\n        )\n        return encoder\n\n    @torch.inference_mode()\n    def forward(self, audio16k: torch.Tensor, audio16k_length: torch.Tensor):\n        # Extract mel\n        mel, mel_length = self.feature_extractor(audio16k, audio16k_length)\n        # Forward model\n        semantic_feats, semantic_length = super().forward(\n            mel, mel_length, apply_position=True\n        )\n        return semantic_feats, semantic_length"
  },
  {
    "id": "dbec98a561cac4d93d50c96a58595996ad27279a",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/whisper.py",
    "kind": "function",
    "name": "forward",
    "signature": "forward(self, audio16k: torch.Tensor, audio16k_length: torch.Tensor)",
    "docstring": "",
    "identifiers": [
      "apply_position",
      "audio16k",
      "audio16k_length",
      "feature_extractor",
      "forward",
      "mel",
      "mel_length",
      "self",
      "semantic_feats",
      "semantic_length",
      "super",
      "tensor",
      "torch"
    ],
    "start_line": 364,
    "end_line": 371,
    "text": "def forward(self, audio16k: torch.Tensor, audio16k_length: torch.Tensor):\n        # Extract mel\n        mel, mel_length = self.feature_extractor(audio16k, audio16k_length)\n        # Forward model\n        semantic_feats, semantic_length = super().forward(\n            mel, mel_length, apply_position=True\n        )\n        return semantic_feats, semantic_length"
  },
  {
    "id": "65ba14174149f7d675e79f2ff5b9da092e268fd9",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/whisper.py",
    "kind": "function",
    "name": "from_pretrained",
    "signature": "from_pretrained(cls, pretrained_path: str = None)",
    "docstring": "",
    "identifiers": [
      "attn_dropout",
      "ckpt",
      "cls",
      "embed_dim",
      "encoder",
      "eval",
      "feature_extractor",
      "ffn_dim",
      "fmax",
      "fmin",
      "from_pretrained",
      "hop_length",
      "in_dim",
      "load",
      "load_state_dict",
      "map_location",
      "max_positions",
      "n_fft",
      "num_heads",
      "num_layers",
      "num_mels",
      "p",
      "parameters",
      "pretrained_path",
      "requires_grad_",
      "sampling_rate",
      "str",
      "torch",
      "whispermelextractor"
    ],
    "start_line": 335,
    "end_line": 361,
    "text": "def from_pretrained(cls, pretrained_path: str = None):\n        encoder = cls(\n            in_dim=128,\n            embed_dim=1280,\n            num_layers=32,\n            num_heads=20,\n            ffn_dim=5120,\n            attn_dropout=0.0,\n            max_positions=1500,\n        )\n        if pretrained_path is not None:\n            ckpt = torch.load(pretrained_path, map_location=\"cpu\")\n            encoder.load_state_dict(ckpt)\n        encoder.eval()\n        # Disable grad\n        for p in encoder.parameters():\n            p.requires_grad_(False)\n        # Add Mel extractor\n        encoder.feature_extractor = WhisperMelExtractor(\n            num_mels=128,\n            sampling_rate=16000,\n            hop_length=160,\n            n_fft=400,\n            fmin=0,\n            fmax=8000,\n        )\n        return encoder"
  },
  {
    "id": "0edb2dae6f9c5bd45a24b3a1192e59b73ecff797",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/whisper.py",
    "kind": "class",
    "name": "WhisperMelExtractor",
    "signature": "class WhisperMelExtractor(nn.Module)",
    "docstring": "",
    "identifiers": [
      "__call__",
      "__init__",
      "abs",
      "audio",
      "audio16k",
      "audio16k_length",
      "clamp",
      "device",
      "dim",
      "extract_fbank",
      "float",
      "float32",
      "fmax",
      "fmin",
      "from_numpy",
      "hann_window",
      "hop_length",
      "int",
      "keepdim",
      "log10",
      "log_spec",
      "magnitudes",
      "max",
      "max_frequency",
      "max_val",
      "maximum",
      "mel",
      "mel_filter_bank",
      "mel_filters",
      "mel_length",
      "mel_scale",
      "mel_spec",
      "min",
      "min_frequency",
      "module",
      "n_fft",
      "nn",
      "norm",
      "num_frequency_bins",
      "num_mel_filters",
      "num_mels",
      "padding_value",
      "return_complex",
      "sampling_rate",
      "self",
      "stft",
      "super",
      "t",
      "tensor",
      "to",
      "torch",
      "transpose",
      "type",
      "whispermelextractor",
      "window"
    ],
    "start_line": 275,
    "end_line": 329,
    "text": "class WhisperMelExtractor(nn.Module):\n    def __init__(\n        self,\n        num_mels: int = 128,\n        sampling_rate: int = 16000,\n        hop_length: int = 160,\n        n_fft: int = 400,\n        fmin: float = 0,\n        fmax: float = 8000,\n        padding_value=0.0,\n    ):\n        super().__init__()\n        self.num_mels = num_mels\n        self.sampling_rate = sampling_rate\n        self.hop_length = hop_length\n        self.n_fft = n_fft\n        self.fmin = fmin\n        self.fmax = fmax\n        self.padding_value = padding_value\n        self.mel_filters = mel_filter_bank(\n            num_frequency_bins=(1 + n_fft // 2),\n            num_mel_filters=num_mels,\n            min_frequency=fmin,\n            max_frequency=fmax,\n            sampling_rate=sampling_rate,\n            norm=\"slaney\",\n            mel_scale=\"slaney\",\n        )\n\n    def extract_fbank(self, audio: torch.Tensor):\n        \"\"\"\n        Args:\n            audio: batched audio of shape (b, t)\n        \"\"\"\n        device = audio.device  # compute on cuda if input is on cuda\n        # Mel\n        window = torch.hann_window(self.n_fft).to(device)\n        stft = torch.stft(\n            audio, self.n_fft, self.hop_length, window=window, return_complex=True\n        )\n        magnitudes = stft[..., :-1].abs() ** 2\n        mel_filters = torch.from_numpy(self.mel_filters).type(torch.float32).to(device)\n        mel_spec = mel_filters.T @ magnitudes\n        log_spec = torch.clamp(mel_spec, min=1e-10).log10()\n        # Norm\n        max_val = log_spec.max(dim=2, keepdim=True)[0].max(dim=1, keepdim=True)[0]\n        log_spec = torch.maximum(log_spec, max_val - 8.0)\n        log_spec = (log_spec + 4.0) / 4.0\n        return log_spec\n\n    def __call__(self, audio16k: torch.Tensor, audio16k_length: torch.Tensor):\n        mel = self.extract_fbank(audio16k).transpose(1, 2)\n        mel_length = audio16k_length // self.hop_length\n        # mel: (b, t, c=128)\n        return mel, mel_length"
  },
  {
    "id": "e8a9bba9358ec8cd7cbce3e760644a8d1fe73028",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/whisper.py",
    "kind": "function",
    "name": "__call__",
    "signature": "__call__(self, audio16k: torch.Tensor, audio16k_length: torch.Tensor)",
    "docstring": "",
    "identifiers": [
      "__call__",
      "audio16k",
      "audio16k_length",
      "extract_fbank",
      "hop_length",
      "mel",
      "mel_length",
      "self",
      "tensor",
      "torch",
      "transpose"
    ],
    "start_line": 325,
    "end_line": 329,
    "text": "def __call__(self, audio16k: torch.Tensor, audio16k_length: torch.Tensor):\n        mel = self.extract_fbank(audio16k).transpose(1, 2)\n        mel_length = audio16k_length // self.hop_length\n        # mel: (b, t, c=128)\n        return mel, mel_length"
  },
  {
    "id": "a02f245c977cb8a23c2b026943ad3e5b6fe65000",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/whisper.py",
    "kind": "function",
    "name": "extract_fbank",
    "signature": "extract_fbank(self, audio: torch.Tensor)",
    "docstring": "Args:\n            audio: batched audio of shape (b, t)",
    "identifiers": [
      "abs",
      "audio",
      "clamp",
      "device",
      "dim",
      "extract_fbank",
      "float32",
      "from_numpy",
      "hann_window",
      "hop_length",
      "keepdim",
      "log10",
      "log_spec",
      "magnitudes",
      "max",
      "max_val",
      "maximum",
      "mel_filters",
      "mel_spec",
      "min",
      "n_fft",
      "return_complex",
      "self",
      "stft",
      "t",
      "tensor",
      "to",
      "torch",
      "type",
      "window"
    ],
    "start_line": 304,
    "end_line": 323,
    "text": "def extract_fbank(self, audio: torch.Tensor):\n        \"\"\"\n        Args:\n            audio: batched audio of shape (b, t)\n        \"\"\"\n        device = audio.device  # compute on cuda if input is on cuda\n        # Mel\n        window = torch.hann_window(self.n_fft).to(device)\n        stft = torch.stft(\n            audio, self.n_fft, self.hop_length, window=window, return_complex=True\n        )\n        magnitudes = stft[..., :-1].abs() ** 2\n        mel_filters = torch.from_numpy(self.mel_filters).type(torch.float32).to(device)\n        mel_spec = mel_filters.T @ magnitudes\n        log_spec = torch.clamp(mel_spec, min=1e-10).log10()\n        # Norm\n        max_val = log_spec.max(dim=2, keepdim=True)[0].max(dim=1, keepdim=True)[0]\n        log_spec = torch.maximum(log_spec, max_val - 8.0)\n        log_spec = (log_spec + 4.0) / 4.0\n        return log_spec"
  },
  {
    "id": "4b55ff12cb2c731ee82ce2ef059572322e53d68f",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/whisper.py",
    "kind": "function",
    "name": "__init__",
    "signature": "__init__(\n        self,\n        num_mels: int = 128,\n        sampling_rate: int = 16000,\n        hop_length: int = 160,\n        n_fft: int = 400,\n        fmin: float = 0,\n        fmax: float = 8000,\n        padding_value=0.0,\n    )",
    "docstring": "",
    "identifiers": [
      "__init__",
      "float",
      "fmax",
      "fmin",
      "hop_length",
      "int",
      "max_frequency",
      "mel_filter_bank",
      "mel_filters",
      "mel_scale",
      "min_frequency",
      "n_fft",
      "norm",
      "num_frequency_bins",
      "num_mel_filters",
      "num_mels",
      "padding_value",
      "sampling_rate",
      "self",
      "super"
    ],
    "start_line": 276,
    "end_line": 302,
    "text": "def __init__(\n        self,\n        num_mels: int = 128,\n        sampling_rate: int = 16000,\n        hop_length: int = 160,\n        n_fft: int = 400,\n        fmin: float = 0,\n        fmax: float = 8000,\n        padding_value=0.0,\n    ):\n        super().__init__()\n        self.num_mels = num_mels\n        self.sampling_rate = sampling_rate\n        self.hop_length = hop_length\n        self.n_fft = n_fft\n        self.fmin = fmin\n        self.fmax = fmax\n        self.padding_value = padding_value\n        self.mel_filters = mel_filter_bank(\n            num_frequency_bins=(1 + n_fft // 2),\n            num_mel_filters=num_mels,\n            min_frequency=fmin,\n            max_frequency=fmax,\n            sampling_rate=sampling_rate,\n            norm=\"slaney\",\n            mel_scale=\"slaney\",\n        )"
  },
  {
    "id": "909ed2176abadf6c1a6b38099c53847689068ab2",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/whisper.py",
    "kind": "class",
    "name": "WhisperEncoder",
    "signature": "class WhisperEncoder(nn.Module)",
    "docstring": "",
    "identifiers": [
      "_",
      "__init__",
      "_init_weights",
      "apply",
      "apply_position",
      "arange",
      "attention_mask",
      "attn_dropout",
      "bias",
      "bool",
      "conv1",
      "conv1d",
      "conv2",
      "copy_",
      "data",
      "device",
      "dropout",
      "embed_dim",
      "embed_positions",
      "embedding",
      "f",
      "ffn_dim",
      "float",
      "forward",
      "functional",
      "gelu",
      "hidden_length",
      "hidden_states",
      "in_dim",
      "int",
      "isinstance",
      "kernel_size",
      "layer",
      "layer_norm",
      "layernorm",
      "layers",
      "linear",
      "make_nonpad_mask",
      "max_positions",
      "mean",
      "module",
      "modulelist",
      "nn",
      "normal_",
      "num_heads",
      "num_layers",
      "p",
      "padding",
      "padding_idx",
      "pos_embed",
      "range",
      "requires_grad_",
      "self",
      "shape",
      "sinusoids",
      "std",
      "stride",
      "super",
      "tensor",
      "torch",
      "training",
      "transpose",
      "unsqueeze",
      "weight",
      "whisperencoder",
      "whisperencoderlayer",
      "zero_"
    ],
    "start_line": 195,
    "end_line": 272,
    "text": "class WhisperEncoder(nn.Module):\n    def __init__(\n        self,\n        in_dim: int,\n        embed_dim: int,\n        num_layers: int,\n        num_heads: int,\n        ffn_dim: int = None,\n        attn_dropout: float = 0.0,\n        dropout: float = 0.0,\n        max_positions: int = 1500,\n    ):\n        super().__init__()\n        self.in_dim = in_dim\n        self.embed_dim = embed_dim\n        self.dropout = dropout\n        # Input downsampling\n        self.conv1 = nn.Conv1d(in_dim, embed_dim, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, stride=2, padding=1)\n        # Fixed positional embedding\n        self.max_positions = max_positions\n        self.embed_positions = nn.Embedding(self.max_positions, embed_dim)\n        self.embed_positions.requires_grad_(False)\n        # Transformer\n        self.layers = nn.ModuleList(\n            [\n                WhisperEncoderLayer(\n                    embed_dim, num_heads, ffn_dim, attn_dropout, dropout\n                )\n                for _ in range(num_layers)\n            ]\n        )\n        # Output norm\n        self.layer_norm = nn.LayerNorm(embed_dim)\n        # Init weight\n        self.apply(self._init_weights)\n        # Init position embedding\n        self.embed_positions.weight.copy_(sinusoids(*self.embed_positions.weight.shape))\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        hidden_length: torch.Tensor,\n        apply_position: bool = True,\n    ):\n        # Downsampling\n        hidden_states = hidden_states.transpose(1, 2)\n        hidden_states = F.gelu(self.conv1(hidden_states))\n        hidden_states = F.gelu(self.conv2(hidden_states))\n        hidden_states = hidden_states.transpose(1, 2)\n        hidden_length = hidden_length // 2  # from 100Hz -> 50Hz\n        # Pos encoding\n        if apply_position:\n            pos_embed = self.embed_positions(\n                torch.arange(0, hidden_states.shape[1], device=hidden_states.device)\n            )\n            hidden_states = hidden_states + pos_embed\n        hidden_states = nn.functional.dropout(\n            hidden_states, p=self.dropout, training=self.training\n        )\n        # Transformer\n        attention_mask = make_nonpad_mask(hidden_length).unsqueeze(1)  # (b, 1, t)\n        for layer in self.layers:\n            hidden_states = layer(hidden_states, attention_mask)\n\n        hidden_states = self.layer_norm(hidden_states)\n        return hidden_states, hidden_length\n\n    def _init_weights(self, module):\n        std = 0.02\n        if isinstance(module, (nn.Linear, nn.Conv1d)):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()"
  },
  {
    "id": "48f40f12643f3628a44764c66028ef9b3d2779e4",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/whisper.py",
    "kind": "function",
    "name": "_init_weights",
    "signature": "_init_weights(self, module)",
    "docstring": "",
    "identifiers": [
      "_init_weights",
      "bias",
      "conv1d",
      "data",
      "embedding",
      "isinstance",
      "linear",
      "mean",
      "module",
      "nn",
      "normal_",
      "padding_idx",
      "self",
      "std",
      "weight",
      "zero_"
    ],
    "start_line": 263,
    "end_line": 272,
    "text": "def _init_weights(self, module):\n        std = 0.02\n        if isinstance(module, (nn.Linear, nn.Conv1d)):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()"
  },
  {
    "id": "f67e1eacb7d34f25e0ba36646c77fd260033e3eb",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/whisper.py",
    "kind": "function",
    "name": "forward",
    "signature": "forward(\n        self,\n        hidden_states: torch.Tensor,\n        hidden_length: torch.Tensor,\n        apply_position: bool = True,\n    )",
    "docstring": "",
    "identifiers": [
      "apply_position",
      "arange",
      "attention_mask",
      "bool",
      "conv1",
      "conv2",
      "device",
      "dropout",
      "embed_positions",
      "f",
      "forward",
      "functional",
      "gelu",
      "hidden_length",
      "hidden_states",
      "layer",
      "layer_norm",
      "layers",
      "make_nonpad_mask",
      "nn",
      "p",
      "pos_embed",
      "self",
      "shape",
      "tensor",
      "torch",
      "training",
      "transpose",
      "unsqueeze"
    ],
    "start_line": 234,
    "end_line": 261,
    "text": "def forward(\n        self,\n        hidden_states: torch.Tensor,\n        hidden_length: torch.Tensor,\n        apply_position: bool = True,\n    ):\n        # Downsampling\n        hidden_states = hidden_states.transpose(1, 2)\n        hidden_states = F.gelu(self.conv1(hidden_states))\n        hidden_states = F.gelu(self.conv2(hidden_states))\n        hidden_states = hidden_states.transpose(1, 2)\n        hidden_length = hidden_length // 2  # from 100Hz -> 50Hz\n        # Pos encoding\n        if apply_position:\n            pos_embed = self.embed_positions(\n                torch.arange(0, hidden_states.shape[1], device=hidden_states.device)\n            )\n            hidden_states = hidden_states + pos_embed\n        hidden_states = nn.functional.dropout(\n            hidden_states, p=self.dropout, training=self.training\n        )\n        # Transformer\n        attention_mask = make_nonpad_mask(hidden_length).unsqueeze(1)  # (b, 1, t)\n        for layer in self.layers:\n            hidden_states = layer(hidden_states, attention_mask)\n\n        hidden_states = self.layer_norm(hidden_states)\n        return hidden_states, hidden_length"
  },
  {
    "id": "89a07e6c908816772be14019712324c710394d94",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/whisper.py",
    "kind": "function",
    "name": "__init__",
    "signature": "__init__(\n        self,\n        in_dim: int,\n        embed_dim: int,\n        num_layers: int,\n        num_heads: int,\n        ffn_dim: int = None,\n        attn_dropout: float = 0.0,\n        dropout: float = 0.0,\n        max_positions: int = 1500,\n    )",
    "docstring": "",
    "identifiers": [
      "_",
      "__init__",
      "_init_weights",
      "apply",
      "attn_dropout",
      "conv1",
      "conv1d",
      "conv2",
      "copy_",
      "dropout",
      "embed_dim",
      "embed_positions",
      "embedding",
      "ffn_dim",
      "float",
      "in_dim",
      "int",
      "kernel_size",
      "layer_norm",
      "layernorm",
      "layers",
      "max_positions",
      "modulelist",
      "nn",
      "num_heads",
      "num_layers",
      "padding",
      "range",
      "requires_grad_",
      "self",
      "shape",
      "sinusoids",
      "stride",
      "super",
      "weight",
      "whisperencoderlayer"
    ],
    "start_line": 196,
    "end_line": 232,
    "text": "def __init__(\n        self,\n        in_dim: int,\n        embed_dim: int,\n        num_layers: int,\n        num_heads: int,\n        ffn_dim: int = None,\n        attn_dropout: float = 0.0,\n        dropout: float = 0.0,\n        max_positions: int = 1500,\n    ):\n        super().__init__()\n        self.in_dim = in_dim\n        self.embed_dim = embed_dim\n        self.dropout = dropout\n        # Input downsampling\n        self.conv1 = nn.Conv1d(in_dim, embed_dim, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, stride=2, padding=1)\n        # Fixed positional embedding\n        self.max_positions = max_positions\n        self.embed_positions = nn.Embedding(self.max_positions, embed_dim)\n        self.embed_positions.requires_grad_(False)\n        # Transformer\n        self.layers = nn.ModuleList(\n            [\n                WhisperEncoderLayer(\n                    embed_dim, num_heads, ffn_dim, attn_dropout, dropout\n                )\n                for _ in range(num_layers)\n            ]\n        )\n        # Output norm\n        self.layer_norm = nn.LayerNorm(embed_dim)\n        # Init weight\n        self.apply(self._init_weights)\n        # Init position embedding\n        self.embed_positions.weight.copy_(sinusoids(*self.embed_positions.weight.shape))"
  },
  {
    "id": "0c8b56bb4cac9e728fdf0a8c97ca1010079df1a0",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/whisper.py",
    "kind": "class",
    "name": "WhisperEncoderLayer",
    "signature": "class WhisperEncoderLayer(nn.Module)",
    "docstring": "",
    "identifiers": [
      "__init__",
      "attention_mask",
      "attn_dropout",
      "dropout",
      "embed_dim",
      "f",
      "fc1",
      "fc2",
      "ffn_dim",
      "final_layer_norm",
      "float",
      "forward",
      "forward_chunk",
      "gelu",
      "hidden_states",
      "int",
      "kv_cache",
      "layernorm",
      "linear",
      "module",
      "new_kv_cache",
      "nn",
      "num_heads",
      "p",
      "residual",
      "self",
      "self_attn",
      "self_attn_layer_norm",
      "super",
      "tensor",
      "torch",
      "training",
      "whisperencoderlayer",
      "whispersdpaattention"
    ],
    "start_line": 121,
    "end_line": 192,
    "text": "class WhisperEncoderLayer(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        ffn_dim: int = None,\n        attn_dropout: float = 0.0,\n        dropout: float = 0.0,\n    ):\n        super().__init__()\n        self.dropout = dropout\n        # Attention\n        self.self_attn = WhisperSdpaAttention(embed_dim, num_heads, attn_dropout)\n        self.self_attn_layer_norm = nn.LayerNorm(embed_dim)\n        # FFN\n        ffn_dim = ffn_dim if ffn_dim is not None else embed_dim * 4\n        self.fc1 = nn.Linear(embed_dim, ffn_dim)\n        self.fc2 = nn.Linear(ffn_dim, embed_dim)\n        # Output norm\n        self.final_layer_norm = nn.LayerNorm(embed_dim)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: torch.Tensor,\n    ):\n        # Attention\n        residual = hidden_states\n        hidden_states = self.self_attn_layer_norm(hidden_states)\n        hidden_states = self.self_attn(hidden_states, attention_mask)\n        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n        hidden_states = residual + hidden_states\n\n        # FFN\n        residual = hidden_states\n        hidden_states = self.final_layer_norm(hidden_states)\n        hidden_states = F.gelu(self.fc1(hidden_states))\n        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n        hidden_states = self.fc2(hidden_states)\n        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n    def forward_chunk(\n        self,\n        hidden_states: torch.Tensor,\n        kv_cache: torch.Tensor = None,\n    ):\n        \"\"\"Forward self-attention with kv cache.\n\n        Args:\n            hidden_states: shape (b, t, c)\n            kv_cache: shape (b, nh, t, c*2)\n        \"\"\"\n        # Attention\n        residual = hidden_states\n        hidden_states = self.self_attn_layer_norm(hidden_states)\n        hidden_states, new_kv_cache = self.self_attn.forward_chunk(\n            hidden_states, kv_cache\n        )\n        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n        hidden_states = residual + hidden_states\n\n        # FFN\n        residual = hidden_states\n        hidden_states = self.final_layer_norm(hidden_states)\n        hidden_states = F.gelu(self.fc1(hidden_states))\n        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n        hidden_states = self.fc2(hidden_states)\n        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n        hidden_states = residual + hidden_states\n        return hidden_states, new_kv_cache"
  },
  {
    "id": "cca13abfe71add43b57f39f397a42f24dcf691e0",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/whisper.py",
    "kind": "function",
    "name": "forward_chunk",
    "signature": "forward_chunk(\n        self,\n        hidden_states: torch.Tensor,\n        kv_cache: torch.Tensor = None,\n    )",
    "docstring": "Forward self-attention with kv cache.\n\n        Args:\n            hidden_states: shape (b, t, c)\n            kv_cache: shape (b, nh, t, c*2)",
    "identifiers": [
      "dropout",
      "f",
      "fc1",
      "fc2",
      "final_layer_norm",
      "forward_chunk",
      "gelu",
      "hidden_states",
      "kv_cache",
      "new_kv_cache",
      "p",
      "residual",
      "self",
      "self_attn",
      "self_attn_layer_norm",
      "tensor",
      "torch",
      "training"
    ],
    "start_line": 164,
    "end_line": 192,
    "text": "def forward_chunk(\n        self,\n        hidden_states: torch.Tensor,\n        kv_cache: torch.Tensor = None,\n    ):\n        \"\"\"Forward self-attention with kv cache.\n\n        Args:\n            hidden_states: shape (b, t, c)\n            kv_cache: shape (b, nh, t, c*2)\n        \"\"\"\n        # Attention\n        residual = hidden_states\n        hidden_states = self.self_attn_layer_norm(hidden_states)\n        hidden_states, new_kv_cache = self.self_attn.forward_chunk(\n            hidden_states, kv_cache\n        )\n        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n        hidden_states = residual + hidden_states\n\n        # FFN\n        residual = hidden_states\n        hidden_states = self.final_layer_norm(hidden_states)\n        hidden_states = F.gelu(self.fc1(hidden_states))\n        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n        hidden_states = self.fc2(hidden_states)\n        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n        hidden_states = residual + hidden_states\n        return hidden_states, new_kv_cache"
  },
  {
    "id": "56eba88959e8198f353efc344a1f77b0e4a852f4",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/whisper.py",
    "kind": "function",
    "name": "forward",
    "signature": "forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: torch.Tensor,\n    )",
    "docstring": "",
    "identifiers": [
      "attention_mask",
      "dropout",
      "f",
      "fc1",
      "fc2",
      "final_layer_norm",
      "forward",
      "gelu",
      "hidden_states",
      "p",
      "residual",
      "self",
      "self_attn",
      "self_attn_layer_norm",
      "tensor",
      "torch",
      "training"
    ],
    "start_line": 142,
    "end_line": 162,
    "text": "def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: torch.Tensor,\n    ):\n        # Attention\n        residual = hidden_states\n        hidden_states = self.self_attn_layer_norm(hidden_states)\n        hidden_states = self.self_attn(hidden_states, attention_mask)\n        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n        hidden_states = residual + hidden_states\n\n        # FFN\n        residual = hidden_states\n        hidden_states = self.final_layer_norm(hidden_states)\n        hidden_states = F.gelu(self.fc1(hidden_states))\n        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n        hidden_states = self.fc2(hidden_states)\n        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n        hidden_states = residual + hidden_states\n        return hidden_states"
  },
  {
    "id": "dfda2b6ca49fb7462867c31c9456708454fc101a",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/whisper.py",
    "kind": "function",
    "name": "__init__",
    "signature": "__init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        ffn_dim: int = None,\n        attn_dropout: float = 0.0,\n        dropout: float = 0.0,\n    )",
    "docstring": "",
    "identifiers": [
      "__init__",
      "attn_dropout",
      "dropout",
      "embed_dim",
      "fc1",
      "fc2",
      "ffn_dim",
      "final_layer_norm",
      "float",
      "int",
      "layernorm",
      "linear",
      "nn",
      "num_heads",
      "self",
      "self_attn",
      "self_attn_layer_norm",
      "super",
      "whispersdpaattention"
    ],
    "start_line": 122,
    "end_line": 140,
    "text": "def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        ffn_dim: int = None,\n        attn_dropout: float = 0.0,\n        dropout: float = 0.0,\n    ):\n        super().__init__()\n        self.dropout = dropout\n        # Attention\n        self.self_attn = WhisperSdpaAttention(embed_dim, num_heads, attn_dropout)\n        self.self_attn_layer_norm = nn.LayerNorm(embed_dim)\n        # FFN\n        ffn_dim = ffn_dim if ffn_dim is not None else embed_dim * 4\n        self.fc1 = nn.Linear(embed_dim, ffn_dim)\n        self.fc2 = nn.Linear(ffn_dim, embed_dim)\n        # Output norm\n        self.final_layer_norm = nn.LayerNorm(embed_dim)"
  },
  {
    "id": "7934ec092992f225bc0ac4eb957660ff6a0e9b17",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/whisper.py",
    "kind": "class",
    "name": "WhisperSdpaAttention",
    "signature": "class WhisperSdpaAttention(nn.Module)",
    "docstring": "",
    "identifiers": [
      "_",
      "__init__",
      "_shape",
      "attention_mask",
      "attn_mask",
      "attn_output",
      "bias",
      "bool",
      "bsz",
      "cat",
      "chunk",
      "contiguous",
      "dim",
      "dropout",
      "dropout_p",
      "embed_dim",
      "f",
      "float",
      "forward",
      "forward_chunk",
      "head_dim",
      "hidden_states",
      "int",
      "k_cache",
      "k_proj",
      "key_states",
      "kv_cache",
      "len",
      "linear",
      "module",
      "new_kv_cache",
      "nn",
      "num_heads",
      "optional",
      "out_proj",
      "q_proj",
      "query_states",
      "reshape",
      "scaled_dot_product_attention",
      "self",
      "seq_len",
      "shape",
      "size",
      "super",
      "tensor",
      "tgt_len",
      "torch",
      "training",
      "transpose",
      "unsqueeze",
      "v_cache",
      "v_proj",
      "value_states",
      "view",
      "whispersdpaattention"
    ],
    "start_line": 23,
    "end_line": 118,
    "text": "class WhisperSdpaAttention(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        bias: bool = True,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.head_dim = embed_dim // num_heads\n        self.bias = bias\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return (\n            tensor.view(bsz, seq_len, self.num_heads, self.head_dim)\n            .transpose(1, 2)\n            .contiguous()\n        )\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n    ):\n        \"\"\"\n        Args:\n            attention_mask: Bool mask or float mask. Bool mask, True indicates should attend. Float mask is added to the attention score.\n        \"\"\"\n        bsz, tgt_len, _ = hidden_states.size()\n\n        query_states = self._shape(self.q_proj(hidden_states), tgt_len, bsz)\n        key_states = self._shape(self.k_proj(hidden_states), tgt_len, bsz)\n        value_states = self._shape(self.v_proj(hidden_states), tgt_len, bsz)\n\n        # NOTE sdpa needs a 4-dim attention_mask: (b, nh, tq, tv)\n        if attention_mask is not None and len(attention_mask.shape) == 3:\n            attention_mask = attention_mask.unsqueeze(1)\n\n        attn_output = F.scaled_dot_product_attention(\n            query_states,\n            key_states,\n            value_states,\n            attn_mask=attention_mask,\n            dropout_p=self.dropout if self.training else 0.0,\n        )  # (bsz, nh, l, d)\n        attn_output = attn_output.transpose(1, 2)\n        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n\n        attn_output = self.out_proj(attn_output)\n        return attn_output\n\n    def forward_chunk(\n        self,\n        hidden_states: torch.Tensor,\n        kv_cache: torch.Tensor = None,\n    ):\n        \"\"\"Forward self-attention with kv cache.\n\n        Args:\n            hidden_states: shape (b, t, c)\n            kv_cache: shape (b, nh, t, c*2)\n        \"\"\"\n        bsz, tgt_len, _ = hidden_states.size()\n\n        # shape (b, nh, t, c)\n        query_states = self._shape(self.q_proj(hidden_states), tgt_len, bsz)\n        key_states = self._shape(self.k_proj(hidden_states), tgt_len, bsz)\n        value_states = self._shape(self.v_proj(hidden_states), tgt_len, bsz)\n\n        # unpack cache\n        if kv_cache is not None:\n            k_cache, v_cache = kv_cache.chunk(2, dim=-1)\n            key_states = torch.cat([k_cache, key_states], dim=2)\n            value_states = torch.cat([v_cache, value_states], dim=2)\n        new_kv_cache = torch.cat([key_states, value_states], dim=-1)\n\n        # attention\n        attn_output = F.scaled_dot_product_attention(\n            query_states,\n            key_states,\n            value_states,\n            attn_mask=None,\n            dropout_p=0.0,\n        )  # (bsz, nh, l, d)\n        attn_output = attn_output.transpose(1, 2)\n        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n\n        attn_output = self.out_proj(attn_output)\n        return attn_output, new_kv_cache"
  },
  {
    "id": "f6a3a790429aed20e8b4051f9c4da223bb783d25",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/whisper.py",
    "kind": "function",
    "name": "forward_chunk",
    "signature": "forward_chunk(\n        self,\n        hidden_states: torch.Tensor,\n        kv_cache: torch.Tensor = None,\n    )",
    "docstring": "Forward self-attention with kv cache.\n\n        Args:\n            hidden_states: shape (b, t, c)\n            kv_cache: shape (b, nh, t, c*2)",
    "identifiers": [
      "_",
      "_shape",
      "attn_mask",
      "attn_output",
      "bsz",
      "cat",
      "chunk",
      "dim",
      "dropout_p",
      "embed_dim",
      "f",
      "forward_chunk",
      "hidden_states",
      "k_cache",
      "k_proj",
      "key_states",
      "kv_cache",
      "new_kv_cache",
      "out_proj",
      "q_proj",
      "query_states",
      "reshape",
      "scaled_dot_product_attention",
      "self",
      "size",
      "tensor",
      "tgt_len",
      "torch",
      "transpose",
      "v_cache",
      "v_proj",
      "value_states"
    ],
    "start_line": 81,
    "end_line": 118,
    "text": "def forward_chunk(\n        self,\n        hidden_states: torch.Tensor,\n        kv_cache: torch.Tensor = None,\n    ):\n        \"\"\"Forward self-attention with kv cache.\n\n        Args:\n            hidden_states: shape (b, t, c)\n            kv_cache: shape (b, nh, t, c*2)\n        \"\"\"\n        bsz, tgt_len, _ = hidden_states.size()\n\n        # shape (b, nh, t, c)\n        query_states = self._shape(self.q_proj(hidden_states), tgt_len, bsz)\n        key_states = self._shape(self.k_proj(hidden_states), tgt_len, bsz)\n        value_states = self._shape(self.v_proj(hidden_states), tgt_len, bsz)\n\n        # unpack cache\n        if kv_cache is not None:\n            k_cache, v_cache = kv_cache.chunk(2, dim=-1)\n            key_states = torch.cat([k_cache, key_states], dim=2)\n            value_states = torch.cat([v_cache, value_states], dim=2)\n        new_kv_cache = torch.cat([key_states, value_states], dim=-1)\n\n        # attention\n        attn_output = F.scaled_dot_product_attention(\n            query_states,\n            key_states,\n            value_states,\n            attn_mask=None,\n            dropout_p=0.0,\n        )  # (bsz, nh, l, d)\n        attn_output = attn_output.transpose(1, 2)\n        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n\n        attn_output = self.out_proj(attn_output)\n        return attn_output, new_kv_cache"
  },
  {
    "id": "4ad3a808e9b5244593d152213f52bd972c2f9096",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/whisper.py",
    "kind": "function",
    "name": "forward",
    "signature": "forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n    )",
    "docstring": "Args:\n            attention_mask: Bool mask or float mask. Bool mask, True indicates should attend. Float mask is added to the attention score.",
    "identifiers": [
      "_",
      "_shape",
      "attention_mask",
      "attn_mask",
      "attn_output",
      "bsz",
      "dropout",
      "dropout_p",
      "embed_dim",
      "f",
      "forward",
      "hidden_states",
      "k_proj",
      "key_states",
      "len",
      "optional",
      "out_proj",
      "q_proj",
      "query_states",
      "reshape",
      "scaled_dot_product_attention",
      "self",
      "shape",
      "size",
      "tensor",
      "tgt_len",
      "torch",
      "training",
      "transpose",
      "unsqueeze",
      "v_proj",
      "value_states"
    ],
    "start_line": 49,
    "end_line": 79,
    "text": "def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n    ):\n        \"\"\"\n        Args:\n            attention_mask: Bool mask or float mask. Bool mask, True indicates should attend. Float mask is added to the attention score.\n        \"\"\"\n        bsz, tgt_len, _ = hidden_states.size()\n\n        query_states = self._shape(self.q_proj(hidden_states), tgt_len, bsz)\n        key_states = self._shape(self.k_proj(hidden_states), tgt_len, bsz)\n        value_states = self._shape(self.v_proj(hidden_states), tgt_len, bsz)\n\n        # NOTE sdpa needs a 4-dim attention_mask: (b, nh, tq, tv)\n        if attention_mask is not None and len(attention_mask.shape) == 3:\n            attention_mask = attention_mask.unsqueeze(1)\n\n        attn_output = F.scaled_dot_product_attention(\n            query_states,\n            key_states,\n            value_states,\n            attn_mask=attention_mask,\n            dropout_p=self.dropout if self.training else 0.0,\n        )  # (bsz, nh, l, d)\n        attn_output = attn_output.transpose(1, 2)\n        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n\n        attn_output = self.out_proj(attn_output)\n        return attn_output"
  },
  {
    "id": "81f4ae64866807395c8d5341ca04a85fbdd856d5",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/whisper.py",
    "kind": "function",
    "name": "_shape",
    "signature": "_shape(self, tensor: torch.Tensor, seq_len: int, bsz: int)",
    "docstring": "",
    "identifiers": [
      "_shape",
      "bsz",
      "contiguous",
      "head_dim",
      "int",
      "num_heads",
      "self",
      "seq_len",
      "tensor",
      "torch",
      "transpose",
      "view"
    ],
    "start_line": 42,
    "end_line": 47,
    "text": "def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return (\n            tensor.view(bsz, seq_len, self.num_heads, self.head_dim)\n            .transpose(1, 2)\n            .contiguous()\n        )"
  },
  {
    "id": "47203a64293cf5e25b6fce7db241698e16304a03",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/whisper.py",
    "kind": "function",
    "name": "__init__",
    "signature": "__init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        bias: bool = True,\n    )",
    "docstring": "",
    "identifiers": [
      "__init__",
      "bias",
      "bool",
      "dropout",
      "embed_dim",
      "float",
      "head_dim",
      "int",
      "k_proj",
      "linear",
      "nn",
      "num_heads",
      "out_proj",
      "q_proj",
      "self",
      "super",
      "v_proj"
    ],
    "start_line": 24,
    "end_line": 40,
    "text": "def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        bias: bool = True,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.head_dim = embed_dim // num_heads\n        self.bias = bias\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)"
  },
  {
    "id": "78010d7176bd419fb63cf12e2b619926a359188b",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/whisper.py",
    "kind": "function",
    "name": "sinusoids",
    "signature": "sinusoids(length: int, channels: int, max_timescale: float = 10000)",
    "docstring": "Returns sinusoids for positional embedding",
    "identifiers": [
      "arange",
      "cat",
      "channels",
      "cos",
      "dim",
      "exp",
      "float",
      "int",
      "inv_timescales",
      "length",
      "log",
      "log_timescale_increment",
      "math",
      "max_timescale",
      "scaled_time",
      "sin",
      "sinusoids",
      "tensor",
      "torch",
      "valueerror",
      "view"
    ],
    "start_line": 11,
    "end_line": 20,
    "text": "def sinusoids(length: int, channels: int, max_timescale: float = 10000) -> torch.Tensor:\n    \"\"\"Returns sinusoids for positional embedding\"\"\"\n    if channels % 2 != 0:\n        raise ValueError(\n            f\"Number of channels has to be divisible by 2 for sinusoidal positional embeddings, got {channels} channels.\"\n        )\n    log_timescale_increment = math.log(max_timescale) / (channels // 2 - 1)\n    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))\n    scaled_time = torch.arange(length).view(-1, 1) * inv_timescales.view(1, -1)\n    return torch.cat([scaled_time.sin(), scaled_time.cos()], dim=1)"
  },
  {
    "id": "5d472f0d50e24c982de248b45f490fa5a58f2a89",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/rvq.py",
    "kind": "class",
    "name": "ResidualVQ",
    "signature": "class ResidualVQ(nn.Module)",
    "docstring": "",
    "identifiers": [
      "__init__",
      "all_indices",
      "append",
      "b",
      "clone",
      "code_i",
      "codebook_dim",
      "codebook_size",
      "codes",
      "decode_code",
      "decode_codes",
      "device",
      "dtype",
      "emb",
      "encode_code",
      "encode_codes",
      "enumerate",
      "float",
      "float32",
      "i",
      "identity",
      "indices_i",
      "input_dim",
      "input_proj",
      "int",
      "kernel_size",
      "module",
      "modulelist",
      "nn",
      "nq",
      "num_quantizers",
      "out_project",
      "output_dim",
      "output_proj",
      "quantized_i",
      "quantizer",
      "quantizers",
      "range",
      "residual",
      "residualvq",
      "rvq_dim",
      "self",
      "shape",
      "stack",
      "super",
      "t",
      "tensor",
      "torch",
      "vectorquantize",
      "wnconv1d",
      "z",
      "z_q_i",
      "zeros"
    ],
    "start_line": 92,
    "end_line": 164,
    "text": "class ResidualVQ(nn.Module):\n    def __init__(\n        self,\n        input_dim: int = 768,  # Input dimension, unrelated to RVQ\n        rvq_dim=None,  # RVQ dimension. If different from input_dim/output_dim, will add input_dim->rvq_dim/rvq_dim->output_dim projection\n        output_dim: int = None,  # Output dimension, unrelated to RVQ\n        num_quantizers: int = 8,\n        codebook_size: int = 1024,\n        codebook_dim: int = 256,  # Dimension of each codebook. If different from rvq_dim, will add rvq_dim->codebook_dim and codebook_dim->rvq_dim projections\n    ):\n        super().__init__()\n        self.input_dim = input_dim\n\n        self.num_quantizers = num_quantizers\n        self.codebook_size = codebook_size\n        self.codebook_dim = codebook_dim\n        self.rvq_dim = rvq_dim\n\n        self.input_proj = (\n            WNConv1d(input_dim, rvq_dim, kernel_size=1)\n            if input_dim != rvq_dim\n            else nn.Identity()\n        )\n        self.output_proj = (\n            WNConv1d(rvq_dim, output_dim, kernel_size=1)\n            if rvq_dim != output_dim\n            else nn.Identity()\n        )\n\n        self.quantizers = nn.ModuleList(\n            [\n                VectorQuantize(\n                    input_dim=rvq_dim,\n                    codebook_size=self.codebook_size,\n                    codebook_dim=codebook_dim,\n                )\n                for i in range(num_quantizers)\n            ]\n        )\n\n    def encode_codes(self, z: torch.Tensor):\n        z = self.input_proj(z)\n        residual = z.clone().float()  # (B, D, T), ensure fp32\n        all_indices = []\n        # Quantize to tokens\n        for i, quantizer in enumerate(self.quantizers):\n            # (B, D, T), (B), scalar, (B, T), (B, D', T), ensure fp32\n            z_q_i, indices_i = quantizer.encode_code(residual)\n            residual = residual - z_q_i\n            all_indices.append(indices_i)  # (B, T)\n        all_indices = torch.stack(all_indices)  # (N, B, T)\n        return all_indices\n\n    def decode_codes(self, codes):  # codes: (nq, B, T)\n        \"\"\"Decode codes from multiple quantizers to embeddings.\n\n        Args:\n            codes: Tensor of shape (nq, B, T) containing code indices for each quantizer.\n\n        Returns:\n            emb: Tensor of shape (B, D, T) representing the decoded embeddings.\n        \"\"\"\n        nq, B, T = codes.shape\n        device = codes.device\n        emb = torch.zeros(\n            B, self.rvq_dim, T, device=device, dtype=torch.float32\n        )  # (B, D, T)\n        for i, quantizer in enumerate(self.quantizers[:nq]):\n            code_i = codes[i]  # (B, T)\n            quantized_i = quantizer.decode_code(code_i)  # (B, D', T)\n            emb += quantizer.out_project(quantized_i)  # Accumulate quantized embeddings\n        emb = self.output_proj(emb)  # (B, D, T), apply output projection\n        return emb  # (B, D, T)"
  },
  {
    "id": "58e70bd18ad0dbd51caaf12effc6b17fbb2598c4",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/rvq.py",
    "kind": "function",
    "name": "decode_codes",
    "signature": "decode_codes(self, codes)",
    "docstring": "Decode codes from multiple quantizers to embeddings.\n\n        Args:\n            codes: Tensor of shape (nq, B, T) containing code indices for each quantizer.\n\n        Returns:\n            emb: Tensor of shape (B, D, T) representing the decoded embeddings.",
    "identifiers": [
      "b",
      "code_i",
      "codes",
      "decode_code",
      "decode_codes",
      "device",
      "dtype",
      "emb",
      "enumerate",
      "float32",
      "i",
      "nq",
      "out_project",
      "output_proj",
      "quantized_i",
      "quantizer",
      "quantizers",
      "rvq_dim",
      "self",
      "shape",
      "t",
      "torch",
      "zeros"
    ],
    "start_line": 145,
    "end_line": 164,
    "text": "def decode_codes(self, codes):  # codes: (nq, B, T)\n        \"\"\"Decode codes from multiple quantizers to embeddings.\n\n        Args:\n            codes: Tensor of shape (nq, B, T) containing code indices for each quantizer.\n\n        Returns:\n            emb: Tensor of shape (B, D, T) representing the decoded embeddings.\n        \"\"\"\n        nq, B, T = codes.shape\n        device = codes.device\n        emb = torch.zeros(\n            B, self.rvq_dim, T, device=device, dtype=torch.float32\n        )  # (B, D, T)\n        for i, quantizer in enumerate(self.quantizers[:nq]):\n            code_i = codes[i]  # (B, T)\n            quantized_i = quantizer.decode_code(code_i)  # (B, D', T)\n            emb += quantizer.out_project(quantized_i)  # Accumulate quantized embeddings\n        emb = self.output_proj(emb)  # (B, D, T), apply output projection\n        return emb  # (B, D, T)"
  },
  {
    "id": "e6fd53ca807b9683f87df3eb8b617fb795831a31",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/rvq.py",
    "kind": "function",
    "name": "encode_codes",
    "signature": "encode_codes(self, z: torch.Tensor)",
    "docstring": "",
    "identifiers": [
      "all_indices",
      "append",
      "clone",
      "encode_code",
      "encode_codes",
      "enumerate",
      "float",
      "i",
      "indices_i",
      "input_proj",
      "quantizer",
      "quantizers",
      "residual",
      "self",
      "stack",
      "tensor",
      "torch",
      "z",
      "z_q_i"
    ],
    "start_line": 132,
    "end_line": 143,
    "text": "def encode_codes(self, z: torch.Tensor):\n        z = self.input_proj(z)\n        residual = z.clone().float()  # (B, D, T), ensure fp32\n        all_indices = []\n        # Quantize to tokens\n        for i, quantizer in enumerate(self.quantizers):\n            # (B, D, T), (B), scalar, (B, T), (B, D', T), ensure fp32\n            z_q_i, indices_i = quantizer.encode_code(residual)\n            residual = residual - z_q_i\n            all_indices.append(indices_i)  # (B, T)\n        all_indices = torch.stack(all_indices)  # (N, B, T)\n        return all_indices"
  },
  {
    "id": "081b405bbb7738841b02b35cf7996c7cbbf31bf2",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/rvq.py",
    "kind": "function",
    "name": "__init__",
    "signature": "__init__(\n        self,\n        input_dim: int = 768,  # Input dimension, unrelated to RVQ\n        rvq_dim=None,  # RVQ dimension. If different from input_dim/output_dim, will add input_dim->rvq_dim/rvq_dim->output_dim projection\n        output_dim: int = None,  # Output dimension, unrelated to RVQ\n        num_quantizers: int = 8,\n        codebook_size: int = 1024,\n        codebook_dim: int = 256,  # Dimension of each codebook. If different from rvq_dim, will add rvq_dim->codebook_dim and codebook_dim->rvq_dim projections\n    )",
    "docstring": "",
    "identifiers": [
      "__init__",
      "codebook_dim",
      "codebook_size",
      "i",
      "identity",
      "input_dim",
      "input_proj",
      "int",
      "kernel_size",
      "modulelist",
      "nn",
      "num_quantizers",
      "output_dim",
      "output_proj",
      "quantizers",
      "range",
      "rvq_dim",
      "self",
      "super",
      "vectorquantize",
      "wnconv1d"
    ],
    "start_line": 93,
    "end_line": 130,
    "text": "def __init__(\n        self,\n        input_dim: int = 768,  # Input dimension, unrelated to RVQ\n        rvq_dim=None,  # RVQ dimension. If different from input_dim/output_dim, will add input_dim->rvq_dim/rvq_dim->output_dim projection\n        output_dim: int = None,  # Output dimension, unrelated to RVQ\n        num_quantizers: int = 8,\n        codebook_size: int = 1024,\n        codebook_dim: int = 256,  # Dimension of each codebook. If different from rvq_dim, will add rvq_dim->codebook_dim and codebook_dim->rvq_dim projections\n    ):\n        super().__init__()\n        self.input_dim = input_dim\n\n        self.num_quantizers = num_quantizers\n        self.codebook_size = codebook_size\n        self.codebook_dim = codebook_dim\n        self.rvq_dim = rvq_dim\n\n        self.input_proj = (\n            WNConv1d(input_dim, rvq_dim, kernel_size=1)\n            if input_dim != rvq_dim\n            else nn.Identity()\n        )\n        self.output_proj = (\n            WNConv1d(rvq_dim, output_dim, kernel_size=1)\n            if rvq_dim != output_dim\n            else nn.Identity()\n        )\n\n        self.quantizers = nn.ModuleList(\n            [\n                VectorQuantize(\n                    input_dim=rvq_dim,\n                    codebook_size=self.codebook_size,\n                    codebook_dim=codebook_dim,\n                )\n                for i in range(num_quantizers)\n            ]\n        )"
  },
  {
    "id": "8b5a04d481c0367d5f24bba18192411302d2a779",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/rvq.py",
    "kind": "class",
    "name": "VectorQuantize",
    "signature": "class VectorQuantize(nn.Module)",
    "docstring": "",
    "identifiers": [
      "__init__",
      "b",
      "bool",
      "clone",
      "codebook",
      "codebook_dim",
      "codebook_size",
      "decode_code",
      "detach",
      "dist",
      "dtype",
      "embed",
      "embed_id",
      "embedding",
      "encode_code",
      "encodings",
      "f",
      "float",
      "identity",
      "in_project",
      "indices",
      "input_dim",
      "int",
      "keepdim",
      "kernel_size",
      "max",
      "module",
      "nn",
      "out_project",
      "pow",
      "rearrange",
      "register_buffer",
      "self",
      "size",
      "sum",
      "super",
      "t",
      "tensor",
      "torch",
      "transpose",
      "vectorquantize",
      "wnconv1d",
      "z",
      "z_e",
      "z_q",
      "zeros"
    ],
    "start_line": 16,
    "end_line": 89,
    "text": "class VectorQuantize(nn.Module):\n    def __init__(\n        self,\n        input_dim: int,\n        codebook_size: int,\n        codebook_dim: int,\n    ):\n        super().__init__()\n        self.input_dim = input_dim\n        self.codebook_size = codebook_size\n        self.codebook_dim = codebook_dim\n\n        self.in_project = (\n            WNConv1d(\n                self.input_dim, self.codebook_dim, kernel_size=1\n            )  # (B, D, T) -> (B, D', T)\n            if self.input_dim != self.codebook_dim\n            else nn.Identity()\n        )\n        self.out_project = (\n            WNConv1d(\n                self.codebook_dim, self.input_dim, kernel_size=1\n            )  # (B, D', T) -> (B, D, T)\n            if self.input_dim != self.codebook_dim\n            else nn.Identity()\n        )\n\n        # Initialize codebook and EMA buffers\n        self.register_buffer(\n            \"codebook\", torch.zeros(codebook_size, codebook_dim).float()\n        )  # (codebook_size, D'), ensure fp32\n        # Place holder, not used in inference\n        self.register_buffer(\"inited\", torch.tensor([True], dtype=torch.bool))  # (1)\n        self.register_buffer(\n            \"cluster_size\", torch.zeros(codebook_size).float()\n        )  # (codebook_size), ensure fp32\n        self.register_buffer(\n            \"embed_avg\", self.codebook.clone().float()\n        )  # (codebook_size, D'), ensure fp32\n\n    def decode_code(self, embed_id):  # embed_id: (B, T)\n        embed = (\n            F.embedding(embed_id, self.codebook).transpose(1, 2).float()\n        )  # (B, D', T), ensure fp32\n        return embed\n\n    def encode_code(self, z: torch.Tensor):  # z: (B, D, T)\n        # logging.info(f\"{self.cluster_size = }, {self.codebook = }, {self.embed_avg = }, {self.inited = }\")\n        z = z.float()  # Ensure fp32\n        z_e = self.in_project(z).float()  # (B, D', T), ensure fp32\n\n        # Rearrange for quantization\n        encodings = rearrange(z_e, \"b d t -> (b t) d\").float()  # (B*T, D'), ensure fp32\n\n        # Quantization\n        dist = (\n            encodings.pow(2).sum(1, keepdim=True)  # (B*T, 1)\n            - 2 * encodings @ self.codebook.float().t()  # (B*T, codebook_size)\n            + self.codebook.float().pow(2).sum(1, keepdim=True).t()\n        )  # (1, codebook_size)\n\n        # dist: (B*T, codebook_size)\n        indices = (-dist).max(1)[1]  # (B*T)\n        indices = rearrange(indices, \"(b t) -> b t\", b=z.size(0))  # (B, T)\n\n        # Get quantized vectors\n        z_q = self.decode_code(indices).float()  # (B, D', T), ensure fp32\n\n        # Straight-through estimator\n        z_q = z_e + (z_q - z_e).detach()  # (B, D', T)\n        z_q = self.out_project(z_q).float()  # (B, D, T), ensure fp32\n\n        # z_q: (B, D, T), commit_loss: (B), indices: (B, T), z: (B, D', T)\n        return z_q, indices"
  },
  {
    "id": "b3873dbe6e1b3d1246cb87df78abd92fec3573aa",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/rvq.py",
    "kind": "function",
    "name": "encode_code",
    "signature": "encode_code(self, z: torch.Tensor)",
    "docstring": "",
    "identifiers": [
      "b",
      "codebook",
      "decode_code",
      "detach",
      "dist",
      "encode_code",
      "encodings",
      "float",
      "in_project",
      "indices",
      "keepdim",
      "max",
      "out_project",
      "pow",
      "rearrange",
      "self",
      "size",
      "sum",
      "t",
      "tensor",
      "torch",
      "z",
      "z_e",
      "z_q"
    ],
    "start_line": 62,
    "end_line": 89,
    "text": "def encode_code(self, z: torch.Tensor):  # z: (B, D, T)\n        # logging.info(f\"{self.cluster_size = }, {self.codebook = }, {self.embed_avg = }, {self.inited = }\")\n        z = z.float()  # Ensure fp32\n        z_e = self.in_project(z).float()  # (B, D', T), ensure fp32\n\n        # Rearrange for quantization\n        encodings = rearrange(z_e, \"b d t -> (b t) d\").float()  # (B*T, D'), ensure fp32\n\n        # Quantization\n        dist = (\n            encodings.pow(2).sum(1, keepdim=True)  # (B*T, 1)\n            - 2 * encodings @ self.codebook.float().t()  # (B*T, codebook_size)\n            + self.codebook.float().pow(2).sum(1, keepdim=True).t()\n        )  # (1, codebook_size)\n\n        # dist: (B*T, codebook_size)\n        indices = (-dist).max(1)[1]  # (B*T)\n        indices = rearrange(indices, \"(b t) -> b t\", b=z.size(0))  # (B, T)\n\n        # Get quantized vectors\n        z_q = self.decode_code(indices).float()  # (B, D', T), ensure fp32\n\n        # Straight-through estimator\n        z_q = z_e + (z_q - z_e).detach()  # (B, D', T)\n        z_q = self.out_project(z_q).float()  # (B, D, T), ensure fp32\n\n        # z_q: (B, D, T), commit_loss: (B), indices: (B, T), z: (B, D', T)\n        return z_q, indices"
  },
  {
    "id": "fb0596ca3c93c240b743500c4c9bde018e410a47",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/rvq.py",
    "kind": "function",
    "name": "decode_code",
    "signature": "decode_code(self, embed_id)",
    "docstring": "",
    "identifiers": [
      "codebook",
      "decode_code",
      "embed",
      "embed_id",
      "embedding",
      "f",
      "float",
      "self",
      "transpose"
    ],
    "start_line": 56,
    "end_line": 60,
    "text": "def decode_code(self, embed_id):  # embed_id: (B, T)\n        embed = (\n            F.embedding(embed_id, self.codebook).transpose(1, 2).float()\n        )  # (B, D', T), ensure fp32\n        return embed"
  },
  {
    "id": "6406e7f720502d7c238ae2b1decc08ab1f16a5ef",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/rvq.py",
    "kind": "function",
    "name": "__init__",
    "signature": "__init__(\n        self,\n        input_dim: int,\n        codebook_size: int,\n        codebook_dim: int,\n    )",
    "docstring": "",
    "identifiers": [
      "__init__",
      "bool",
      "clone",
      "codebook",
      "codebook_dim",
      "codebook_size",
      "dtype",
      "float",
      "identity",
      "in_project",
      "input_dim",
      "int",
      "kernel_size",
      "nn",
      "out_project",
      "register_buffer",
      "self",
      "super",
      "tensor",
      "torch",
      "wnconv1d",
      "zeros"
    ],
    "start_line": 17,
    "end_line": 54,
    "text": "def __init__(\n        self,\n        input_dim: int,\n        codebook_size: int,\n        codebook_dim: int,\n    ):\n        super().__init__()\n        self.input_dim = input_dim\n        self.codebook_size = codebook_size\n        self.codebook_dim = codebook_dim\n\n        self.in_project = (\n            WNConv1d(\n                self.input_dim, self.codebook_dim, kernel_size=1\n            )  # (B, D, T) -> (B, D', T)\n            if self.input_dim != self.codebook_dim\n            else nn.Identity()\n        )\n        self.out_project = (\n            WNConv1d(\n                self.codebook_dim, self.input_dim, kernel_size=1\n            )  # (B, D', T) -> (B, D, T)\n            if self.input_dim != self.codebook_dim\n            else nn.Identity()\n        )\n\n        # Initialize codebook and EMA buffers\n        self.register_buffer(\n            \"codebook\", torch.zeros(codebook_size, codebook_dim).float()\n        )  # (codebook_size, D'), ensure fp32\n        # Place holder, not used in inference\n        self.register_buffer(\"inited\", torch.tensor([True], dtype=torch.bool))  # (1)\n        self.register_buffer(\n            \"cluster_size\", torch.zeros(codebook_size).float()\n        )  # (codebook_size), ensure fp32\n        self.register_buffer(\n            \"embed_avg\", self.codebook.clone().float()\n        )  # (codebook_size, D'), ensure fp32"
  },
  {
    "id": "c92e1c53176dce6fb04e7fd1f5d8824668891ef7",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/rvq.py",
    "kind": "function",
    "name": "WNConvTranspose1d",
    "signature": "WNConvTranspose1d(*args, **kwargs)",
    "docstring": "",
    "identifiers": [
      "args",
      "convtranspose1d",
      "kwargs",
      "nn",
      "weight_norm",
      "wnconvtranspose1d"
    ],
    "start_line": 12,
    "end_line": 13,
    "text": "def WNConvTranspose1d(*args, **kwargs):\n    return weight_norm(nn.ConvTranspose1d(*args, **kwargs))"
  },
  {
    "id": "5e641b9931bebfb0938d860cd0d6577d9f6969f4",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/fireredtts2/codec/rvq.py",
    "kind": "function",
    "name": "WNConv1d",
    "signature": "WNConv1d(*args, **kwargs)",
    "docstring": "",
    "identifiers": [
      "args",
      "conv1d",
      "kwargs",
      "nn",
      "weight_norm",
      "wnconv1d"
    ],
    "start_line": 8,
    "end_line": 9,
    "text": "def WNConv1d(*args, **kwargs):\n    return weight_norm(nn.Conv1d(*args, **kwargs))"
  },
  {
    "id": "a9b62df6b345454490fb2f1b3c77d53583922902",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/bin/finetune_example/posttrain.py",
    "kind": "function",
    "name": "train",
    "signature": "train(args: argparse.Namespace, config: dict, trial: optuna.Trial = None)",
    "docstring": "trial is only used when we are sweeping hyperparameters.",
    "identifiers": [
      "accelerator",
      "adamw",
      "append",
      "argparse",
      "args",
      "autocast",
      "backward",
      "batch_size",
      "c0_loss",
      "c_loss",
      "checkpoint_path",
      "clip_grad_norm_",
      "config",
      "create_dataloaders",
      "cuda",
      "current_device",
      "current_gpu",
      "device",
      "device_count",
      "dict",
      "eff_batch_size",
      "epoch",
      "float",
      "get_grad_norm",
      "get_state_dict",
      "global_step",
      "grad_norm",
      "infinite_train",
      "int",
      "is_main_process",
      "item",
      "join",
      "len",
      "load_model",
      "log_c0_loss",
      "log_c_loss",
      "log_dir",
      "log_loss",
      "log_text_loss",
      "logs_folder",
      "loss",
      "lr",
      "model",
      "n_gpus",
      "namespace",
      "ndigits",
      "num_workers",
      "optim",
      "optimizer",
      "optuna",
      "os",
      "param_groups",
      "parameters",
      "path",
      "prepare",
      "print",
      "range",
      "real_step",
      "round",
      "save",
      "scalar_dict",
      "scalars",
      "scheduler",
      "state",
      "state_dict",
      "step",
      "sum",
      "summarize",
      "summarywriter",
      "text_loss",
      "to",
      "tokens",
      "tokens_mask",
      "torch",
      "total_c0_loss",
      "total_c_loss",
      "total_loss",
      "total_step",
      "total_steps",
      "total_text_loss",
      "train",
      "train_datasets",
      "trainloader",
      "trial",
      "validation_datasets",
      "valloader",
      "wait_for_everyone",
      "warmupdecaylr",
      "weight_decay",
      "writer",
      "zero_grad"
    ],
    "start_line": 22,
    "end_line": 206,
    "text": "def train(args: argparse.Namespace, config: dict, trial: optuna.Trial = None):\n    \"\"\"\n    trial is only used when we are sweeping hyperparameters.\n    \"\"\"\n\n    # accelerator\n    accelerator = Accelerator()\n    current_gpu = int(torch.cuda.current_device())\n    device = accelerator.device  # \"cuda\"\n    n_gpus = torch.cuda.device_count()\n    print(\n        f\"---Number of GPUs: {n_gpus}\",\n        \"---current_gpu:\",\n        current_gpu,\n        \"---device:\",\n        device,\n    )\n\n    # prepare log\n    logs_folder = config[\"train\"][\"logs_folder\"]\n    if accelerator.is_main_process:\n        writer = SummaryWriter(log_dir=logs_folder)\n\n    print(\"---Load LLM Model...\")\n    model = load_model(config, args.checkpoint_path, device)\n\n    trainloader, valloader = create_dataloaders(\n        train_datasets=config[\"dataset\"][\"train_dataset_dir\"],\n        validation_datasets=config[\"dataset\"][\"valid_dataset_dir\"],\n        batch_size=config[\"train\"][\"batch_size\"],\n        device=device,\n        infinite_train=False,\n        num_workers=8,\n    )\n\n    eff_batch_size = config[\"train\"][\"batch_size\"] * config[\"train\"][\"accumulate_num\"]\n\n    total_steps = (config[\"train\"][\"n_epochs\"] * len(trainloader)) // config[\"train\"][\n        \"accumulate_num\"\n    ]\n    print(\"---total_steps:\", total_steps)\n\n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=config[\"train\"][\"lr\"],\n        weight_decay=config[\"train\"][\"weight_decay\"],\n    )\n    scheduler = WarmupDecayLR(\n        optimizer,\n        config[\"train\"][\"warmup_steps\"],\n        total_steps,\n        config[\"train\"][\"lr_decay\"],\n    )\n\n    state = {\n        \"model\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n        \"scheduler\": scheduler.state_dict(),\n        \"effective_batch_size\": eff_batch_size,\n        \"config\": config,\n        \"args\": args,\n        \"best_val_loss\": float(\"inf\"),\n    }\n\n    # accelerate prepare\n    (model, trainloader, optimizer, scheduler) = accelerator.prepare(\n        model, trainloader, optimizer, scheduler\n    )\n\n    # Training loop\n    total_step = 1\n    real_step = 1\n    model.train()\n    for epoch in range(config[\"train\"][\"n_epochs\"]):\n        step = 1\n        total_loss = 0.0\n        total_text_loss = 0.0\n        total_c0_loss = 0.0\n        total_c_loss = 0.0\n        log_loss = []\n        log_text_loss = []\n        log_c0_loss = []\n        log_c_loss = []\n        for tokens, tokens_mask in trainloader:\n            tokens, tokens_mask = tokens.to(device), tokens_mask.to(device)\n            with accelerator.autocast():\n                loss, text_loss, c0_loss, c_loss = model(tokens, tokens_mask)\n                loss = loss / config[\"train\"][\"accumulate_num\"]\n\n                # only for logs\n                text_loss = text_loss / config[\"train\"][\"accumulate_num\"]\n                c0_loss = c0_loss / config[\"train\"][\"accumulate_num\"]\n                c_loss = c_loss / config[\"train\"][\"accumulate_num\"]\n\n                total_loss += loss.item()\n                total_text_loss += text_loss.item()\n                total_c0_loss += c0_loss.item()\n                total_c_loss += c_loss.item()\n\n            # 梯度传导\n            accelerator.backward(loss)\n\n            # 到达累积步数时一次更新\n            if (real_step) % config[\"train\"][\"accumulate_num\"] == 0:\n                grad_norm = get_grad_norm(model=model)\n                accelerator.clip_grad_norm_(\n                    model.parameters(), config[\"train\"][\"max_grad_norm\"]\n                )\n                accelerator.wait_for_everyone()\n                optimizer.step()\n                optimizer.zero_grad()\n                scheduler.step()\n                accelerator.wait_for_everyone()\n\n                if accelerator.is_main_process:\n                    log_loss.append(total_loss)\n                    log_text_loss.append(total_text_loss)\n                    log_c0_loss.append(total_c0_loss)\n                    log_c_loss.append(total_c_loss)\n\n                    print(\n                        \"--"
  },
  {
    "id": "54bbaf8717886f3dd07d514bc487075263e9c8f4",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/bin/finetune_example/posttrain_dataloader.py",
    "kind": "function",
    "name": "create_dataloaders",
    "signature": "create_dataloaders(\n    train_datasets: str,\n    validation_datasets: str,\n    batch_size: int,\n    device,  # for debug\n    infinite_train: bool = False,\n    num_workers: int = 0,\n)",
    "docstring": "",
    "identifiers": [
      "batch_size",
      "bool",
      "collate_fn",
      "create_dataloaders",
      "dataloader",
      "dataset_dir",
      "device",
      "infinite_train",
      "int",
      "num_workers",
      "pin_memory",
      "shuffle",
      "str",
      "tokenizeddataset",
      "train_datasets",
      "trainloader",
      "trainset",
      "validation_datasets",
      "valloader",
      "valset"
    ],
    "start_line": 288,
    "end_line": 319,
    "text": "def create_dataloaders(\n    train_datasets: str,\n    validation_datasets: str,\n    batch_size: int,\n    device,  # for debug\n    infinite_train: bool = False,\n    num_workers: int = 0,\n):\n\n    trainset = TokenizedDataset(dataset_dir=train_datasets, device=device)\n    valset = TokenizedDataset(dataset_dir=validation_datasets, device=device)\n    trainloader = DataLoader(\n        trainset,\n        # batch_sampler=trainsampler,\n        batch_size=batch_size,\n        collate_fn=collate_fn,\n        num_workers=12,\n        pin_memory=True,\n        shuffle=True,\n    )\n\n    valloader = DataLoader(\n        valset,\n        # batch_sampler=valsampler,\n        batch_size=batch_size,\n        collate_fn=collate_fn,\n        num_workers=num_workers,\n        pin_memory=True,\n        shuffle=False,\n    )\n\n    return trainloader, valloader"
  },
  {
    "id": "30116e70c6352bd74cf61c064122f8a3b1c0a2b6",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/bin/finetune_example/posttrain_dataloader.py",
    "kind": "class",
    "name": "BucketSampler",
    "signature": "class BucketSampler(Sampler)",
    "docstring": "Groups samples of similar lengths into bins to minimize padding.",
    "identifiers": [
      "_",
      "__init__",
      "__iter__",
      "__len__",
      "_create_bins",
      "_shuffle_bins",
      "append",
      "batch_size",
      "bin_",
      "bin_indices",
      "bins",
      "bool",
      "bucketsampler",
      "current_bin",
      "enumerate",
      "epoch",
      "idx",
      "indices_with_lengths",
      "int",
      "is_infinite",
      "key",
      "len",
      "lengths",
      "list",
      "local_step",
      "np",
      "random",
      "random_seed",
      "randomstate",
      "rng",
      "sampler",
      "self",
      "shuffle",
      "sorted",
      "x"
    ],
    "start_line": 231,
    "end_line": 285,
    "text": "class BucketSampler(Sampler):\n    \"\"\"\n    Groups samples of similar lengths into bins to minimize padding.\n    \"\"\"\n\n    def __init__(\n        self,\n        lengths: List[int],\n        batch_size: int,\n        shuffle: bool = True,\n        is_infinite: bool = True,\n        random_seed: int = 42,\n    ):\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.is_infinite = is_infinite\n        self.random_seed = random_seed\n        self.local_step = 0\n        self.bins = self._create_bins(lengths, batch_size)\n\n    def _create_bins(self, lengths: List[int], batch_size: int) -> List[List[int]]:\n        indices_with_lengths = sorted(enumerate(lengths), key=lambda x: x[1])\n        bins, current_bin = [], []\n\n        for idx, _ in indices_with_lengths:\n            current_bin.append(idx)\n            if len(current_bin) >= batch_size:\n                bins.append(current_bin)\n                current_bin = []\n\n        if current_bin:\n            bins.append(current_bin)\n\n        return bins\n\n    def _shuffle_bins(self, epoch: int):\n        rng = np.random.RandomState(epoch + self.random_seed)\n        rng.shuffle(self.bins)\n        for bin_ in self.bins:\n            rng.shuffle(bin_)\n\n    def __iter__(self):\n        epoch = 0\n        while True:\n            if self.shuffle:\n                self._shuffle_bins(epoch)\n            for bin_indices in self.bins:\n                yield bin_indices\n                self.local_step += 1\n            if not self.is_infinite:\n                break\n            epoch += 1\n\n    def __len__(self):\n        return len(self.bins)"
  },
  {
    "id": "4bc2fc88f2ca40cd60ec00d253574fb73f7f3df1",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/bin/finetune_example/posttrain_dataloader.py",
    "kind": "function",
    "name": "__len__",
    "signature": "__len__(self)",
    "docstring": "",
    "identifiers": [
      "__len__",
      "bins",
      "len",
      "self"
    ],
    "start_line": 284,
    "end_line": 285,
    "text": "def __len__(self):\n        return len(self.bins)"
  },
  {
    "id": "09075ef6a31f9eef922e60a6f32b349385a1d45e",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/bin/finetune_example/posttrain_dataloader.py",
    "kind": "function",
    "name": "__iter__",
    "signature": "__iter__(self)",
    "docstring": "",
    "identifiers": [
      "__iter__",
      "_shuffle_bins",
      "bin_indices",
      "bins",
      "epoch",
      "is_infinite",
      "local_step",
      "self",
      "shuffle"
    ],
    "start_line": 272,
    "end_line": 282,
    "text": "def __iter__(self):\n        epoch = 0\n        while True:\n            if self.shuffle:\n                self._shuffle_bins(epoch)\n            for bin_indices in self.bins:\n                yield bin_indices\n                self.local_step += 1\n            if not self.is_infinite:\n                break\n            epoch += 1"
  },
  {
    "id": "105b3a440bb96e4664cc6ef7a6f90e0c3bfdda44",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/bin/finetune_example/posttrain_dataloader.py",
    "kind": "function",
    "name": "_shuffle_bins",
    "signature": "_shuffle_bins(self, epoch: int)",
    "docstring": "",
    "identifiers": [
      "_shuffle_bins",
      "bin_",
      "bins",
      "epoch",
      "int",
      "np",
      "random",
      "random_seed",
      "randomstate",
      "rng",
      "self",
      "shuffle"
    ],
    "start_line": 266,
    "end_line": 270,
    "text": "def _shuffle_bins(self, epoch: int):\n        rng = np.random.RandomState(epoch + self.random_seed)\n        rng.shuffle(self.bins)\n        for bin_ in self.bins:\n            rng.shuffle(bin_)"
  },
  {
    "id": "93444344ec5dc35ff00f5b1f5775974652c57036",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/bin/finetune_example/posttrain_dataloader.py",
    "kind": "function",
    "name": "_create_bins",
    "signature": "_create_bins(self, lengths: List[int], batch_size: int)",
    "docstring": "",
    "identifiers": [
      "_",
      "_create_bins",
      "append",
      "batch_size",
      "bins",
      "current_bin",
      "enumerate",
      "idx",
      "indices_with_lengths",
      "int",
      "key",
      "len",
      "lengths",
      "list",
      "self",
      "sorted",
      "x"
    ],
    "start_line": 251,
    "end_line": 264,
    "text": "def _create_bins(self, lengths: List[int], batch_size: int) -> List[List[int]]:\n        indices_with_lengths = sorted(enumerate(lengths), key=lambda x: x[1])\n        bins, current_bin = [], []\n\n        for idx, _ in indices_with_lengths:\n            current_bin.append(idx)\n            if len(current_bin) >= batch_size:\n                bins.append(current_bin)\n                current_bin = []\n\n        if current_bin:\n            bins.append(current_bin)\n\n        return bins"
  },
  {
    "id": "72522df67f3b8963e465ee2727eeb0d5503241e3",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/bin/finetune_example/posttrain_dataloader.py",
    "kind": "function",
    "name": "__init__",
    "signature": "__init__(\n        self,\n        lengths: List[int],\n        batch_size: int,\n        shuffle: bool = True,\n        is_infinite: bool = True,\n        random_seed: int = 42,\n    )",
    "docstring": "",
    "identifiers": [
      "__init__",
      "_create_bins",
      "batch_size",
      "bins",
      "bool",
      "int",
      "is_infinite",
      "lengths",
      "list",
      "local_step",
      "random_seed",
      "self",
      "shuffle"
    ],
    "start_line": 236,
    "end_line": 249,
    "text": "def __init__(\n        self,\n        lengths: List[int],\n        batch_size: int,\n        shuffle: bool = True,\n        is_infinite: bool = True,\n        random_seed: int = 42,\n    ):\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.is_infinite = is_infinite\n        self.random_seed = random_seed\n        self.local_step = 0\n        self.bins = self._create_bins(lengths, batch_size)"
  },
  {
    "id": "901b341ac7774915b513cdf9e3c64ff971ab853d",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/bin/finetune_example/posttrain_dataloader.py",
    "kind": "function",
    "name": "collate_fn",
    "signature": "collate_fn(batch: List[dict])",
    "docstring": "Collate function for tokenized audio and text.\n    Merges variable-length audio/text into a single padded tensor.",
    "identifiers": [
      "append",
      "audio_segment_len",
      "audio_tokens",
      "audio_tokens_flatten",
      "batch",
      "batch_first",
      "collate_fn",
      "dict",
      "interleave",
      "interleave_masks_concat",
      "interleave_tokens_concat",
      "item",
      "list",
      "pad_sequence",
      "padding_value",
      "text_segment_len",
      "text_tokens",
      "tokens",
      "tokens_mask"
    ],
    "start_line": 200,
    "end_line": 228,
    "text": "def collate_fn(batch: List[dict]):\n    \"\"\"\n    Collate function for tokenized audio and text.\n    Merges variable-length audio/text into a single padded tensor.\n    \"\"\"\n    tokens, tokens_mask = [], []\n\n    for item in batch:\n        audio_tokens = item[\"audio\"]  # [audio_seq_len*16]\n        text_tokens = item[\"text\"]  # [text_seq_len]\n        audio_segment_len = item[\"audio_segment_len\"]\n        text_segment_len = item[\"text_segment_len\"]\n\n        # interleave format\n        interleave_tokens_concat, interleave_masks_concat = interleave(\n            audio_tokens_flatten=audio_tokens,\n            text_tokens=text_tokens,\n            audio_segment_len=audio_segment_len,\n            text_segment_len=text_segment_len,\n        )\n\n        # Concatenate and collect\n        tokens.append(interleave_tokens_concat)\n        tokens_mask.append(interleave_masks_concat)\n\n    tokens = pad_sequence(tokens, batch_first=True)\n    tokens_mask = pad_sequence(tokens_mask, batch_first=True, padding_value=False)\n\n    return tokens, tokens_mask"
  },
  {
    "id": "b9c1892ce9136eaa658e328407372430257627ef",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/bin/finetune_example/posttrain_dataloader.py",
    "kind": "function",
    "name": "interleave",
    "signature": "interleave(audio_tokens_flatten, text_tokens, audio_segment_len, text_segment_len)",
    "docstring": "_summary_\n\n    Args:\n        audio_tokens_flatten (_type_): _description_\n        text_tokens (_type_): _description_\n        audio_segment_len (_type_): _description_\n        text_segment_len (_type_): _description_",
    "identifiers": [
      "append",
      "audio_frame",
      "audio_frame_mask",
      "audio_max_len",
      "audio_num_codebooks",
      "audio_segment_index",
      "audio_segment_len",
      "audio_segment_tokens",
      "audio_tokens_flatten",
      "bool",
      "cat",
      "dim",
      "dtype",
      "end_index",
      "eos_frame",
      "get_index",
      "i",
      "interleave",
      "interleave_masks",
      "interleave_masks_concat",
      "interleave_tokens",
      "interleave_tokens_concat",
      "len",
      "len_recorder",
      "long",
      "max_segments",
      "new_total_audio_len",
      "print",
      "randint",
      "random",
      "random_num",
      "range",
      "reshape",
      "shape",
      "size",
      "start_index",
      "tensors",
      "text_frame",
      "text_frame_mask",
      "text_segment_index",
      "text_segment_len",
      "text_segment_tokens",
      "text_tokens",
      "torch",
      "total_audio_len",
      "total_max_len",
      "transpose",
      "zeros"
    ],
    "start_line": 94,
    "end_line": 197,
    "text": "def interleave(audio_tokens_flatten, text_tokens, audio_segment_len, text_segment_len):\n    \"\"\"_summary_\n\n    Args:\n        audio_tokens_flatten (_type_): _description_\n        text_tokens (_type_): _description_\n        audio_segment_len (_type_): _description_\n        text_segment_len (_type_): _description_\n    \"\"\"\n    AUDIO_MAX_LEN = 2300\n    TOTAL_MAX_LEN = 3100\n\n    # step1.\n    audio_segment_index, text_segment_index = get_index(\n        audio_segment_len, text_segment_len\n    )\n\n    # step2.\n    MAX_SEGMENTS = 16\n    if len(audio_segment_index) > MAX_SEGMENTS:\n        start_index = random.randint(0, len(audio_segment_index) - MAX_SEGMENTS)\n        end_index = start_index + MAX_SEGMENTS\n        audio_segment_index = audio_segment_index[start_index:end_index]\n        text_segment_index = text_segment_index[start_index:end_index]\n\n    total_audio_len = (\n        audio_segment_index[-1][-1] - audio_segment_index[0][0]\n    ) // AUDIO_NUM_CODEBOOKS\n\n    # step3.\n    if total_audio_len > AUDIO_MAX_LEN:\n        start_index = 0\n        end_index = len(audio_segment_index) - 1\n        while True:\n            random_num = random.random()\n            if random_num <= 0.5:\n                start_index += 1\n            else:\n                end_index -= 1\n\n            new_total_audio_len = (\n                audio_segment_index[end_index][-1] - audio_segment_index[start_index][0]\n            ) // AUDIO_NUM_CODEBOOKS\n\n            if start_index == end_index:\n                print(\"start_index == end_index\", start_index, \"==\", end_index)\n                break\n\n            if new_total_audio_len <= AUDIO_MAX_LEN:\n                audio_segment_index = audio_segment_index[start_index:end_index]\n                text_segment_index = text_segment_index[start_index:end_index]\n                break\n\n    # step4.\n    interleave_tokens = []\n    interleave_masks = []\n    len_recorder = 0\n    for i in range(len(audio_segment_index)):\n        text_segment_tokens = text_tokens[\n            text_segment_index[i][0] : text_segment_index[i][1]\n        ]\n        audio_segment_tokens = audio_tokens_flatten[\n            audio_segment_index[i][0] : audio_segment_index[i][1]\n        ].reshape([AUDIO_NUM_CODEBOOKS, -1])\n\n        len_recorder += text_segment_tokens.shape[0] + audio_segment_tokens.shape[1]\n        if len_recorder > TOTAL_MAX_LEN:\n            break\n\n        # Add EOS frame to audio\n        eos_frame = torch.zeros(audio_segment_tokens.size(0), 1, dtype=torch.long)\n        audio_segment_tokens = torch.cat([audio_segment_tokens, eos_frame], dim=1)\n\n        # extra dimension is for text tokens\n        audio_frame = torch.zeros(\n            audio_segment_tokens.size(1), AUDIO_NUM_CODEBOOKS + 1\n        ).long()\n        audio_frame[:, :-1] = audio_segment_tokens.transpose(0, 1)\n        audio_frame_mask = torch.zeros(\n            audio_segment_tokens.size(1), AUDIO_NUM_CODEBOOKS + 1\n        ).bool()\n        audio_frame_mask[:, :-1] = True\n\n        # Format text frame with same shape\n        text_frame = torch.zeros(\n            len(text_segment_tokens), AUDIO_NUM_CODEBOOKS + 1\n        ).long()\n        text_frame[:, -1] = text_segment_tokens\n        text_frame_mask = torch.zeros(\n            len(text_segment_tokens), AUDIO_NUM_CODEBOOKS + 1\n        ).bool()\n        text_frame_mask[:, -1] = True\n\n        interleave_tokens.append(text_frame)\n        interleave_tokens.append(audio_frame)\n\n        interleave_masks.append(text_frame_mask)\n        interleave_masks.append(audio_frame_mask)\n\n    # concat interleave\n    interleave_tokens_concat = torch.cat(tensors=interleave_tokens, dim=0)\n    interleave_masks_concat = torch.cat(tensors=interleave_masks, dim=0)\n\n    return interleave_tokens_concat, interleave_masks_concat"
  },
  {
    "id": "9d789270a8b09f3b555e6545e5a201b91c4e2ad1",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/bin/finetune_example/posttrain_dataloader.py",
    "kind": "function",
    "name": "get_index",
    "signature": "get_index(audio_segment_len, text_segment_len)",
    "docstring": "_summary_\n\n    Args:\n        audio_segment_len (_type_): _description_\n        text_segment_len (_type_): _description_",
    "identifiers": [
      "append",
      "audio_segment_index",
      "audio_segment_len",
      "end_audio_segment_index",
      "end_text_segment_index",
      "get_index",
      "i",
      "len",
      "range",
      "start_audio_segment_index",
      "start_text_segment_index",
      "text_segment_index",
      "text_segment_len"
    ],
    "start_line": 67,
    "end_line": 91,
    "text": "def get_index(audio_segment_len, text_segment_len):\n    \"\"\"_summary_\n\n    Args:\n        audio_segment_len (_type_): _description_\n        text_segment_len (_type_): _description_\n    \"\"\"\n    audio_segment_index = []\n    text_segment_index = []\n\n    start_audio_segment_index = 0\n    start_text_segment_index = 0\n\n    for i in range(len(audio_segment_len)):\n        end_audio_segment_index = start_audio_segment_index + audio_segment_len[i]\n        end_text_segment_index = start_text_segment_index + text_segment_len[i]\n\n        # 记录index\n        audio_segment_index.append([start_audio_segment_index, end_audio_segment_index])\n        text_segment_index.append([start_text_segment_index, end_text_segment_index])\n\n        start_audio_segment_index = end_audio_segment_index\n        start_text_segment_index = end_text_segment_index\n\n    return audio_segment_index, text_segment_index"
  },
  {
    "id": "3d294edf98b3d3769926164b6ed067cf03541fae",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/bin/finetune_example/posttrain_dataloader.py",
    "kind": "class",
    "name": "TokenizedDataset",
    "signature": "class TokenizedDataset(Dataset)",
    "docstring": ".arrow-backed dataset for tokenized audio and text samples.\n    Assumes audio is saved as flat vlen int32 arrays (flattened [n_codebooks, seq_len]).",
    "identifiers": [
      "__getitem__",
      "__init__",
      "__len__",
      "audio_segment_len",
      "dataset",
      "dataset_dir",
      "dataset_path",
      "device",
      "dtype",
      "flat_audio",
      "get_seq_len",
      "hf_dataset",
      "idx",
      "int",
      "load_from_disk",
      "long",
      "num_rows",
      "print",
      "self",
      "seq_lengths",
      "str",
      "tensor",
      "text",
      "text_segment_len",
      "tokenizeddataset",
      "torch"
    ],
    "start_line": 20,
    "end_line": 64,
    "text": "class TokenizedDataset(Dataset):\n    \"\"\"\n    .arrow-backed dataset for tokenized audio and text samples.\n    Assumes audio is saved as flat vlen int32 arrays (flattened [n_codebooks, seq_len]).\n    \"\"\"\n\n    def __init__(self, dataset_dir: str, device):\n        \"\"\"_summary_\n\n        Args:\n            dataset_dir (str): _description_\n            device (_type_): for debug\n        \"\"\"\n        self.dataset_dir = dataset_dir\n        self.hf_dataset = HF_Dataset.load_from_disk(dataset_path=dataset_dir)\n        print(\"---dataset_info:\\n\", self.hf_dataset)\n        print(\"---num_rows:\", self.hf_dataset.num_rows)\n\n        self.device = device\n\n    def __len__(self):\n        return self.hf_dataset.num_rows\n\n    def get_seq_len(self):\n        seq_lengths = self.hf_dataset[:][\"len\"]\n        return seq_lengths\n\n    def __getitem__(self, idx: int):\n        # print(\"---device:\", self.device, \"---idx:\", idx)\n\n        flat_audio = self.hf_dataset[idx][\"audio\"]\n        text = self.hf_dataset[idx][\"text\"]\n        audio_segment_len = self.hf_dataset[idx][\"audio_segment_len\"]\n        text_segment_len = self.hf_dataset[idx][\"text_segment_len\"]\n\n        # to tensor\n        flat_audio = torch.tensor(flat_audio, dtype=torch.long)\n        text = torch.tensor(text, dtype=torch.long)\n\n        return {\n            \"audio\": flat_audio,\n            \"text\": text,\n            \"audio_segment_len\": audio_segment_len,\n            \"text_segment_len\": text_segment_len,\n        }"
  },
  {
    "id": "6bee73f5bfe559a8c86ee3c4baccb6ac7dd49911",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/bin/finetune_example/posttrain_dataloader.py",
    "kind": "function",
    "name": "__getitem__",
    "signature": "__getitem__(self, idx: int)",
    "docstring": "",
    "identifiers": [
      "__getitem__",
      "audio_segment_len",
      "dtype",
      "flat_audio",
      "hf_dataset",
      "idx",
      "int",
      "long",
      "self",
      "tensor",
      "text",
      "text_segment_len",
      "torch"
    ],
    "start_line": 47,
    "end_line": 64,
    "text": "def __getitem__(self, idx: int):\n        # print(\"---device:\", self.device, \"---idx:\", idx)\n\n        flat_audio = self.hf_dataset[idx][\"audio\"]\n        text = self.hf_dataset[idx][\"text\"]\n        audio_segment_len = self.hf_dataset[idx][\"audio_segment_len\"]\n        text_segment_len = self.hf_dataset[idx][\"text_segment_len\"]\n\n        # to tensor\n        flat_audio = torch.tensor(flat_audio, dtype=torch.long)\n        text = torch.tensor(text, dtype=torch.long)\n\n        return {\n            \"audio\": flat_audio,\n            \"text\": text,\n            \"audio_segment_len\": audio_segment_len,\n            \"text_segment_len\": text_segment_len,\n        }"
  },
  {
    "id": "f8ebd22599ac7b9fd15c00e2b6e70cb7254ea8ce",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/bin/finetune_example/posttrain_dataloader.py",
    "kind": "function",
    "name": "get_seq_len",
    "signature": "get_seq_len(self)",
    "docstring": "",
    "identifiers": [
      "get_seq_len",
      "hf_dataset",
      "self",
      "seq_lengths"
    ],
    "start_line": 43,
    "end_line": 45,
    "text": "def get_seq_len(self):\n        seq_lengths = self.hf_dataset[:][\"len\"]\n        return seq_lengths"
  },
  {
    "id": "93e339cfbbcd3f9652c8a8214506d06894125b90",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/bin/finetune_example/posttrain_dataloader.py",
    "kind": "function",
    "name": "__len__",
    "signature": "__len__(self)",
    "docstring": "",
    "identifiers": [
      "__len__",
      "hf_dataset",
      "num_rows",
      "self"
    ],
    "start_line": 40,
    "end_line": 41,
    "text": "def __len__(self):\n        return self.hf_dataset.num_rows"
  },
  {
    "id": "49eb46d3a852efecbb2e3d1a1f79925edac4c37a",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/bin/finetune_example/posttrain_dataloader.py",
    "kind": "function",
    "name": "__init__",
    "signature": "__init__(self, dataset_dir: str, device)",
    "docstring": "_summary_\n\n        Args:\n            dataset_dir (str): _description_\n            device (_type_): for debug",
    "identifiers": [
      "__init__",
      "dataset_dir",
      "dataset_path",
      "device",
      "hf_dataset",
      "load_from_disk",
      "num_rows",
      "print",
      "self",
      "str"
    ],
    "start_line": 26,
    "end_line": 38,
    "text": "def __init__(self, dataset_dir: str, device):\n        \"\"\"_summary_\n\n        Args:\n            dataset_dir (str): _description_\n            device (_type_): for debug\n        \"\"\"\n        self.dataset_dir = dataset_dir\n        self.hf_dataset = HF_Dataset.load_from_disk(dataset_path=dataset_dir)\n        print(\"---dataset_info:\\n\", self.hf_dataset)\n        print(\"---num_rows:\", self.hf_dataset.num_rows)\n\n        self.device = device"
  },
  {
    "id": "169380d70a23c5963b759515ccddb86fcd935b8e",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/bin/finetune_example/data_preparation/step1_create_meta.py",
    "kind": "function",
    "name": "create_meta",
    "signature": "create_meta(data_dir, out_jsonl)",
    "docstring": "_summary_\n\n    Args:\n        data_dir (_type_): LJspeech root path\n        out_jsonl (_type_): output meta",
    "identifiers": [
      "append",
      "close",
      "create_meta",
      "data_dir",
      "dump",
      "dur",
      "ensure_ascii",
      "exists",
      "f_in",
      "f_out",
      "file",
      "filename",
      "full_path",
      "get_duration",
      "get_uttrid2path",
      "join",
      "json",
      "librosa",
      "line",
      "lines",
      "meta_path",
      "mode",
      "open",
      "os",
      "out_dict",
      "out_jsonl",
      "path",
      "readlines",
      "segment_dict",
      "segments",
      "sep",
      "split",
      "strip",
      "text",
      "tn_text",
      "tqdm",
      "uttr_id",
      "uttrid2path",
      "wav_dir",
      "write"
    ],
    "start_line": 29,
    "end_line": 78,
    "text": "def create_meta(data_dir, out_jsonl):\n    \"\"\"_summary_\n\n    Args:\n        data_dir (_type_): LJspeech root path\n        out_jsonl (_type_): output meta\n    \"\"\"\n    wav_dir = os.path.join(data_dir, \"wavs\")\n    meta_path = os.path.join(data_dir, \"metadata.csv\")\n    assert os.path.exists(wav_dir)\n    assert os.path.exists(meta_path)\n\n    uttrid2path = get_uttrid2path(wav_dir=wav_dir)\n\n    # print(\"---uttrid2path:\\n\", uttrid2path)\n\n    f_in = open(meta_path)\n    lines = f_in.readlines()\n    f_in.close()\n\n    f_out = open(file=out_jsonl, mode=\"w\")\n\n    for line in tqdm(lines):\n        out_dict = {}\n        uttr_id, text, tn_text = line.strip().split(sep=\"|\")\n        # debug\n        # print(\"---uttr_id:\", uttr_id)\n        # print(\"---text:\", text)\n        # print(\"---tn_text:\", tn_text)\n\n        # segment\n        segments = []\n        if uttr_id in uttrid2path:\n            segment_dict = {}\n\n            full_path = uttrid2path[uttr_id]\n            dur = librosa.get_duration(filename=full_path)\n\n            segment_dict[\"duration\"] = dur\n            segment_dict[\"audio_path\"] = full_path\n            segment_dict[\"speaker\"] = \"[S_DIALOG_1]\"\n            segment_dict[\"text\"] = tn_text\n            segments.append(segment_dict)\n\n            out_dict[\"segments\"] = segments\n\n            json.dump(out_dict, f_out, ensure_ascii=False)\n            f_out.write(\"\\n\")\n\n    f_out.close()"
  },
  {
    "id": "fd3bb6096e501193283c236b1300065d265abb2b",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/bin/finetune_example/data_preparation/step1_create_meta.py",
    "kind": "function",
    "name": "get_uttrid2path",
    "signature": "get_uttrid2path(wav_dir)",
    "docstring": "_summary_\n\n    Args:\n        wav_dir (_type_): _description_",
    "identifiers": [
      "full_path",
      "get_uttrid2path",
      "join",
      "listdir",
      "os",
      "path",
      "sep",
      "split",
      "uttr_id",
      "uttrid2path",
      "wav_dir",
      "wav_file",
      "wav_files"
    ],
    "start_line": 13,
    "end_line": 26,
    "text": "def get_uttrid2path(wav_dir):\n    \"\"\"_summary_\n\n    Args:\n        wav_dir (_type_): _description_\n    \"\"\"\n    uttrid2path = {}\n    wav_files = os.listdir(wav_dir)\n    for wav_file in wav_files:\n        uttr_id = wav_file.split(sep=\".\")[0]\n        full_path = os.path.join(wav_dir, wav_file)\n        uttrid2path[uttr_id] = full_path\n\n    return uttrid2path"
  },
  {
    "id": "1e4d335318cc2107a9588ac22bda99063e1e0a40",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/bin/finetune_example/data_preparation/step2_extract_token.py",
    "kind": "function",
    "name": "extract_tokens",
    "signature": "extract_tokens(jsonl, pretrained_dir)",
    "docstring": "_summary_\n\n    Args:\n        jsonl (_type_): _description_\n        pretrained_dir (_type_): _description_",
    "identifiers": [
      "_load_one_audio",
      "abs",
      "audio16k",
      "audio16k_length",
      "audio_path",
      "batch_size",
      "close",
      "codec_ckpt_path",
      "codec_config_path",
      "cpu",
      "data_dict",
      "data_list",
      "desire_len",
      "device",
      "dtype",
      "dump",
      "duration",
      "encode",
      "encoding",
      "ensure_ascii",
      "eval",
      "exists",
      "extract_tokens",
      "f_out",
      "from_pretrained",
      "input_jsonl_file",
      "int",
      "join",
      "json",
      "jsonl",
      "long",
      "numpy",
      "one_token",
      "open",
      "os",
      "output_jsonl_file",
      "path",
      "pretrained_dir",
      "print",
      "read_jsonl",
      "redcodecinfer",
      "segment",
      "shape",
      "squeeze",
      "tensor",
      "to",
      "token",
      "token_length",
      "tolist",
      "torch",
      "torch_codec",
      "tqdm",
      "write"
    ],
    "start_line": 54,
    "end_line": 101,
    "text": "def extract_tokens(jsonl, pretrained_dir):\n    \"\"\"_summary_\n\n    Args:\n        jsonl (_type_): _description_\n        pretrained_dir (_type_): _description_\n    \"\"\"\n    codec_config_path = os.path.join(pretrained_dir, \"config_codec.json\")\n    codec_ckpt_path = os.path.join(pretrained_dir, \"codec.pt\")\n    assert os.path.exists(codec_config_path)\n    assert os.path.exists(codec_ckpt_path)\n\n    # ==== Load Torch Audio Tokenizer ====\n    device = torch.device(\"cuda\")\n    torch_codec = RedCodecInfer.from_pretrained(codec_config_path, codec_ckpt_path)\n    torch_codec.eval()\n    torch_codec = torch_codec.to(device)\n    print(\"[INFO] Codec Loaded...\")\n\n    input_jsonl_file = jsonl\n    output_jsonl_file = jsonl[:-6] + \"_token\" + \".jsonl\"\n    data_list = read_jsonl(path=input_jsonl_file)\n\n    f_out = open(output_jsonl_file, \"w\", encoding=\"utf-8\")\n\n    for data_dict in tqdm(data_list):\n        for segment in data_dict[\"segments\"]:\n            audio_path = segment[\"audio_path\"]\n            duration = segment[\"duration\"]\n            audio16k = _load_one_audio(audio_path=audio_path)\n            audio16k_length = torch.tensor([audio16k.shape[1]], dtype=torch.long)\n\n            token, token_length = torch_codec.encode(\n                audio16k.to(device), audio16k_length.to(device), batch_size=1\n            )\n\n            desire_len = int(duration * 12.5)\n            if abs(token.shape[-1] - desire_len) > 2:\n                print(\"---wrong token length,skip!!：\", audio_path)\n                continue\n\n            one_token = token.squeeze().cpu().numpy().tolist()\n            segment[\"audio_tokens\"] = one_token\n\n        json.dump(data_dict, f_out, ensure_ascii=False)\n        f_out.write(\"\\n\")\n\n    f_out.close()"
  },
  {
    "id": "42a74278efb83ef9d9d069efcdbfdecf9dba6e3b",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/bin/finetune_example/data_preparation/step2_extract_token.py",
    "kind": "function",
    "name": "_load_one_audio",
    "signature": "_load_one_audio(audio_path: str)",
    "docstring": "",
    "identifiers": [
      "_load_one_audio",
      "audio",
      "audio16k",
      "audio_path",
      "audio_sr",
      "functional",
      "load",
      "resample",
      "str",
      "torchaudio"
    ],
    "start_line": 48,
    "end_line": 51,
    "text": "def _load_one_audio(audio_path: str):\n    audio, audio_sr = torchaudio.load(audio_path)\n    audio16k = torchaudio.functional.resample(audio, audio_sr, 16000)\n    return audio16k"
  },
  {
    "id": "13c12eeb8a238ba71aa32bd73a50bbe386413667",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/bin/finetune_example/data_preparation/step2_extract_token.py",
    "kind": "function",
    "name": "split_list_into_chunks",
    "signature": "split_list_into_chunks(lst, chunk_size)",
    "docstring": "",
    "identifiers": [
      "chunk_size",
      "i",
      "len",
      "lst",
      "range",
      "split_list_into_chunks"
    ],
    "start_line": 44,
    "end_line": 45,
    "text": "def split_list_into_chunks(lst, chunk_size):\n    return [lst[i : i + chunk_size] for i in range(0, len(lst), chunk_size)]"
  },
  {
    "id": "a17cd394768d4a5eabddbbdb5d28b3a82d5447b2",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/bin/finetune_example/data_preparation/step2_extract_token.py",
    "kind": "function",
    "name": "read_jsonl",
    "signature": "read_jsonl(path)",
    "docstring": "",
    "identifiers": [
      "append",
      "data",
      "data_list",
      "expanduser",
      "f",
      "json",
      "json_str",
      "line",
      "loads",
      "open",
      "os",
      "path",
      "read",
      "read_jsonl",
      "splitlines"
    ],
    "start_line": 33,
    "end_line": 41,
    "text": "def read_jsonl(path):\n    path = os.path.expanduser(path)\n    with open(path, \"r\") as f:\n        json_str = f.read()\n    data_list = []\n    for line in json_str.splitlines():\n        data = json.loads(line)\n        data_list.append(data)\n    return data_list"
  },
  {
    "id": "d3354e00f74c921adfe89b473f1e3df16e396314",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/bin/finetune_example/data_preparation/step3_write_arrow.py",
    "kind": "function",
    "name": "read_test",
    "signature": "read_test(dataset_dir)",
    "docstring": "_summary_\n\n    Args:\n        dataset_dir (_type_): _description_",
    "identifiers": [
      "all_len",
      "audio_segment_len",
      "audio_tokens_flatten",
      "dataset_dir",
      "dataset_path",
      "ds",
      "dtype",
      "hf_dataset",
      "idx",
      "load_from_disk",
      "long",
      "num_rows",
      "print",
      "range",
      "read_test",
      "recovery_debug",
      "tensor",
      "text_segment_len",
      "text_tokens",
      "torch",
      "total_len"
    ],
    "start_line": 211,
    "end_line": 241,
    "text": "def read_test(dataset_dir):\n    \"\"\"_summary_\n\n    Args:\n        dataset_dir (_type_): _description_\n    \"\"\"\n    # read test\n    ds = HF_Dataset.load_from_disk(dataset_path=dataset_dir)\n    print(\"basic_info:\", ds)\n    print(\"---num_rows:\", ds.num_rows)\n    # print(\"ds[0]:\\n\", ds[0])\n\n    all_len = ds[:][\"len\"]\n    print(\"---all_len:\\n\", all_len)\n\n    idx = 1\n\n    for idx in range(ds.num_rows):\n\n        audio_tokens_flatten = torch.tensor(ds[idx][\"audio\"], dtype=torch.long)\n        text_tokens = torch.tensor(ds[idx][\"text\"], dtype=torch.long)\n        total_len = ds[idx][\"len\"]\n        audio_segment_len = ds[idx][\"audio_segment_len\"]\n        text_segment_len = ds[idx][\"text_segment_len\"]\n\n        recovery_debug(\n            audio_tokens_flatten=audio_tokens_flatten,\n            text_tokens=text_tokens,\n            audio_segment_len=audio_segment_len,\n            text_segment_len=text_segment_len,\n        )"
  },
  {
    "id": "fe8a0baddb5139e98a1e5e45d702a300c71a7c17",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/bin/finetune_example/data_preparation/step3_write_arrow.py",
    "kind": "function",
    "name": "recovery_debug",
    "signature": "recovery_debug(\n    audio_tokens_flatten, text_tokens, audio_segment_len, text_segment_len\n)",
    "docstring": "_summary_\n\n    Args:\n        audio_tokens_flatten (_type_): _description_\n        text_tokens (_type_): _description_\n        audio_segment_len (_type_): _description_\n        text_segment_len (_type_): _description_",
    "identifiers": [
      "append",
      "audio_segment_index",
      "audio_segment_len",
      "audio_segment_tokens",
      "audio_tokens_flatten",
      "end_audio_segment_index",
      "end_text_segment_index",
      "i",
      "len",
      "print",
      "range",
      "recovery_debug",
      "reshape",
      "shape",
      "start_audio_segment_index",
      "start_text_segment_index",
      "text_segment_index",
      "text_segment_len",
      "text_segment_tokens",
      "text_tokens"
    ],
    "start_line": 157,
    "end_line": 208,
    "text": "def recovery_debug(\n    audio_tokens_flatten, text_tokens, audio_segment_len, text_segment_len\n):\n    \"\"\"_summary_\n\n    Args:\n        audio_tokens_flatten (_type_): _description_\n        text_tokens (_type_): _description_\n        audio_segment_len (_type_): _description_\n        text_segment_len (_type_): _description_\n    \"\"\"\n    audio_segment_index = []\n    text_segment_index = []\n\n    start_audio_segment_index = 0\n    start_text_segment_index = 0\n\n    for i in range(len(audio_segment_len)):\n        end_audio_segment_index = start_audio_segment_index + audio_segment_len[i]\n        end_text_segment_index = start_text_segment_index + text_segment_len[i]\n        audio_segment_index.append([start_audio_segment_index, end_audio_segment_index])\n        text_segment_index.append([start_text_segment_index, end_text_segment_index])\n\n        start_audio_segment_index = end_audio_segment_index\n        start_text_segment_index = end_text_segment_index\n\n    print(\"---audio_segment_index:\\n\", audio_segment_index)\n    print(\"---text_segment_index:\\n\", text_segment_index)\n\n    # 复原tokens\n    for i in range(len(audio_segment_index)):\n        audio_segment_tokens = audio_tokens_flatten[\n            audio_segment_index[i][0] : audio_segment_index[i][1]\n        ].reshape([16, -1])\n\n        text_segment_tokens = text_tokens[\n            text_segment_index[i][0] : text_segment_index[i][1]\n        ]\n\n        print(\n            \"---audio_segment_tokens:\\n\",\n            audio_segment_tokens,\n            audio_segment_tokens.shape,\n        )\n\n        print(\n            \"---text_segment_tokens:\\n\",\n            text_segment_tokens,\n            text_segment_tokens.shape,\n        )\n\n    print(\"\\n\\n\\n\")"
  },
  {
    "id": "390fc1b77192789c78eb52f328bf84b94d1ba38d",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/bin/finetune_example/data_preparation/step3_write_arrow.py",
    "kind": "function",
    "name": "pack",
    "signature": "pack(jsonl, pretrained_dir, dataset_dir, prefix)",
    "docstring": "pack .arrow\n\n    Args:\n        jsonl (_type_): _description_\n        pretrained_dir (_type_): _description_\n        dataset_dir (_type_): _description_\n        prefix (_type_): _description_",
    "identifiers": [
      "append",
      "array",
      "arrow_writer",
      "arrowwriter",
      "audio_segment_len",
      "audio_text_ratio",
      "audio_token_list",
      "audio_tokens",
      "audio_tokens_flatten",
      "axis",
      "close",
      "column_features",
      "concatenate",
      "dataset_dir",
      "duration",
      "encode",
      "exists",
      "feature",
      "features",
      "finalize",
      "full_out_arrow_name",
      "get_speaker_dict",
      "i",
      "is_useful",
      "join",
      "jsonl",
      "len",
      "load_custom_tokenizer",
      "max_audio_duration",
      "max_text_token_len",
      "np",
      "os",
      "pack",
      "path",
      "prefix",
      "pretrained_dir",
      "pretrained_qwen_path",
      "print",
      "qwen2_tokenizer_path",
      "range",
      "read_jsonl",
      "reshape",
      "seg",
      "segments",
      "sequence",
      "shape",
      "speaker",
      "speaker_dict",
      "text",
      "text_segment_len",
      "text_token_list",
      "text_tokenizer",
      "text_tokens",
      "token_data",
      "token_data_list",
      "total_len",
      "tqdm",
      "value",
      "write"
    ],
    "start_line": 58,
    "end_line": 154,
    "text": "def pack(jsonl, pretrained_dir, dataset_dir, prefix):\n    \"\"\"pack .arrow\n\n    Args:\n        jsonl (_type_): _description_\n        pretrained_dir (_type_): _description_\n        dataset_dir (_type_): _description_\n        prefix (_type_): _description_\n    \"\"\"\n    # load tokenizers\n    pretrained_qwen_path = os.path.join(pretrained_dir, \"Qwen2.5-1.5B\")\n    assert os.path.exists(pretrained_qwen_path)\n    text_tokenizer = load_custom_tokenizer(qwen2_tokenizer_path=pretrained_qwen_path)\n    full_out_arrow_name = os.path.join(dataset_dir, prefix + \".arrow\")\n\n    # arrow definition\n    column_features = Features(\n        {\n            \"audio\": Sequence(feature=Value(\"int64\")),\n            \"text\": Sequence(feature=Value(\"int64\")),\n            \"len\": Value(\"int64\"),\n            \"audio_segment_len\": Sequence(feature=Value(\"int64\")),\n            \"text_segment_len\": Sequence(feature=Value(\"int64\")),\n        }\n    )\n    arrow_writer = ArrowWriter(path=full_out_arrow_name, features=column_features)\n    token_data_list = read_jsonl(path=jsonl)\n    for i in tqdm(range(len(token_data_list))):\n        token_data = token_data_list[i]\n        is_useful = True\n        segments = token_data[\"segments\"]\n\n        # Find all speakers\n        speaker_dict = get_speaker_dict(segments=segments)\n        if len(speaker_dict) > 5:\n            print(\"Skipping: too many speakers....\")\n            is_useful = False\n\n        audio_token_list = []\n        text_token_list = []\n        audio_segment_len = []\n        text_segment_len = []\n        total_len = 0\n\n        for seg in segments:\n            speaker = seg[\"speaker\"]\n            text = seg[\"text\"]\n            duration = seg[\"duration\"]\n\n            # audio tokens\n            audio_tokens = seg[\"audio_tokens\"]\n            audio_tokens = np.array(audio_tokens)\n            audio_tokens_flatten = audio_tokens.reshape([-1])\n\n            # tokenize text\n            text = speaker + \"<|text_start|>\" + text + \"<|text_end|>\"\n            text_tokens = text_tokenizer.encode(text)\n\n            if duration > MAX_AUDIO_DURATION:\n                print(\"Skipping: audio exceeds 35s....\")\n                continue\n\n            if len(text_tokens) > MAX_TEXT_TOKEN_LEN:\n                print(\"Skipping: text too long...\", text, len(text_tokens))\n                continue\n\n            audio_text_ratio = audio_tokens.shape[1] / len(text_tokens)\n\n            if audio_text_ratio < 1.5 and len(text_tokens) > 30:\n                print(\"Text-to-audio too dense — skipping....\", text, len(text_tokens))\n                continue\n            if audio_text_ratio > 7.5:\n                print(\"Skipping: insufficient text density....\", text, len(text_tokens))\n                continue\n\n            audio_token_list.append(audio_tokens_flatten)\n            text_token_list.append(text_tokens)\n            audio_segment_len.append(len(audio_tokens_flatten))\n            text_segment_len.append(len(text_tokens))\n\n            # total lengths: don't forget extra EOS token len\n            total_len += audio_tokens.shape[-1] + len(text_tokens) + 1\n\n        if len(audio_token_list) > 0:\n            audio_tokens_flatten = np.concatenate(audio_token_list, axis=0)\n            text_tokens = np.concatenate(text_token_list, axis=0)\n            arrow_writer.write(\n                {\n                    \"audio\": audio_tokens_flatten,\n                    \"text\": text_tokens,\n                    \"len\": total_len,\n                    \"audio_segment_len\": audio_segment_len,\n                    \"text_segment_len\": text_segment_len,\n                }\n            )\n    arrow_writer.finalize()\n    arrow_writer.close()"
  },
  {
    "id": "49826eaedc5d2ae5a1b32dd8431a1712591514e6",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/bin/finetune_example/data_preparation/step3_write_arrow.py",
    "kind": "function",
    "name": "get_speaker_dict",
    "signature": "get_speaker_dict(segments)",
    "docstring": "_summary_\n\n    Args:\n        segment (_type_): _description_",
    "identifiers": [
      "get_speaker_dict",
      "num",
      "seg",
      "segments",
      "speaker",
      "speaker_dict",
      "str"
    ],
    "start_line": 42,
    "end_line": 55,
    "text": "def get_speaker_dict(segments):\n    \"\"\"_summary_\n\n    Args:\n        segment (_type_): _description_\n    \"\"\"\n    speaker_dict = {}\n    num = 1\n    for seg in segments:\n        speaker = seg[\"speaker\"]\n        if speaker not in speaker_dict:\n            speaker_dict[speaker] = \"[S\" + str(num) + \"]\"\n            num += 1\n    return speaker_dict"
  },
  {
    "id": "f8af3832d1a504bb01fb6ad947701544828cccc9",
    "file": "/home/mehdi/CMSC473/473-Capstone-Project-EP2C/PaperCodeSync/src/app/repo/bin/finetune_example/data_preparation/step3_write_arrow.py",
    "kind": "function",
    "name": "read_jsonl",
    "signature": "read_jsonl(path)",
    "docstring": "",
    "identifiers": [
      "append",
      "data",
      "data_list",
      "expanduser",
      "f",
      "json",
      "json_str",
      "line",
      "loads",
      "open",
      "os",
      "path",
      "read",
      "read_jsonl",
      "splitlines"
    ],
    "start_line": 31,
    "end_line": 39,
    "text": "def read_jsonl(path):\n    path = os.path.expanduser(path)\n    with open(path, \"r\") as f:\n        json_str = f.read()\n    data_list = []\n    for line in json_str.splitlines():\n        data = json.loads(line)\n        data_list.append(data)\n    return data_list"
  }
]