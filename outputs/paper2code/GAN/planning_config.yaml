## config.yaml
training:
  learning_rate: 0.0002
  batch_size: 128
  epochs: 100
  optimizer:
    type: "Adam"
    beta1: 0.5
dataset:
  name: "MNIST"
  transform: "normalize to [0,1] and flatten to 784-dim"
model:
  generator:
    layers: [1000, 500, 250, 100]
    activation: "ReLU"
    output_activation: "sigmoid"
    batch_normalization: true
  discriminator:
    layers: [1000, 500, 250, 1]
    activation: "ReLU"
    output_activation: "sigmoid"
    batch_normalization: true