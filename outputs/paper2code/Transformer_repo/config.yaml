## config.yaml
training:
  optimizer: "Adam"
  beta1: 0.9
  beta2: 0.98
  epsilon: 1e-9
  warmup_steps: 4000
  learning_rate_schedule: "d_model^-0.5 * min(step_num^-0.5, step_num * warmup_steps^-1.5)"
  base_train_steps: 100000  # Approximately 12 hours on 8 P100 GPUs
  big_train_steps: 300000   # Approximately 3.5 days on 8 P100 GPUs

hyperparameters:
  base_model:
    num_layers: 6
    d_model: 512
    d_ff: 2048
    num_heads: 8
    dropout: 0.1
    label_smoothing: 0.1
  big_model:
    num_layers: 6
    d_model: 1024
    d_ff: 4096
    num_heads: 16
    dropout: 0.1

dataset:
  translation:
    en_de:
      description: "WMT 2014 English-German dataset (~4.5M sentence pairs)"
      tokenization: "Byte-Pair Encoding with a shared vocabulary of ~37000 tokens"
    en_fr:
      description: "WMT 2014 English-French dataset (~36M sentence pairs)"
      tokenization: "Word-piece segmentation with a vocabulary of ~32000 tokens"
    batch_size_tokens: 25000  # Approximate number of tokens per side in each batch
  parsing:
    wsj:
      description: "WSJ portion of the Penn Treebank (~40K sentences)"
      vocabulary_size: 16000
    semi_supervised:
      description: "High-confidence/BerkeleyParser corpus (~17M sentences)"
      vocabulary_size: 32000

inference:
  translation:
    beam_search:
      beam_size: 4
      length_penalty: 0.6
      max_output_length: "input_length + 50"
  parsing:
    beam_search:
      beam_size: 21
      length_penalty: 0.3
      max_output_length: "input_length + 300"