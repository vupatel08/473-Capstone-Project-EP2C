```python
"""evaluation.py
This module defines the Evaluation class that assesses a trained Transformer model
using beam search decoding, and then reports the final evaluation metrics.
For machine translation tasks it computes BLEU (using sacreBLEU) and for constituency
parsing tasks, a token-overlap based F1 score is computed as a proxy.
All inference parameters (beam size, length penalty, maximum output length offset) are read
from the configuration (config.yaml). This module strictly follows the design and interfaces.
"""

import math
import logging
from typing import Any, Dict, List

import torch
import torch.nn as nn
import torch.nn.functional as F
import sacrebleu

# Constants for special tokens (as set in SentencePiece during tokenization)
BOS_TOKEN: int = 2  # Beginning-of-sequence token
EOS_TOKEN: int = 3  # End-of-sequence token
PAD_TOKEN: int = 0  # Padding token


class Evaluation:
    """
    Evaluation class that uses beam search decoding to generate predictions
    for a test dataset and then computes evaluation metrics.
    
    Attributes:
        model (nn.Module): The trained TransformerModel.
        data (Dict[str, Any]): Dictionary with test data loaders.
        config (Dict[str, Any]): Configuration dictionary (from config.yaml).
        task (str): The current task ("translation" or "parsing").
        test_dataloader: The DataLoader for the test split.
        beam_size (int): Beam search beam size.
        length_penalty (float): Length penalty for candidate score adjustment.
        max_output_offset (int): The offset that is added to the input length to determine maximum output length.
        device (torch.device): Computation device.
        logger (logging.Logger): Logger for evaluation messages.
    """

    def __init__(self, model: nn.Module, data: Dict[str, Any], config: Dict[str, Any]) -> None:
        """
        Initializes Evaluation with the given model, evaluation data, and configuration.
        It determines the current task by inspecting the provided data and config.
        It also extracts inference parameters (beam search settings) from config.
        """
        self.model: nn.Module = model
        self.data: Dict[str, Any] = data
        self.config: Dict[str, Any] = config

        self.device: torch.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        self.model.eval()  # Set model to evaluation mode

        # Determine the evaluation task and select the appropriate test dataloader.
        if "translation" in self.data and len(self.data["translation"]) > 0:
            self.task = "translation"
            # Select the first available translation dataset (e.g., "en_de" or "en_fr")
            dataset_key: str = list(self.data["translation"].keys())[0]
            self.test_dataloader = self.data["translation"][dataset_key].get("test")
            inference_conf: Dict[str, Any] = self.config.get("inference", {}).get("translation", {})
        elif "parsing" in self.data and len(self.data["parsing"]) > 0:
            self.task = "parsing"
            # Select the first available parsing dataset (e.g., "wsj")
            dataset_key = list(self.data["parsing"].keys())[0]
            self.test_dataloader = self.data["parsing"][dataset_key].get("test")
            inference_conf = self.config.get("inference", {}).get("parsing", {})
        else:
            raise ValueError("No valid evaluation data found in the provided data dictionary.")

        # Extract beam search settings from the inference configuration.
        beam_search_conf: Dict[str, Any] = inference_conf.get("beam_search", {})
        if self.task == "translation":
            self.beam_size: int = beam_search_conf.get("beam_size", 4)
            self.length_penalty: float = beam_search_conf.get("length_penalty", 0.6)
            max_output_expr: Any = beam_search_conf.get("max_output_length", "input_length + 50")
        else:  # parsing
            self.beam_size = beam_search_conf.get("beam_size", 21)
            self.length_penalty = beam_search_conf.get("length_penalty", 0.3)
            max_output_expr = beam_search_conf.get("max_output_length", "input_length + 300")
        self.max_output_offset: int = self._parse_max_output_offset(max_output_expr)

        # Set up logger.
        self.logger = logging.getLogger("Evaluation")
        if not self.logger.hasHandlers():
            self.logger.setLevel(logging.INFO)
            console_handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            console_handler.setFormatter(formatter)
            self.logger.addHandler(console_handler)

    def _parse_max_output_offset(self, expr: Any) -> int:
        """
        Parses the max output length expression from config.
        Expected format: "input_length + <offset>" or an integer.
        Returns the integer offset value.
        """
        if isinstance(expr, int):
            return expr
        elif isinstance(expr, str):
            if "input_length" in expr:
                parts = expr.split("+")
                if len(parts) == 2:
                    try:
                        return int(parts[1].strip())
                    except ValueError:
                        return 50  # Default offset for translation
                else:
                    return 50
            else:
                try:
                    return int(expr)
                except ValueError:
                    return 50
        else:
            return 50

    def _beam_search(self, src: torch.Tensor, src_mask: torch.Tensor, beam_size: int,
                     length_penalty: float, max_length: int) -> List[int]:
        """
        Performs beam search decoding for a single source input.

        Args:
            src (torch.Tensor): Source tensor of shape (1, src_seq_len).
            src_mask (torch.Tensor): Source mask tensor of shape (1, src_seq_len).
            beam_size (int): Beam search beam size.
            length_penalty (float): Length penalty factor.
            max_length (int): Maximum output length = input_length + offset.

        Returns:
            List[int]: The best candidate output token IDs.
        """
        # Get model dimensions and compute encoder output.
        d_model: int = self.model.embedding.embedding_dim
        src_emb: torch.Tensor = self.model.embedding(src) * math.sqrt(d_model)
        src_emb = self.model.pos_encoding(src_emb)
        memory: torch.Tensor = self.model.encoder(src_emb, src_mask)

        # Initialize the beam with a single candidate: start with BOS.
        beam: List[tuple] = [([BOS_TOKEN], 0.0, False)]  # (token_sequence, cumulative_log_prob, finished)

        for _ in range(max_length):
            new_beam: List[tuple] = []
            for seq, cum_log_prob, finished in beam:
                if finished:
                    new_beam.append((seq, cum_log_prob, finished))
                    continue

                # Prepare current candidate as target sequence.
                tgt_seq: torch.Tensor = torch.tensor(seq, dtype=torch.long, device=self.device).unsqueeze(0)
                # Create target mask (all positions valid since no padding, using ones).
                tgt_mask: torch.Tensor = torch.ones((1, tgt_seq.size(1)), dtype=torch.bool, device=self.device)

                # Compute decoder output for the candidate.
                tgt_emb: torch.Tensor = self.model.embedding(tgt_seq) * math.sqrt(d_model)
                tgt_emb = self.model.pos_encoding(tgt_emb)
                decoder_output: torch.Tensor = self.model.decoder(tgt_emb, memory, src_mask, tgt_mask)
                logits: torch.Tensor = self.model.fc_out(decoder_output)  # Shape: (1, seq_len, vocab_size)
                last_logits: torch.Tensor = logits[:, -1, :]  # Get logits for the last time step (shape: (1, vocab_size))
                log_probs: torch.Tensor = torch.log_softmax(last_logits, dim=-1)

                # Select the top beam_size next tokens.
                topk_log_probs, topk_indices = torch.topk(log_probs, beam_size, dim=-1)
                topk_log_probs = topk_log_probs.squeeze(0)
                topk_indices = topk_indices.squeeze(0)

                for i in range(beam_size):
                    next_token: int = topk_indices[i].item()
                    token_log_prob: float = topk_log_probs[i].item()
                    new_seq = seq + [next_token]
                    new_cum_log_prob = cum_log_prob + token_log_prob
                    new_finished = (next_token == EOS_TOKEN)
                    new_beam.append((new_seq, new_cum_log_prob, new_finished))

            # Apply length penalty and select the best candidates.
            scored_candidates: List[tuple] = []
            for seq, score, finished in new_beam:
                length: int = len(seq)
                penalty: float = ((5 + length) / 6) ** length_penalty
                penalized_score: float = score / penalty
                scored_candidates.append((seq, penalized_score, finished))
            scored_candidates.sort(key=lambda x: x[1], reverse=True)
            beam = scored_candidates[:beam_size]

            # If all candidates have reached EOS, finish early.
            if all(finished for (_, _, finished) in beam):
                break

        # Select the candidate with the best (highest) penalized score.
        best_seq, best_score, _ = max(beam, key=lambda x: x[1])
        return best_seq

    def _decode(self, token_ids: List[int]) -> str:
        """
        Decodes a list of token IDs into a human-readable string.
        It removes BOS tokens and stops decoding at EOS.
        
        Args:
            token_ids (List[int]): The sequence of token IDs.
        
        Returns:
            str: Decoded string.
        """
        filtered_tokens: List[str] = []
        for token in token_ids:
            if token == BOS_TOKEN:
                continue
            if token == EOS_TOKEN:
                break
            # Here, we simply convert the integer token to string.
            # In a full implementation, one would use the original tokenizer's decode method.
            filtered_tokens.append(str(token))
        return " ".join(filtered_tokens)

    def _compute_f1(self, predictions: List[str], references: List[str]) -> float:
        """
        A simple placeholder function to compute token-level F1 score for constituency parsing.
        This implementation computes F1 based on overlap between the sets of tokens.
        
        Args:
            predictions (List[str]): List of predicted strings.
            references (List[str]): List of reference strings.
        
        Returns:
            float: The average F1 score.
        """
        if not predictions:
            return 0.0
        total_f1: float = 0.0
        for pred, ref in zip(predictions, references):
            pred_tokens = set(pred.split())
            ref_tokens = set(ref.split())
            if not ref_tokens:
                f1 = 0.0
            else:
                common: int = len(pred_tokens.intersection(ref_tokens))
                precision: float = common / len(pred_tokens) if pred_tokens else 0.0
                recall: float = common / len(ref_tokens)
                if precision + recall == 0:
                    f1 = 0.0
                else:
                    f1 = 2 * precision * recall / (precision + recall)
            total_f1 += f1
        return total_f1 / len(predictions)

    def evaluate(self) -> Dict[str, float]:
        """
        Runs the evaluation over the test dataset:
          - Iterates over each example in the test dataloader.
          - For each example, performs beam search decoding.
          - Converts predicted token sequences and reference token sequences to text.
          - Computes BLEU (for translation) or token-level F1 (for parsing).
        
        Returns:
            Dict[str, float]: A dictionary containing evaluation metrics.
        """
        self.model.eval()
        predictions: List[str] = []
        references: List[str] = []

        with torch.no_grad():
            for batch in self.test_dataloader:
                # Expect batch to have keys: "src", "tgt", "src_mask", "tgt_mask"
                src_batch: torch.Tensor = batch["src"].to(self.device)    # Shape: (batch_size, src_seq_len)
                tgt_batch: torch.Tensor = batch["tgt"].to(self.device)    # Shape: (batch_size, tgt_seq_len)
                src_mask_batch: torch.Tensor = batch["src_mask"].to(self.device)

                batch_size: int = src_batch.size(0)
                for i in range(batch_size):
                    # Determine valid source length (non-pad tokens)
                    src_mask_i: torch.Tensor = src_mask_batch[i]
                    valid_length: int = int(src_mask_i.sum().item())
                    src_example: torch.Tensor = src_batch[i, :valid_length].unsqueeze(0)
                    src_mask_example: torch.Tensor = src_mask_batch[i, :valid_length].unsqueeze(0)

                    # Maximum output length is defined as (input_length + offset)
                    max_output_length: int = valid_length + self.max_output_offset

                    # Run beam search decoding for this example.
                    predicted_token_ids: List[int] = self._beam_search(
                        src_example, src_mask_example, self.beam_size, self.length_penalty, max_output_length
                    )
                    decoded_prediction: str = self._decode(predicted_token_ids)
                    predictions.append(decoded_prediction)

                    # Decode the reference target.
                    tgt_example: torch.Tensor = tgt_batch[i]
                    tgt_mask_i = batch["tgt_mask"][i]
                    tgt_valid_length: int = int(tgt_mask_i.sum().item())
                    tgt_tokens: List[int] = tgt_example[:tgt_valid_length].tolist()
                    decoded_reference: str = self._decode(tgt_tokens)
                    references.append(decoded_reference)

        results: Dict[str, float] = {}
        if self.task == "translation":
            # Compute BLEU score using sacreBLEU.
            bleu = sacrebleu.corpus_bleu(predictions, [references])
            results = {"task": "translation", "BLEU": bleu.score}
            self.logger.info(f"Final Translation BLEU: {bleu.score:.2f}")
        elif self.task == "parsing":
            # Compute token-level F1 score as a proxy for parsing F1.
            f1_score: float = self._compute_f1(predictions, references)
            results = {"task": "parsing", "F1": f1_score}
            self.logger.info(f"Final Parsing F1: {f1_score:.2f}")
        else:
            self.logger.error("Unknown evaluation task.")
        return results
```