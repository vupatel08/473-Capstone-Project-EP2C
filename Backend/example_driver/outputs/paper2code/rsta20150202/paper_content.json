{
  "title": "TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space",
  "abstract": "Shaolei Zhang1,3, Tian $\\mathbf { Y } \\mathbf { u } ^ { 1 , 3 }$ , Yang Feng1,2,3\\*",
  "authors": [],
  "body_text": [
    {
      "text": "# TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space"
    },
    {
      "text": "Shaolei Zhang1,3, Tian $\\mathbf { Y } \\mathbf { u } ^ { 1 , 3 }$ , Yang Feng1,2,3\\*"
    },
    {
      "text": "1Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS) 2 Key Laboratory of AI Safety, Chinese Academy of Sciences 3 University of Chinese Academy of Sciences, Beijing, China {zhangshaolei20z, yutian23s, fengyang}@ict.ac.cn"
    },
    {
      "text": "# Abstract"
    },
    {
      "text": "Large Language Models (LLMs) sometimes suffer from producing hallucinations, especially LLMs may generate untruthful responses despite knowing the correct knowledge. Activating the truthfulness within LLM is the key to fully unlocking LLM’s knowledge potential. In this paper, we propose TruthX, an inference-time intervention method to activate the truthfulness of LLM by identifying and editing the features within LLM’s internal representations that govern the truthfulness. TruthX employs an auto-encoder to map LLM’s representations into semantic and truthful latent spaces respectively, and applies contrastive learning to identify a truthful editing direction within the truthful space. During inference, by editing LLM’s internal representations in truthful space, TruthX effectively enhances the truthfulness of LLM. Experiments show that TruthX improves the truthfulness of 13 advanced LLMs by an average of $20 \\%$ on TruthfulQA benchmark. Further analyses suggest that TruthX can control LLM to produce truthful or hallucinatory responses via editing only one vector in LLM’s internal representations1."
    },
    {
      "text": "# 1 Introduction"
    },
    {
      "text": "Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing (NLP) tasks (OpenAI, 2022, 2023; Zhang et al., 2023a). However, LLMs sometimes generate fluent, instruction-compliant yet untruthful responses, commonly referred to as “hallucinations” (Ji et al., 2023). This phenomenon significantly undermines the credibility of LLMs in applications. Mitigating hallucinations of LLMs poses a substantial challenge, as hallucinations may stem from various factors, such as blindly following instructions, noisy data, lack of knowledge and the generation process (Zhang et al., 2023c)."
    },
    {
      "text": "![](images/5fa6b8db65c3f8f2b55713c51d960c59625238effad11703d18246d867d867f6.jpg)  \nFigure 1: A case to show that TruthX can control LLM to generate truthful or hallucinatory coherent responses via editing one vector in LLM’s internal representations."
    },
    {
      "text": "Preceding such factors, a more fundamental issue is: whether LLMs can consistently generate truthful responses, even when they possess the correct knowledge? Recent researches suggest “no” for this question. For instance, Wei et al. (2022) found that LLMs can generate truthful responses in some contexts while producing hallucinations in others. Kadavath et al. (2022) and Dhuliawala et al. (2023) discovered that LLMs can identify the presence of hallucinations generated by themselves through self-validation. Saunders et al. (2022) directly pointed out the existence of the generationdiscrimination gap in LLMs. All these findings indicate that LLMs, even equipped with correct knowledge, are still susceptible to producing hallucinations during the generation process. Further, some works found a correlation between the"
    },
    {
      "text": "LLMs’ internal representations and the truthfulness of outputs (Azaria and Mitchell, 2023; Marks and Tegmark, 2023; Zhao et al., 2024), where some erroneous activations of internal representations lead LLMs to generate hallucinations even when they know the correct knowledge (Li et al., 2023b; Zou et al., 2023). Therefore, activating a well-trained LLM to generate truthful responses is the crucial first step in alleviating the hallucination of LLMs."
    },
    {
      "text": "To this end, we propose TruthX, a truthfulness enhancement approach by editing LLM’s internal representations in the truthful space. To edit LLM in the truthful space without compromising its generative capabilities, TruthX decouples the LLM’s internal representations into truthful and semantic latent spaces respectively using an autoencoder. Then, TruthX employs contrastive learning to probe representations (Alain and Bengio, 2017; Belinkov, 2022) with similar semantics but opposite truthfulness and those with similar truthfulness but different semantics within these two latent spaces. During inference, TruthX effectively regulates the truthfulness of LLM by editing it in the truthful space, while ensuring that the generation capability remains intact. Figure 1 illustrates an example of TruthX controlling LLM to generate either truthful or hallucinatory coherent responses."
    },
    {
      "text": "Experimental results show that TruthX enhances the truthfulness of 13 advanced LLMs, including Llama, Mistral, Baichuan and Chatglm, by an average of $20 \\%$ on TruthfulQA benchmark. Through further analyses, we get the following findings:"
    },
    {
      "text": "• TruthX exhibits superiority in truthfulness control. Editing LLMs along the truthful direction can enhance the truthfulness of responses, conversely, editing LLMs along the opposite direction yields highly hallucinatory responses."
    },
    {
      "text": "• The truthful space extracted from homologous LLMs (i.e., trained sequentially) exhibits a high degree of similarity, so we can directly adopt a well-trained TruthX to different homologous models for truthfulness enhancement."
    },
    {
      "text": "• Layer-wise analysis indicates that the representations in middle layers of LLMs exhibit a higher correlation with the truthfulness of responses."
    },
    {
      "text": "# 2 Related Work"
    },
    {
      "text": "Recent efforts aim to enhance LLM’s truthfulness during inference, falling into contrast decoding and representation editing. Contrast decoding modifies output probabilities based on comparisons between strong/weak models (Li et al., 2023c). By using a weak model with illusions, contrast decoding can improve the truthfulness of LLM (Chuang et al., 2023; Zhang et al., 2023b; Kai et al., 2024)."
    },
    {
      "text": "Representation editing has garnered increasing attention due to its controllability and lightweight properties. Early studies have demonstrated that tasks such as style transfer (Subramani et al., 2022; Hernandez et al., 2023) and controllable text generation (Dathathri et al., 2020; Liu et al., 2022) can be achieved by editing model representations. Recently, Contrast-Consistent Search (CCS) (Burns et al., 2023) finds truthful directions using paired internal activations. Inference-time Intervention (ITI) (Li et al., 2023b) probes and adjusts truthfulness within the attention heads of LLM. Expanding on ITI, Truth Forest (TrFr) (Chen et al., 2024) incorporates orthogonal constraints to refine probing capabilities. While ITI and Trfr have shown promising results, only editing attention heads for minimal interference to the generative capabilities (Brown et al., 2023; Hase et al., 2023) limits their enhancement of LLM’s truthfulness (Li et al., 2023b), as FFN module is always considered a knowledge memory (Geva et al., 2021; Hernandez et al., 2023; Li et al., 2023a). To this end, we propose TruthX, which focuses on all internal representations of LLM rather than solely on attention heads. Furthermore, TruthX probes and edits in the truthful space, thereby demonstrating more effective truthfulness enhancement and greater editing flexibility."
    },
    {
      "text": "# 3 TruthX"
    },
    {
      "text": "To activate the truthfulness of a well-trained LLM, we introduce TruthX to edit its internal representations in truthful space. Figure 2 illustrates the diagram of TruthX."
    },
    {
      "text": "# 3.1 Extracting Internal Representations"
    },
    {
      "text": "Most LLMs typically consist of stacked Transformer blocks, where each block comprises an attention module and a feed-forward neural network (FFN) module interconnected by residual connections (Vaswani et al., 2017). As depicted in Figure 2(a), the generation of the next token in an LLM can be conceptualized as the residual connections serving as the main stream, while the attention and FFN modules extract information from the contexts and parameters and add them to the residual stream (Meng et al., 2022). Naturally, hallucinations should logically originate from the representations generated by these attention (Li et al., 2023b) and FFN modules (Geva et al., 2021, 2022). Therefore, we aim to probe these internal representations."
    },
    {
      "text": "![](images/a14bfe84b233ed0b90a66fec3edc37993a6417a32c7b1d5cf26fdb7608d18d7a.jpg)  \nFigure 2: The schematic diagram of TruthX, which maps the LLM’s internal representations into truthful and semantic latent spaces, and then probes and edits the LLM in the truthful space, thereby enhancing its truthfulness."
    },
    {
      "text": "To this end, we first stimulate LLM with both truthful and untruthful responses and extract its internal representations when generating content with opposite truthfulness. To do so, we construct triplets $\\mathcal { D } = \\{ ( Q , A ^ { p o s } , A ^ { n e g } ) \\}$ , where $Q$ is the question, $A ^ { p o s }$ is a truthful answer, and $A ^ { n e g }$ is an untruthful answer. Given $\\mathcal { D }$ , we stimulate LLM with either $Q + A ^ { p o s }$ or $Q + A ^ { n e g }$ to extract the corresponding internal representations."
    },
    {
      "text": "To minimize the interference in probing caused by divergent token semantics, we only extract the internal representations of those tokens that appear simultaneously in both $A ^ { p o s }$ and $A ^ { n e g }$ , thereby ensuring maximal semantic similarity between representations. Formally, we extract the representations of the attention modules and FFN modules’ outputs in each layer when presented with truthful and untruthful stimuli, denoted as $X ^ { p o s } = \\{ x ^ { p o s } \\}$ and $X ^ { n e g } = \\{ x ^ { n e g } \\}$ , where $x ^ { p o s } , x ^ { n e g } \\in \\mathbb { R } ^ { d _ { m o d e l } }$ are the representations of the same token under truthful/untruthful stimuli, respectively, $d _ { m o d e l }$ is the dimension of the LLM’s hidden states."
    },
    {
      "text": "# 3.2 Probing with Auto-Encoder"
    },
    {
      "text": "Given the internal representations of LLM, we map them to the truthful and semantic latent spaces using an auto-encoder. As depicted in Figure 2(b), the auto-encoder consists of a truthful encoder, a semantic encoder and a decoder, all implemented with multi-layer perceptrons (MLPs)."
    },
    {
      "text": "Representation Reconstruction The primary objective of auto-encoder is to map LLM’s internal representation to different latent spaces via encoders, and then reconstruct itself through decoder (Wang et al., 2016). First, truthful encoder TruthEnc(·) and semantic encoder $\\operatorname { S e m E n c } ( \\cdot )$ map the internal representations $x \\in \\{ X ^ { p o s } , X ^ { n e g } \\}$ to the truthful space and semantic space respectively:"
    },
    {
      "text": "$$\nh _ { t r u t h } \\mathrm { = } \\mathrm { T r u t h E n c } ( x ) , h _ { s e m } \\mathrm { = S e m E n c } ( x ) ,\n$$"
    },
    {
      "text": "where $h _ { t r u t h } , h _ { s e m } \\in \\mathbb { R } ^ { d _ { l a t e n t } }$ are the latent representations in truthful and semantic space respectively, $d _ { l a t e n t }$ is the dimension of latent representations. Then, decoder $\\mathrm { D e c } ( \\cdot )$ reconstructs the LLM’s internal representations from the latent space representations, calculated as:"
    },
    {
      "text": "$$\n\\boldsymbol { x } ^ { ' } = \\operatorname { D e c } ( h _ { s e m } + \\mathrm { A t t n } \\left( h _ { s e m } , h _ { t r u t h } \\right) ) ,\n$$"
    },
    {
      "text": "where $x ^ { ' }$ is the reconstructed representations, Attn is an attention operation from semantic latent representations (serving as query) to truthful latent representations (serving as key and value). The auto-encoder is optimized through the reconstruction loss $\\mathcal { L } _ { \\mathit { r e c o n } }$ between $x ^ { ' }$ and $x$ , calculated as:"
    },
    {
      "text": "$$\n\\mathcal { L } _ { r e c o n } = \\mathrm { M S E } ( x , x ^ { ' } ) ,\n$$"
    },
    {
      "text": "where $\\mathrm { M S E ( \\cdot ) }$ is mean square error loss function."
    },
    {
      "text": "Contrastive Learning To encourage the truthful and semantic spaces to capture truthful and semantic features respectively, we employ contrastive learning on the latent representation within these two spaces. Specifically, our purpose is to create a clear demarcation between truthful and untruthful samples within the truthful space, and between samples with different semantics within the semantic space. Contrastive learning is a common technique used to achieve this goal (Sohn, 2016). Here, we first provide the general objective of contrastive learning. For a representation $s$ in the space, we construct a set of samples $S ^ { + }$ with the same class and a set of samples $S ^ { - }$ from different classes. Contrastive learning aligns representations in the space by minimizing the distance between $s$ and $S ^ { + }$ while maximizing the distance between $s$ and $S ^ { - }$ , where the training objective is calculated as:"
    },
    {
      "text": "$$\n\\begin{array} { r l } & { \\mathrm { C T R } ( s , S ^ { + } , S ^ { - } ) = } \\\\ & { - \\log \\frac { \\sum _ { s ^ { \\prime } \\in S ^ { + } } \\exp ( s i m ( s , s ^ { \\prime } / \\tau ) } { \\sum _ { s ^ { \\prime } \\in ( S ^ { + } , S ^ { - } ) } \\exp \\left( s i m \\left( s , s ^ { \\prime } \\right) / \\tau \\right) } . } \\end{array}\n$$"
    },
    {
      "text": "$s i m ( \\cdot , \\cdot )$ refers to cosine similarity between representations, and $\\tau = 0 . 1$ is the temperature."
    },
    {
      "text": "Since contrastive learning is employed on the entire dataset (Tian et al., 2020), we denote the set composed of latent representations in truthful space of all positive samples $x ^ { p o s } \\in X ^ { p o s }$ a s Hpos and those of negative samples $x ^ { p o s } \\in X ^ { n e g }$ as $H _ { t r u t h } ^ { n e g }$ . Similarly, the set composed of semantic latent representations of all positive and negative samples are denoted as $H _ { s e m } ^ { p o s }$ and $H _ { s e m } ^ { n e g }$ respectively."
    },
    {
      "text": "In the truthful space, the latent representations of truthful and untruthful samples should be differentiated. Therefore, for a given sample $h _ { t r u t h } ^ { p o s }$ , those samples sharing the same truthfulness Hpostrut form $S ^ { + }$ , while those with opposite truthfulness $H _ { t r u t h } ^ { n e g }$ form $S ^ { - }$ . The contrastive learning is:"
    },
    {
      "text": "$$\n\\begin{array} { r } { \\mathcal { L } _ { t r u t h } = \\mathrm { C T R } ( h _ { t r u t h } ^ { p o s } , H _ { t r u t h } ^ { p o s } , H _ { t r u t h } ^ { n e g } ) } \\\\ { + \\mathrm { C T R } ( h _ { t r u t h } ^ { n e g } , H _ { t r u t h } ^ { n e g } , H _ { t r u t h } ^ { p o s } ) . } \\end{array}\n$$"
    },
    {
      "text": "In the semantic space, the latent representations of samples with different token meanings shodifferentiated. Therefore, for a given sample $h _ { s e m } ^ { p o s }$ its corresponding $h _ { s e m } ^ { n e g }$ from the same token but opposite truthfulness form $S ^ { + }$ , while those representations with the same truthfulness but different meaning form $S ^ { - }$ . The contrastive learning is:"
    },
    {
      "text": "$$\n\\begin{array} { r } { \\mathcal { L } _ { s e m } = \\mathrm { C T R } ( h _ { s e m } ^ { p o s } , h _ { s e m } ^ { n e g } , H _ { s e m } ^ { p o s } \\setminus h _ { s e m } ^ { p o s } ) } \\\\ { + \\mathrm { C T R } ( h _ { s e m } ^ { n e g } , h _ { s e m } ^ { p o s } , H _ { s e m } ^ { n e g } \\setminus h _ { s e m } ^ { n e g } ) , } \\end{array}\n$$"
    },
    {
      "text": "where $H _ { s e m } ^ { p o s } \\setminus h _ { s e m } ^ { p o s }$ represents removing the element $h _ { s e m } ^ { p o s }$ from the set $H _ { s e m } ^ { p o s }$ . Totally, the contrastive learning in two spaces is calculated as:"
    },
    {
      "text": "$$\n\\mathcal { L } _ { c t r } = \\mathcal { L } _ { t r u t h } + \\mathcal { L } _ { s e m } .\n$$"
    },
    {
      "text": "Owing to the introduced contrastive learning, truthful space captures truthful features and can probe truth/untruth representations, while the semantic space captures semantic features."
    },
    {
      "text": "Truthfulness Editing After mapping the internal representations of LLM into the truthful and semantic space, TruthX aims to edit the latent representations in the truthful space and reconstruct the corresponding representations. To enhance TruthX’s ability to reconstruct from edited latent representations, we introduce an editing loss. Specifically, for a pair of $( x ^ { p o s } , x ^ { n e g } )$ with opposite truthfulness, we exchange their latent representations in the truthful space $h _ { t r u t h } ^ { p o s } \\Leftrightarrow h _ { t r u t h } ^ { n e g }$ and reconstruct $( x ^ { n e g } , x ^ { p o s } )$ respectively via the decoder, represented as:"
    },
    {
      "text": "$$\n\\begin{array} { r } { x ^ { p o s  n e g } { = } \\mathrm { D e c } ( h _ { s e m } ^ { p o s } { + } \\mathrm { A t t n } ( h _ { s e m } ^ { p o s } , h _ { t r u t h } ^ { n e g } ) ) , } \\\\ { x ^ { n e g  p o s } { = } \\mathrm { D e c } ( h _ { s e m } ^ { n e g } { + } \\mathrm { A t t n } ( h _ { s e m } ^ { n e g } , h _ { t r u t h } ^ { p o s } ) ) . } \\end{array}\n$$"
    },
    {
      "text": "$x ^ { p o s  n e g }$ is reconstructed from $h _ { s e m } ^ { p o s }$ and $h _ { t r u t h } ^ { n e g }$ i.e., altering truthfulness from positive to negative, so the reconstructed representation is expected to be close to $x ^ { n e g }$ . Similarly, $x ^ { n e g  p o s }$ should be close to $x ^ { p o s }$ . Therefore, the editing loss $\\mathcal { L } _ { e d i t }$ is:"
    },
    {
      "text": "$$\n\\begin{array} { r } { \\mathcal { L } _ { e d i t } = \\mathrm { M S E } ( x ^ { n e g } , x ^ { p o s \\to n e g } ) } \\\\ { + \\mathrm { M S E } ( x ^ { p o s } , x ^ { n e g \\to p o s } ) . } \\end{array}\n$$"
    },
    {
      "text": "With editing loss, TruthX can adjust the truthfulness by editing the latent representations in the truthful space. Totally, the training objective $\\mathcal { L }$ of TruthX consists of reconstruction loss, contrastive learning and editing loss:"
    }
  ],
  "url": ""
}