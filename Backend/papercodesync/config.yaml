# my first adjustment was token_pattern: [A-Za-z0-9_]+ -> [A-Za-z0-9_\-]+ to capture hyphenated tokens
# although i know it will have more noiser retrieval, i think smaller chunks is worth getting more precise areas
# currently, a lot of the symbols jump to the same chunk, so my second adjustment was chunk_max_chars: 1400 -> 700 and chunk_hard_max_chars: 1600 -> 1000
# for my third adjustment, I tried excluding more section titles that are common but not useful for code search, such as abstract, etc.
# for my fourth adjustment, I increased the alpha from 0.6 to 0.8 to give more weight to overlap (sparse) matching, since code search benefits more from exact term matches.
# okay, i ended up playing with more stuff and got too tired to write my explanations for each change
# surprisingly, changing the semantic model from all-MiniLM-L6-v2 to all-MiniLM-L6-v1 improved results a lot
# also changing to tfidf from bm25 for overlap matching improved results


utils:
  # changes what counts as a "token", which affects the identifiers/terms collected
  # from symbols and docs, which ultimately can shift TF-IDF/BM25 vocab and embedding text
  token_pattern: "[A-Za-z0-9_\\-]+" 
  # changes paper_id and any slug outputs that propagate to filenames/keys
  slugify_maxlen: 80
  # changes all emitted IDs (stable but different length), which flow into symbols/chunks/matches
  id_truncate: 12         
  text:
    # used by keep_text(...)
    min_paragraph_len: 20           
    # rolling split target
    chunk_max_chars: 700            
    # safety cap if a single block is huge
    chunk_hard_max_chars: 1000      
    # how we join lines/blocks before chunking
    paragraph_join: "\n\n"          
  markdown:
    # fenced math delimiter line
    eq_fence: "$$"                  
    # headings that trigger references section
    reference_headings:             
      - "references"
      - "bibliography"
      - "works cited"
    # keep current behavior of adding alt as [figure]
    fold_image_alt_into_prose: true 
  latex:
    # \alpha X -> \alphaX
    strip_space_after_commands: true      
    # \mathrm{R e L U} -> \mathrm{ReLU}
    collapse_mathrm_inner_spaces: true    
    # tabs/dupe spaces -> single space
    collapse_multi_spaces: true           
  python:
    # how far upward we scan for # blocks before a symbol, which creates longer windows and
    # can pull in more doc text leading to different docstring, identifiers and BoW, leading to different matches
    max_leading_lines: 12
    # stop scanning on the first blank line, which can merge separated comment islands 
    # into one docstring (bigger docs)
    stop_on_blank_line: true
  c_like:
    # upward scan limit for slashes/JSDoc, which imapcts the doc size
    max_leading_lines: 12
chunks:
  # Where to take the paper title from 
  title_source: "h1_first"          
  # Which headings become sections (controls section boundaries and counts)
  min_heading_level: 1              
  max_heading_level: 6
  # Skip sections by title (regex, case-insensitive)
  exclude_section_titles_regex: "(acknowledg(e)?ments?|references|funding|appendix|supplement|abstract|introduction|background|overview|related work|methodology)"
  # Drop sections that end up empty (no paragraphs/figures/equations)
  keep_empty_sections: false
  # toggle off to omit references[] altogether
  collect_references: true           
  include_equations: true
  # injects `[eq:<id>]` anchors into paragraph text
  inline_equation_anchors: true      
  include_images: true
  # prefix when folding captions into prose
  figure_caption_prefix: "[figure] " 
  # true = also keep image src in figures[]
  figure_include_src: false          
  # short paragraphs are dropped before chunking
  paragraph_min_chars: 20            
  # join consecutive blocks before chunking
  join_paragraphs_across_blocks: true  
symbols:
  # Skip directories/files
  exclude_dirs:
    - ".git"
    - "node_modules"
    - "dist"
    - "build"
    - "out"
    - "cmake-build-debug"
    - "cmake-build-release"
    - "__pycache__"
    - ".venv"
    - "venv"
    - ".mypy_cache"
    - ".pytest_cache"
  # e.g. "(^|/)tests?/|/vendor/|\\.min\\.(js|css)$"
  file_exclude_regex: ""   
  # include code span text in output
  keep_text_span: true          
  # cap text length per symbol (if keep_text_span=true)
  text_max_chars: 4000          
  # skip very large files (=2MB)
  max_file_bytes: 2000000       
  # Control which symbol kinds are emitted
  include_symbol_kinds: ["function", "method", "class", "constructor", "interface", "enum", "struct"]
  # How to build Python docstrings
  # "both" = triple-quoted + leading #
  # "only_docstring" = just triple-quoted
  # "only_leading" = just leading #
  python_doc_merge_strategy: "both"
  # Module-level span emission defaults
  emit_module_span_default: false
  module_header_capture: true
  module_header_max_lines: 80
  # Filesystem traversal
  follow_symlinks: false
map:
  # "bm25" or "tfidf"
  overlap_method: "tfidf"         
  # weight for overlap; semantic gets (1 - alpha)
  alpha: 0.8                     
  # top-K returned + saved in rows
  top_k: 10                      
  # "minmax" or "none"
  normalization: "minmax"        
  # BM25 options (impactful for sparse text)
  bm25:
    # term saturation
    k1: 1.8    
    # length normalization                  
    b: 0.5                      
  # TF-IDF options (impact vocab & recall)
  tfidf:
    # 1=unigrams only
    ngram_min: 1      
    # include bigrams           
    ngram_max: 2            
    # or "" to disable     
    stop_words: "english"        
    sublinear_tf: true
    use_idf: true
  # Semantic encoder (big lever)
  semantic_model: "sentence-transformers/all-MiniLM-L6-v1"
  semantic:
    normalize_embeddings: true
    # encode speed/VRAM tradeoff
    batch_size: 64               
  # Query construction (changes what you search with)
  query:
    use_full_text_for_semantic: false
    use_full_text_for_overlap: false
    # cap BoW size
    bow_max_terms: 30            